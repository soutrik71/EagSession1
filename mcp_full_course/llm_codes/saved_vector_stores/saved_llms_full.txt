DOCUMENT 1
SOURCE: https://langchain-ai.github.io/langgraph/concepts/
CONTENT:
Conceptual Guide¶
This guide provides explanations of the key concepts behind the LangGraph framework and AI applications more broadly.
We recommend that you go through at least the Quickstart before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.
The conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the Tutorials and How-to guides. For detailed reference material, please see the API reference.
LangGraph¶
High Level¶

Why LangGraph?: A high-level overview of LangGraph and its goals.

Concepts¶

LangGraph Glossary: LangGraph workflows are designed as graphs, with nodes representing different components and edges representing the flow of information between them. This guide provides an overview of the key concepts associated with LangGraph graph primitives.
Common Agentic Patterns: An agent uses an LLM to pick its own control flow to solve more complex problems! Agents are a key building block in many LLM applications. This guide explains the different types of agent architectures and how they can be used to control the flow of an application.
Multi-Agent Systems: Complex LLM applications can often be broken down into multiple agents, each responsible for a different part of the application. This guide explains common patterns for building multi-agent systems.
Breakpoints: Breakpoints allow pausing the execution of a graph at specific points. Breakpoints allow stepping through graph execution for debugging purposes.
Human-in-the-Loop: Explains different ways of integrating human feedback into a LangGraph application.
Time Travel: Time travel allows you to replay past actions in your LangGraph application to explore alternative paths and debug issues.
Persistence: LangGraph has a built-in persistence layer, implemented through checkpointers. This persistence layer helps to support powerful capabilities like human-in-the-loop, memory, time travel, and fault-tolerance.
Memory: Memory in AI applications refers to the ability to process, store, and effectively recall information from past interactions. With memory, your agents can learn from feedback and adapt to users' preferences.
Streaming: Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.
Functional API: @entrypoint and @task decorators that allow you to add LangGraph functionality to an existing codebase.
Durable Execution: LangGraph's built-in persistence layer provides durable execution for workflows, ensuring that the state of each execution step is saved to a durable store. 
Pregel: Pregel is LangGraph's runtime, which is responsible for managing the execution of LangGraph applications.
FAQ: Frequently asked questions about LangGraph.

LangGraph Platform¶
LangGraph Platform is a commercial solution for deploying agentic applications in production, built on the open-source LangGraph framework.
The LangGraph Platform offers a few different deployment options described in the deployment options guide.

Tip

LangGraph is an MIT-licensed open-source library, which we are committed to maintaining and growing for the community.
You can always deploy LangGraph applications on your own infrastructure using the open-source LangGraph project without using LangGraph Platform.

High Level¶

Why LangGraph Platform?: The LangGraph platform is an opinionated way to deploy and manage LangGraph applications. This guide provides an overview of the key features and concepts behind LangGraph Platform.
Platform Architecture: A high-level overview of the architecture of the LangGraph Platform.
Scalability and Resilience: LangGraph Platform is designed to be scalable and resilient. This document explains how the platform achieves this.
Deployment Options: LangGraph Platform offers four deployment options: Cloud SaaS, Self-Hosted Data Plane, Self-Hosted Control Plane, and Standalone Container. This guide explains the differences between these options, and which Plans they are available on.
Plans: LangGraph Platforms offer three different plans: Developer, Plus, Enterprise. This guide explains the differences between these options, what deployment options are available for each, and how to sign up for each one.
Template Applications: Reference applications designed to help you get started quickly when building with LangGraph.

Components¶
The LangGraph Platform comprises several components that work together to support the deployment and management of LangGraph applications:

LangGraph Server: The LangGraph Server is designed to support a wide range of agentic application use cases, from background processing to real-time interactions.
LangGraph Studio: LangGraph Studio is a specialized IDE that can connect to a LangGraph Server to enable visualization, interaction, and debugging of the application locally.
LangGraph CLI: LangGraph CLI is a command-line interface that helps to interact with a local LangGraph
Python/JS SDK: The Python/JS SDK provides a programmatic way to interact with deployed LangGraph Applications.
Remote Graph: A RemoteGraph allows you to interact with any deployed LangGraph application as though it were running locally.
LangGraph Control Plane: The LangGraph Control Plane refers to the Control Plane UI where users create and update LangGraph Servers and the Control Plane APIs that support the UI experience.
LangGraph Data Plane: The LangGraph Data Plane refers to LangGraph Servers, the corresponding infrastructure for each server, and the "listener" application that continuously polls for updates from the LangGraph Control Plane.

LangGraph Server¶

Application Structure: A LangGraph application consists of one or more graphs, a LangGraph API Configuration file (langgraph.json), a file that specifies dependencies, and environment variables.
Assistants: Assistants are a way to save and manage different configurations of your LangGraph applications.
Web-hooks: Webhooks allow your running LangGraph application to send data to external services on specific events.
Cron Jobs: Cron jobs are a way to schedule tasks to run at specific times in your LangGraph application.
Double Texting: Double texting is a common issue in LLM applications where users may send multiple messages before the graph has finished running. This guide explains how to handle double texting with LangGraph Deploy.
Authentication & Access Control: Learn about options for authentication and access control when deploying the LangGraph Platform.

Deployment Options¶

Cloud SaaS(Beta): Connect to your GitHub repositories and deploy LangGraph Servers to LangChain's cloud. We manage everything.
Self-Hosted Data Plane(Beta): Create deployments from the Control Plane UI and deploy LangGraph Servers to your cloud. We manage the control plane, you manage the deployments.
Self-Hosted Control Plane(Beta): Create deployments from a self-hosted Control Plane UI and deploy LangGraph Servers to your cloud. You manage everything.
Standalone Container: Deploy LangGraph Server Docker images however you like.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 2
SOURCE: https://langchain-ai.github.io/langgraph/concepts/langgraph_server/
CONTENT:
LangGraph Server¶

Prerequisites

LangGraph Platform
LangGraph Glossary

Overview¶
LangGraph Server offers an API for creating and managing agent-based applications. It is built on the concept of assistants, which are agents configured for specific tasks, and includes built-in persistence and a task queue. This versatile API supports a wide range of agentic application use cases, from background processing to real-time interactions.
Key Features¶
The LangGraph Platform incorporates best practices for agent deployment, so you can focus on building your agent logic.

Streaming endpoints: Endpoints that expose multiple different streaming modes. We've made these work even for long-running agents that may go minutes between consecutive stream events.
Background runs: The LangGraph Server supports launching assistants in the background with endpoints for polling the status of the assistant's run and webhooks to monitor run status effectively.
Support for long runs: Our blocking endpoints for running assistants send regular heartbeat signals, preventing unexpected connection closures when handling requests that take a long time to complete.
Task queue: We've added a task queue to make sure we don't drop any requests if they arrive in a bursty nature.
Horizontally scalable infrastructure: LangGraph Server is designed to be horizontally scalable, allowing you to scale up and down your usage as needed.
Double texting support: Many times users might interact with your graph in unintended ways. For instance, a user may send one message and before the graph has finished running send a second message. We call this "double texting" and have added four different ways to handle this.
Optimized checkpointer: LangGraph Platform comes with a built-in checkpointer optimized for LangGraph applications.
Human-in-the-loop endpoints: We've exposed all endpoints needed to support human-in-the-loop features.
Memory: In addition to thread-level persistence (covered above by checkpointers), LangGraph Platform also comes with a built-in memory store.
Cron jobs: Built-in support for scheduling tasks, enabling you to automate regular actions like data clean-up or batch processing within your applications.
Webhooks: Allows your application to send real-time notifications and data updates to external systems, making it easy to integrate with third-party services and trigger actions based on specific events.
Monitoring: LangGraph Server integrates seamlessly with the LangSmith monitoring platform, providing real-time insights into your application's performance and health.

What are you deploying?¶
When you deploy a LangGraph Server, you are deploying one or more graphs, a database for persistence, and a task queue.
Graphs¶
When you deploy a graph with LangGraph Server, you are deploying a "blueprint" for an Assistant. 
An Assistant is a graph paired with specific configuration settings. You can create multiple assistants per graph, each with unique settings to accommodate different use cases
that can be served by the same graph.
Upon deployment, LangGraph Server will automatically create a default assistant for each graph using the graph's default configuration settings.
You can interact with assistants through the LangGraph Server API.

Note
We often think of a graph as implementing an agent, but a graph does not necessarily need to implement an agent. For example, a graph could implement a simple
chatbot that only supports back-and-forth conversation, without the ability to influence any application control flow. In reality, as applications get more complex, a graph will often implement a more complex flow that may use multiple agents working in tandem.

Persistence and Task Queue¶
The LangGraph Server leverages a database for persistence and a task queue.
Currently, only Postgres is supported as a database for LangGraph Server and Redis as the task queue.
If you're deploying using LangGraph Cloud, these components are managed for you. If you're deploying LangGraph Server on your own infrastructure, you'll need to set up and manage these components yourself.
Please review the deployment options guide for more information on how these components are set up and managed.
Application Structure¶
To deploy a LangGraph Server application, you need to specify the graph(s) you want to deploy, as well as any relevant configuration settings, such as dependencies and environment variables.
Read the application structure guide to learn how to structure your LangGraph application for deployment.
LangGraph Server API¶
The LangGraph Server API allows you to create and manage assistants, threads, runs, cron jobs, and more.
The LangGraph Cloud API Reference provides detailed information on the API endpoints and data models.
Assistants¶
An Assistant refers to a graph plus specific configuration settings for that graph.
You can think of an assistant as a saved configuration of an agent.
When building agents, it is fairly common to make rapid changes that do not alter the graph logic. For example, simply changing prompts or the LLM selection can have significant impacts on the behavior of the agents. Assistants offer an easy way to make and save these types of changes to agent configuration.
Threads¶
A thread contains the accumulated state of a sequence of runs. If a run is executed on a thread, then the state of the underlying graph of the assistant will be persisted to the thread.
A thread's current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run.
The state of a thread at a particular point in time is called a checkpoint. Checkpoints can be used to restore the state of a thread at a later time.
For more on threads and checkpoints, see this section of the LangGraph conceptual guide.
The LangGraph Cloud API provides several endpoints for creating and managing threads and thread state. See the API reference for more details.
Runs¶
A run is an invocation of an assistant. Each run may have its own input, configuration, and metadata, which may affect execution and output of the underlying graph. A run can optionally be executed on a thread.
The LangGraph Cloud API provides several endpoints for creating and managing runs. See the API reference for more details.
Store¶
Store is an API for managing persistent key-value store that is available from any thread.
Stores are useful for implementing memory in your LangGraph application.
Cron Jobs¶
There are many situations in which it is useful to run an assistant on a schedule. 
For example, say that you're building an assistant that runs daily and sends an email summary
of the day's news. You could use a cron job to run the assistant every day at 8:00 PM.
LangGraph Cloud supports cron jobs, which run on a user-defined schedule. The user specifies a schedule, an assistant, and some input. After that, on the specified schedule, the server will:

Create a new thread with the specified assistant
Send the specified input to that thread

Note that this sends the same input to the thread every time. See the how-to guide for creating cron jobs.
The LangGraph Cloud API provides several endpoints for creating and managing cron jobs. See the API reference for more details.
Webhooks¶
Webhooks enable event-driven communication from your LangGraph Cloud application to external services. For example, you may want to issue an update to a separate service once an API call to LangGraph Cloud has finished running.
Many LangGraph Cloud endpoints accept a webhook parameter. If this parameter is specified by a an endpoint that can accept POST requests, LangGraph Cloud will send a request at the completion of a run.
See the corresponding how-to guide for more detail.
Related¶

LangGraph Application Structure guide explains how to structure your LangGraph application for deployment.
How-to guides for the LangGraph Platform.
The LangGraph Cloud API Reference provides detailed information on the API endpoints and data models.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 3
SOURCE: https://langchain-ai.github.io/langgraph/concepts/langgraph_cli/
CONTENT:
LangGraph CLI¶

Prerequisites

LangGraph Platform
LangGraph Server

The LangGraph CLI is a multi-platform command-line tool for building and running the LangGraph API server locally. The resulting server includes all API endpoints for your graph's runs, threads, assistants, etc. as well as the other services required to run your agent, including a managed database for checkpointing and storage.
Installation¶
The LangGraph CLI can be installed via Homebrew (on macOS) or pip:
Homebrewpip

brew install langgraph-cli

pip install langgraph-cli

Commands¶
The CLI provides the following core functionality:
build¶
The langgraph build command builds a Docker image for the LangGraph API server that can be directly deployed.
dev¶

New in version 0.1.55
The langgraph dev command was introduced in langgraph-cli version 0.1.55.

Python only
Currently, the CLI only supports Python >= 3.11.
JS support is coming soon.

The langgraph dev command starts a lightweight development server that requires no Docker installation. This server is ideal for rapid development and testing, with features like:

Hot reloading: Changes to your code are automatically detected and reloaded
Debugger support: Attach your IDE's debugger for line-by-line debugging
In-memory state with local persistence: Server state is stored in memory for speed but persisted locally between restarts

To use this command, you need to install the CLI with the "inmem" extra:
pip install -U "langgraph-cli[inmem]"

Note: This command is intended for local development and testing only. It is not recommended for production use. Since it does not use Docker, we recommend using virtual environments to manage your project's dependencies.
up¶
The langgraph up command starts an instance of the LangGraph API server locally in a docker container. This requires the docker server to be running locally. It also requires a LangSmith API key for local development or a license key for production use.
The server includes all API endpoints for your graph's runs, threads, assistants, etc. as well as the other services required to run your agent, including a managed database for checkpointing and storage.
dockerfile¶
The langgraph dockerfile command generates a Dockerfile that can be used to build images for and deploy instances of the LangGraph API server. This is useful if you want to further customize the dockerfile or deploy in a more custom way.

Updating your langgraph.json file
The langgraph dockerfile command translates all the configuration in your langgraph.json file into Dockerfile commands. When using this command, you will have to re-run it whenever you update your langgraph.json file. Otherwise, your changes will not be reflected when you build or run the dockerfile.

Related¶

LangGraph CLI API Reference

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 4
SOURCE: https://langchain-ai.github.io/langgraph/concepts/langgraph_data_plane/
CONTENT:
LangGraph Data Plane¶
The term "data plane" is used broadly to refer to LangGraph Servers (deployments), the corresponding infrastructure for each server, and the "listener" application that continuously polls for updates from the LangGraph Control Plane.
Server Infrastructure¶
In addition to the LangGraph Server itself, the following infrastructure for each server are also included in the broad definition of "data plane":

Postgres
Redis
Secrets store
Autoscalers

See LangGraph Platform Architecture for more details.
"Listener" Application¶
The data plane "listener" application periodically calls Control Plane APIs to:

Determine if new deployments should be created.
Determine if existing deployments should be updated (i.e. new revisions).
Determine if existing deployments should be deleted.

In other words, the data plane "listener" reads the latest state of the control plane (desired state) and takes action to reconcile outstanding deployments (current state) to match the latest state.
Data Plane Features¶
This section describes various features of the data plane.
Lite vs Enterprise¶
There are two versions of the LangGraph Server: Lite and Enterprise.
The Lite version is a limited version of the LangGraph Server that you can run locally or in a self-hosted manner (up to 1 million nodes executed per year). Lite is only available for the Standalone Container deployment option.
The Enterprise version is the full version of the LangGraph Server. To use the Enterprise version, you must acquire a license key that you will need to specify when running the Docker image. To acquire a license key, please email sales@langchain.dev. Enterprise is available for Cloud SaaS, Self-Hosted Data Plane, and Self-Hosted Control Plane deployment options.
Feature Differences:

Lite
Enterprise

Cron Jobs
❌
✅

Custom Authentication
❌
✅

Autoscaling¶
Production type deployments automatically scale up to 10 containers. Scaling is based on 3 metrics:

CPU utilization
Memory utilization
Number of pending (in progress) runs

For CPU utilization, the autoscaler targets 75% utilization. This means the autoscaler will scale the number of containers up or down to ensure that CPU utilization is at or near 75%. For memory utilization, the autoscaler targets 75% utilization as well.
For number of pending runs, the autoscaler targets 10 pending runs. For example, if the current number of containers is 1, but the number of pending runs in 20, the autoscaler will scale up the deployment to 2 containers (20 pending runs / 2 containers = 10 pending runs per container).
Each metric is computed independently and the autoscaler will determine the scaling action based on the metric that results in the most number of containers.
Scale down actions are delayed for 30 minutes before any action is taken. In other words, if the autoscaler decides to scale down a deployment, it will first wait for 30 minutes before scaling down. After 30 minutes, the metrics are recomputed and the deployment will scale down if the recomputed metrics result in a lower number of containers than the current number. Otherwise, the deployment remains scaled up. This "cool down" period ensures that deployments do not scale up and down too frequently.
Static IP Addresses¶

Only for Cloud SaaS
Static IP addresses are only available for Cloud SaaS deployments.

All traffic from deployments created after January 6th 2025 will come through a NAT gateway. This NAT gateway will have several static IP addresses depending on the data region. Refer to the table below for the list of static IP addresses:

US
EU

35.197.29.146
34.13.192.67

34.145.102.123
34.147.105.64

34.169.45.153
34.90.22.166

34.82.222.17
34.147.36.213

35.227.171.135
34.32.137.113

34.169.88.30
34.91.238.184

34.19.93.202
35.204.101.241

34.19.34.50
35.204.48.32

Custom Postgres¶

Only for Self-Hosted Data Plane and Self-Hosted Control Plane
Custom Postgres instances are only available for Self-Hosted Data Plane and Self-Hosted Control Plane deployments.

A custom Postgres instance can be used instead of the one automatically created by the control plane. Specify the POSTGRES_URI_CUSTOM environment variable to use a custom Postgres instance.
Multiple deployments can share the same Postgres instance. For example, for Deployment A, POSTGRES_URI_CUSTOM can be set to postgres://<user>:<password>@/<database_name_1>?host=<hostname_1> and for Deployment B, POSTGRES_URI_CUSTOM can be set to postgres://<user>:<password>@/<database_name_2>?host=<hostname_1>. <database_name_1> and database_name_2 are different databases within the same instance, but <hostname_1> is shared. The same database cannot be used for separate deployments.
Custom Redis¶

Only for Self-Hosted Data Plane and Self-Hosted Control Plane
Custom Redis instances are only available for Self-Hosted Data Plane and Self-Hosted Control Plane deployments.

A custom Redis instance can be used instead of the one automatically created by the control plane. Specify the REDIS_URI_CUSTOM environment variable to use a custom Redis instance.
Multiple deployments can share the same Redis instance. For example, for Deployment A, REDIS_URI_CUSTOM can be set to redis://<hostname_1>:<port>/1 and for Deployment B, REDIS_URI_CUSTOM can be set to redis://<hostname_1>:<port>/2. 1 and 2 are different database numbers within the same instance, but <hostname_1> is shared. The same database number cannot be used for separate deployments.
LangSmith Tracing¶
LangGraph Server is automatically configured to send traces to LangSmith. See the table below for details with respect to each deployment option.

Cloud SaaS
Self-Hosted Data Plane
Self-Hosted Control Plane
Standalone Container

RequiredTrace to LangSmith SaaS.
OptionalDisable tracing or trace to LangSmith SaaS.
OptionalDisable tracing or trace to Self-Hosted LangSmith.
OptionalDisable tracing, trace to LangSmith SaaS, or trace to Self-Hosted LangSmith.

Telemetry¶
LangGraph Server is automatically configured to report telemetry metadata for billing purposes. See the table below for details with respect to each deployment option.

Cloud SaaS
Self-Hosted Data Plane
Self-Hosted Control Plane
Standalone Container

Telemetry sent to LangSmith SaaS.
Telemetry sent to LangSmith SaaS.
Self-reported usage (audit) for air-gapped license key.Telemetry sent to LangSmith SaaS for LangGraph Platform License Key.
Self-reported usage (audit) for air-gapped license key.Telemetry sent to LangSmith SaaS for LangGraph Platform License Key.

Licensing¶
LangGraph Server is automatically configured to perform license key validation. See the table below for details with respect to each deployment option.

Cloud SaaS
Self-Hosted Data Plane
Self-Hosted Control Plane
Standalone Container

LangSmith API Key validated against LangSmith SaaS.
LangSmith API Key validated against LangSmith SaaS.
Air-gapped license key or LangGraph Platform License Key validated against LangSmith SaaS.
Air-gapped license key or LangGraph Platform License Key validated against LangSmith SaaS.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 5
SOURCE: https://langchain-ai.github.io/langgraph/concepts/langgraph_standalone_container/
CONTENT:
Standalone Container¶
To deploy a LangGraph Server, follow the how-to guide for how to deploy a Standalone Container.
Overview¶
The Standalone Container deployment option is the least restrictive model for deployment. There is no control plane. Data plane infrastructure is managed by you.

Control Plane
Data Plane

What is it?
n/a
LangGraph ServersPostgres, Redis, etc

Where is it hosted?
n/a
Your cloud

Who provisions and manages it?
n/a
You

Architecture¶

Compute Platforms¶
Kubernetes¶
The Standalone Container deployment option supports deploying data plane infrastructure to a Kubernetes cluster.
Docker¶
The Standalone Container deployment option supports deploying data plane infrastructure to any Docker-supported compute platform.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 6
SOURCE: https://langchain-ai.github.io/langgraph/concepts/self_hosted/
CONTENT:
Self-Hosted¶

Note

LangGraph Platform
Deployment Options

Versions¶
There are two versions of the self-hosted deployment: Self-Hosted Data Plane and Self-Hosted Control Plane.
Self-Hosted Data Plane¶
The Self-Hosted Data Plane deployment option is a "hybrid" model for deployment where we manage the control plane in our cloud and you manage the data plane in your cloud. This option provides a way to securely manage your data plane infrastructure, while offloading control plane management to us.
When using the Self-Hosted Data Plane version, you authenticate with a LangSmith API key.
Self-Hosted Control Plane¶
The Self-Hosted Control Plane deployment option is a fully self-hosted model for deployment where you manage the control plane and data plane in your cloud. This option give you full control and responsibility of the control plane and data plane infrastructure.
Requirements¶

You use langgraph-cli and/or LangGraph Studio app to test graph locally.
You use langgraph build command to build image.

How it works¶

Deploy Redis and Postgres instances on your own infrastructure.
Build the docker image for LangGraph Server using the LangGraph CLI.
Deploy a web server that will run the docker image and pass in the necessary environment variables.

Note
The LangGraph Platform Deployments view is optionally available for Self-Hosted LangGraph deployments. With one click, self-hosted LangGraph deployments can be deployed in the same Kubernetes cluster where a self-hosted LangSmith instance is deployed.

For step-by-step instructions, see How to set up a self-hosted deployment of LangGraph.
Helm Chart¶
If you would like to deploy LangGraph Cloud on Kubernetes, you can use this Helm chart.
Related¶

How to set up a self-hosted deployment of LangGraph.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 7
SOURCE: https://langchain-ai.github.io/langgraph/concepts/application_structure/
CONTENT:
Application Structure¶

Prerequisites

LangGraph Server
LangGraph Glossary

Overview¶
A LangGraph application consists of one or more graphs, a LangGraph API Configuration file (langgraph.json), a file that specifies dependencies, and an optional .env file that specifies environment variables.
This guide shows a typical structure for a LangGraph application and shows how the required information to deploy a LangGraph application using the LangGraph Platform is specified.
Key Concepts¶
To deploy using the LangGraph Platform, the following information should be provided:

A LangGraph API Configuration file (langgraph.json) that specifies the dependencies, graphs, environment variables to use for the application.
The graphs that implement the logic of the application.
A file that specifies dependencies required to run the application.
Environment variable that are required for the application to run.

File Structure¶
Below are examples of directory structures for Python and JavaScript applications:
Python (requirements.txt)Python (pyproject.toml)JS (package.json)

my-app/
├── my_agent # all project code lies within here
│   ├── utils # utilities for your graph
│   │   ├── __init__.py
│   │   ├── tools.py # tools for your graph
│   │   ├── nodes.py # node functions for you graph
│   │   └── state.py # state definition of your graph
│   ├── __init__.py
│   └── agent.py # code for constructing your graph
├── .env # environment variables
├── requirements.txt # package dependencies
└── langgraph.json # configuration file for LangGraph

my-app/
├── my_agent # all project code lies within here
│   ├── utils # utilities for your graph
│   │   ├── __init__.py
│   │   ├── tools.py # tools for your graph
│   │   ├── nodes.py # node functions for you graph
│   │   └── state.py # state definition of your graph
│   ├── __init__.py
│   └── agent.py # code for constructing your graph
├── .env # environment variables
├── langgraph.json  # configuration file for LangGraph
└── pyproject.toml # dependencies for your project

my-app/
├── src # all project code lies within here
│   ├── utils # optional utilities for your graph
│   │   ├── tools.ts # tools for your graph
│   │   ├── nodes.ts # node functions for you graph
│   │   └── state.ts # state definition of your graph
│   └── agent.ts # code for constructing your graph
├── package.json # package dependencies
├── .env # environment variables
└── langgraph.json # configuration file for LangGraph

Note
The directory structure of a LangGraph application can vary depending on the programming language and the package manager used.

Configuration File¶
The langgraph.json file is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy a LangGraph application.
The file supports specification of the following information:

Key
Description

dependencies
Required. Array of dependencies for LangGraph API server. Dependencies can be one of the following: (1) ".", which will look for local Python packages, (2) pyproject.toml, setup.py or requirements.txt in the app directory "./local_package", or (3) a package name.

graphs
Required. Mapping from graph ID to path where the compiled graph or a function that makes a graph is defined. Example: ./your_package/your_file.py:variable, where variable is an instance of langgraph.graph.state.CompiledStateGraph./your_package/your_file.py:make_graph, where make_graph is a function that takes a config dictionary (langchain_core.runnables.RunnableConfig) and creates an instance of langgraph.graph.state.StateGraph / langgraph.graph.state.CompiledStateGraph.

env
Path to .env file or a mapping from environment variable to its value.

python_version
3.11 or 3.12. Defaults to 3.11.

pip_config_file
Path to pip config file.

dockerfile_lines
Array of additional lines to add to Dockerfile following the import from parent image.

Tip
The LangGraph CLI defaults to using the configuration file langgraph.json in the current directory.

Examples¶
PythonJavaScript

The dependencies involve a custom local package and the langchain_openai package.
A single graph will be loaded from the file ./your_package/your_file.py with the variable variable.
The environment variables are loaded from the .env file.

{
    "dependencies": [
        "langchain_openai",
        "./your_package"
    ],
    "graphs": {
        "my_agent": "./your_package/your_file.py:agent"
    },
    "env": "./.env"
}

The dependencies will be loaded from a dependency file in the local directory (e.g., package.json).
A single graph will be loaded from the file ./your_package/your_file.js with the function agent.
The environment variable OPENAI_API_KEY is set inline.

{
    "dependencies": [
        "."
    ],
    "graphs": {
        "my_agent": "./your_package/your_file.js:agent"
    },
    "env": {
        "OPENAI_API_KEY": "secret-key"
    }
}

Dependencies¶
A LangGraph application may depend on other Python packages or JavaScript libraries (depending on the programming language in which the application is written).
You will generally need to specify the following information for dependencies to be set up correctly:

A file in the directory that specifies the dependencies (e.g., requirements.txt, pyproject.toml, or package.json).
A dependencies key in the LangGraph configuration file that specifies the dependencies required to run the LangGraph application.
Any additional binaries or system libraries can be specified using dockerfile_lines key in the LangGraph configuration file.

Graphs¶
Use the graphs key in the LangGraph configuration file to specify which graphs will be available in the deployed LangGraph application.
You can specify one or more graphs in the configuration file. Each graph is identified by a name (which should be unique) and a path for either: (1) the compiled graph or (2) a function that makes a graph is defined.
Environment Variables¶
If you're working with a deployed LangGraph application locally, you can configure environment variables in the env key of the LangGraph configuration file.
For a production deployment, you will typically want to configure the environment variables in the deployment environment.
Related¶
Please see the following resources for more information:

How-to guides for Application Structure.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 8
SOURCE: https://langchain-ai.github.io/langgraph/concepts/deployment_options/
CONTENT:
Deployment Options¶

Prerequisites

LangGraph Platform
LangGraph Server
LangGraph Platform Plans

Overview¶
There are 4 main options for deploying with the LangGraph Platform:

Cloud SaaS(Beta): Available for Plus and Enterprise plans.

Self-Hosted Data Plane(Beta): Available for the Enterprise plan.

Self-Hosted Control Plane(Beta): Available for the Enterprise plan.

Standalone Container: Available for all plans.

Please see the LangGraph Platform Plans for more information on the different plans.
The guide below will explain the differences between the deployment options.
Cloud SaaS¶
The Cloud SaaS deployment option is a fully managed model for deployment where we manage the control plane and data plane in our cloud. This option provides a simple way to deploy and manage your LangGraph Servers.
Connect your GitHub repositories to the platform and deploy your LangGraph Servers from the Control Plane UI. The build process (i.e. CI/CD) is managed internally by the platform.
For more information, please see:

Cloud SaaS Conceptual Guide
How to deploy to Cloud SaaS

Self-Hosted Data Plane¶
The Self-Hosted Data Plane deployment option is a "hybrid" model for deployemnt where we manage the control plane in our cloud and you manage the data plane in your cloud. This option provides a way to securely manage your data plane infrastructure, while offloading control plane management to us.
Build a Docker image using the LangGraph CLI and deploy your LangGraph Server from the Control Plane UI.
Supported Compute Platforms: Kubernetes, Amazon ECS (coming soon!)
For more information, please see:

Self-Hosted Data Plane Conceptual Guide
How to deploy the Self-Hosted Data Plane

Self-Hosted Control Plane¶
The Self-Hosted Control Plane deployment option is a fully self-hosted model for deployment where you manage the control plane and data plane in your cloud. This option give you full control and responsibility of the control plane and data plane infrastructure.
Build a Docker image using the LangGraph CLI and deploy your LangGraph Server from the Control Plane UI.
Supported Compute Platforms: Kubernetes
For more information, please see:

Self-Hosted Control Plane Conceptual Guide
How to deploy the Self-Hosted Control Plane

Standalone Container¶
The Standalone Container deployment option is the least restrictive model for deployment. Deploy standalone instances of a LangGraph Server in your cloud.
Build a Docker image using the LangGraph CLI and deploy your LangGraph Server using the container deployment tooling of your choice. Images can be deployed to any compute platform.
For more information, please see:

Sandalone Container Conceptual Guide
How to deploy a Standalone Container

Related¶
For more information, please see:

LangGraph Platform plans
LangGraph Platform pricing
Deployment how-to guides

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 9
SOURCE: https://langchain-ai.github.io/langgraph/concepts/langgraph_self_hosted_control_plane/
CONTENT:
Self-Hosted Control Plane (Beta)¶
To deploy a LangGraph Server, follow the how-to guide for how to deploy the Self-Hosted Control Plane.
Overview¶
The Self-Hosted Control Plane deployment option is a fully self-hosted model for deployment where you manage the control plane and data plane in your cloud (this option implies that the data plane is self-hosted).

Control Plane
Data Plane

What is it?
Control Plane UI for creating deployments and revisionsControl Plane APIs for creating deployments and revisions
Data plane "listener" for reconciling deployments with control plane stateLangGraph ServersPostgres, Redis, etc

Where is it hosted?
Your cloud
Your cloud

Who provisions and manages it?
You
You

Architecture¶

Compute Platforms¶
Kubernetes¶
The Self-Hosted Control Plane deployment option supports deploying control plane and data plane infrastructure to any Kubernetes cluster.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 10
SOURCE: https://langchain-ai.github.io/langgraph/concepts/langgraph_control_plane/
CONTENT:
LangGraph Control Plane¶
The term "control plane" is used broadly to refer to the Control Plane UI where users create and update LangGraph Servers (deployments) and the Control Plane APIs that support the UI experience.
When a user makes an update through the Control Plane UI, the update is stored in the control plane state. The LangGraph Data Plane "listener" application polls for these updates by calling the Control Plane APIs.
Control Plane UI¶
From the Control Plane UI, you can:

View a list of outstanding deployments.
View details of an individual deployment.
Create a new deployment.
Update a deployment.
Update environment variables for a deployment.
View build and server logs of a deployment.
Delete a deployment.

The Control Plane UI is embedded in LangSmith.
Control Plane API¶
This section describes data model of the LangGraph Control Plane API. Control Plane API is used to create, update, and delete deployments. However, they are not publicly accessible.
Deployment¶
A deployment is an instance of a LangGraph Server. A single deployment can have many revisions.
Revision¶
A revision is an iteration of a deployment. When a new deployment is created, an initial revision is automatically created. To deploy code changes or update environment variables for a deployment, a new revision must be created.
Environment Variable¶
Environment variables are set for a deployment. All environment variables are stored as secrets (i.e. saved in a secrets store).
Control Plane Features¶
This section describes various features of the control plane.
Deployment Types¶
For simplicity, the control plane offers two deployment types with different resource allocations: Development and Production.

Deployment Type
CPU
Memory
Scaling

Development
1 CPU
1 GB
Up to 1 container

Production
2 CPU
2 GB
Up to 10 containers

CPU and memory resources are per container.

For Cloud SaaS
For Production type deployments, resources can be manually increased on a case-by-case basis depending on use case and capacity constraints. Contact support@langchain.dev to request an increase in resources.

For Self-Hosted Data Plane and Self-Hosted Control Plane
Resources for Self-Hosted Data Plane and Self-Hosted Control Plane deployments can be fully customized.

Database Provisioning¶
The control plane and LangGraph Data Plane "listener" application coordinate to automatically create a Postgres database for each deployment. The database serves as the persistence layer for the deployment.
When implementing a LangGraph application, a checkpointer does not need to be configured by the developer. Instead, a checkpointer is automatically configured for the graph. Any checkpointer configured for a graph will be replaced by the one that is automatically configured.
There is no direct access to the database. All access to the database occurs through the LangGraph Server.
The database is never deleted until the deployment itself is deleted. See Automatic Deletion for additional details.

For Self-Hosted Data Plane and Self-Hosted Control Plane
A custom Postgres instance can be configured for Self-Hosted Data Plane and Self-Hosted Control Plane deployments.

Asynchronous Deployment¶
Infrastructure for deployments and revisions are provisioned and deployed asynchronously. They are not deployed immediately after submission. Currently, deployment can take up to several minutes.

When a new deployment is created, a new database is created for the deployment. Database creation is a one-time step. This step contributes to a longer deployment time for the initial revision of the deployment.
When a subsequent revision is created for a deployment, there is no database creation step. The deployment time for a subsequent revision is significantly faster compared to the deployment time of the initial revision.
The deployment process for each revision contains a build step, which can take up to a few minutes.

The control plane and LangGraph Data Plane "listener" application coordinate to achieve asynchronous deployments.
Automatic Deletion¶

Only for Cloud SaaS
Automatic deletion of deployments is only available for Cloud SaaS.

The control plane automatically deletes deployments after 28 consecutive days of non-use (it is in an unused state). A deployment is in an unused state if there are no traces emitted to LangSmith from the deployment after 28 consecutive days. On any given day, if a deployment emits a trace to LangSmith, the counter for consecutive days of non-use is reset.

An email notification is sent after 7 consecutive days of non-use.
A deployment is deleted after 28 consecutive days of non-use.

Data Cannot Be Recovered
After a deployment is deleted, the data (e.g. Postgres) from the deployment cannot be recovered.

LangSmith Integration¶
A LangSmith tracing project is automatically created for each deployment. The tracing project has the same name as the deployment. When creating a deployment, the LANGCHAIN_TRACING and LANGSMITH_API_KEY/LANGCHAIN_API_KEY environment variables do not need to be specified; they are set automatically by the control plane.
When a deployment is deleted, the traces and the tracing project are not deleted.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 11
SOURCE: https://langchain-ai.github.io/langgraph/concepts/faq/
CONTENT:
FAQ¶
Common questions and their answers!
Do I need to use LangChain to use LangGraph? What’s the difference?¶
No. LangGraph is an orchestration framework for complex agentic systems and is more low-level and controllable than LangChain agents. LangChain provides a standard interface to interact with models and other components, useful for straight-forward chains and retrieval flows.
How is LangGraph different from other agent frameworks?¶
Other agentic frameworks can work for simple, generic tasks but fall short for complex tasks bespoke to a company’s needs. LangGraph provides a more expressive framework to handle companies’ unique tasks without restricting users to a single black-box cognitive architecture.
Does LangGraph impact the performance of my app?¶
LangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.
Is LangGraph open source? Is it free?¶
Yes. LangGraph is an MIT-licensed open-source library and is free to use.
How are LangGraph and LangGraph Platform different?¶
LangGraph is a stateful, orchestration framework that brings added control to agent workflows. LangGraph Platform is a service for deploying and scaling LangGraph applications, with an opinionated API for building agent UXs, plus an integrated developer studio.

Features
LangGraph (open source)
LangGraph Platform

Description
Stateful orchestration framework for agentic applications
Scalable infrastructure for deploying LangGraph applications

SDKs
Python and JavaScript
Python and JavaScript

HTTP APIs
None
Yes - useful for retrieving & updating state or long-term memory, or creating a configurable assistant

Streaming
Basic
Dedicated mode for token-by-token messages

Checkpointer
Community contributed
Supported out-of-the-box

Persistence Layer
Self-managed
Managed Postgres with efficient storage

Deployment
Self-managed
• Cloud SaaS  • Free self-hosted  • Enterprise (BYOC or paid self-hosted)

Scalability
Self-managed
Auto-scaling of task queues and servers

Fault-tolerance
Self-managed
Automated retries

Concurrency Control
Simple threading
Supports double-texting

Scheduling
None
Cron scheduling

Monitoring
None
Integrated with LangSmith for observability

IDE integration
LangGraph Studio
LangGraph Studio

What are my deployment options for LangGraph Platform?¶
We currently have the following deployment options for LangGraph applications:

‍Self-Hosted Lite: A free (up to 1M nodes executed), limited version of LangGraph Platform that you can run locally or in a self-hosted manner. This version requires a LangSmith API key and logs all usage to LangSmith. Fewer features are available than in paid plans.
Cloud SaaS: Fully managed and hosted as part of LangSmith, with automatic updates and zero maintenance.
‍Bring Your Own Cloud (BYOC): Deploy LangGraph Platform within your VPC, provisioned and run as a service. Keep data in your environment while outsourcing the management of the service.
Self-Hosted Enterprise: Deploy LangGraph entirely on your own infrastructure.

Is LangGraph Platform open source?¶
No. LangGraph Platform is proprietary software.
There is a free, self-hosted version of LangGraph Platform with access to basic features. The Cloud SaaS deployment option is free while in beta, but will eventually be a paid service. We will always give ample notice before charging for a service and reward our early adopters with preferential pricing. The Bring Your Own Cloud (BYOC) and Self-Hosted Enterprise options are also paid services. Contact our sales team to learn more.
For more information, see our LangGraph Platform pricing page.
Does LangGraph work with LLMs that don't support tool calling?¶
Yes! You can use LangGraph with any LLMs. The main reason we use LLMs that support tool calling is that this is often the most convenient way to have the LLM make its decision about what to do. If your LLM does not support tool calling, you can still use it - you just need to write a bit of logic to convert the raw LLM string response to a decision about what to do.
Does LangGraph work with OSS LLMs?¶
Yes! LangGraph is totally ambivalent to what LLMs are used under the hood. The main reason we use closed LLMs in most of the tutorials is that they seamlessly support tool calling, while OSS LLMs often don't. But tool calling is not necessary (see this section) so you can totally use LangGraph with OSS LLMs.
Can I use LangGraph Studio without logging to LangSmith¶
Yes! You can use the development version of LangGraph Server to run the backend locally.
This will connect to the studio frontend hosted as part of LangSmith.
If you set an environment variable of LANGSMITH_TRACING=false then no traces will be sent to LangSmith.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 12
SOURCE: https://langchain-ai.github.io/langgraph/concepts/high_level/
CONTENT:
Why LangGraph?¶
LLM applications¶
LLMs make it possible to embed intelligence into a new class of applications. There are many patterns for building applications that use LLMs. Workflows have scaffolding of predefined code paths around LLM calls. LLMs can direct the control flow through these predefined code paths, which some consider to be an "agentic system". In other cases, it's possible to remove this scaffolding, creating autonomous agents that can plan, take actions via tool calls, and directly respond to the feedback from their own actions with further actions.

What LangGraph provides¶
LangGraph provides low-level supporting infrastructure that sits underneath any workflow or agent. It does not abstract prompts or architecture, and provides three central benefits:
Persistence¶
LangGraph has a persistence layer, which offers a number of benefits:

Memory: LangGraph persists arbitrary aspects of your application's state, supporting memory of conversations and other updates within and across user interactions;
Human-in-the-loop: Because state is checkpointed, execution can be interrupted and resumed, allowing for decisions, validation, and corrections via human input.

Streaming¶
LangGraph also provides support for streaming workflow / agent state to the user (or developer) over the course of execution. LangGraph supports streaming of both events (such as feedback from a tool call) and tokens from LLM calls embedded in an application.
Debugging and Deployment¶
LangGraph provides an easy onramp for testing, debugging, and deploying applications via LangGraph Platform. This includes Studio, an IDE that enables visualization, interaction, and debugging of workflows or agents. This also includes numerous options for deployment. 

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 13
SOURCE: https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/
CONTENT:
LangGraph Platform¶
Watch this 4-minute overview of LangGraph Platform to see how it helps you build, deploy, and evaluate agentic applications.

Overview¶
LangGraph Platform is a commercial solution for deploying agentic applications to production, built on the open-source LangGraph framework.
The LangGraph Platform consists of several components that work together to support the development, deployment, debugging, and monitoring of LangGraph applications:

LangGraph Server: The server defines an opinionated API and architecture that incorporates best practices for deploying agentic applications, allowing you to focus on building your agent logic rather than developing server infrastructure.
LangGraph Studio: LangGraph Studio is a specialized IDE that can connect to a LangGraph Server to enable visualization, interaction, and debugging of the application locally.
LangGraph CLI: LangGraph CLI is a command-line interface that helps to interact with a local LangGraph
Python/JS SDK: The Python/JS SDK provides a programmatic way to interact with deployed LangGraph Applications.
Remote Graph: A RemoteGraph allows you to interact with any deployed LangGraph application as though it were running locally.
LangGraph Control Plane: The LangGraph Control Plane refers to the Control Plane UI where users create and update LangGraph Servers and the Control Plane APIs that support the UI experience.
LangGraph Data Plane: The LangGraph Data Plane refers to LangGraph Servers, the corresponding infrastructure for each server, and the "listener" application that continuously polls for updates from the LangGraph Control Plane.

The LangGraph Platform offers a few different deployment options described in the deployment options guide.
Why Use LangGraph Platform?¶
LangGraph Platform handles common issues that arise when deploying LLM applications to production, allowing you to focus on agent logic instead of managing server infrastructure.

Streaming Support: As agents grow more sophisticated, they often benefit from streaming both token outputs and intermediate states back to the user. Without this, users are left waiting for potentially long operations with no feedback. LangGraph Server provides multiple streaming modes optimized for various application needs.

Background Runs: For agents that take longer to process (e.g., hours), maintaining an open connection can be impractical. The LangGraph Server supports launching agent runs in the background and provides both polling endpoints and webhooks to monitor run status effectively.

Support for long runs: Vanilla server setups often encounter timeouts or disruptions when handling requests that take a long time to complete. LangGraph Server’s API provides robust support for these tasks by sending regular heartbeat signals, preventing unexpected connection closures during prolonged processes.

Handling Burstiness: Certain applications, especially those with real-time user interaction, may experience "bursty" request loads where numerous requests hit the server simultaneously. LangGraph Server includes a task queue, ensuring requests are handled consistently without loss, even under heavy loads.

Double Texting: In user-driven applications, it’s common for users to send multiple messages rapidly. This “double texting” can disrupt agent flows if not handled properly. LangGraph Server offers built-in strategies to address and manage such interactions.

Checkpointers and Memory Management: For agents needing persistence (e.g., conversation memory), deploying a robust storage solution can be complex. LangGraph Platform includes optimized checkpointers and a memory store, managing state across sessions without the need for custom solutions.

Human-in-the-loop Support: In many applications, users require a way to intervene in agent processes. LangGraph Server provides specialized endpoints for human-in-the-loop scenarios, simplifying the integration of manual oversight into agent workflows.

By using LangGraph Platform, you gain access to a robust, scalable deployment solution that mitigates these challenges, saving you the effort of implementing and maintaining them manually. This allows you to focus more on building effective agent behavior and less on solving deployment infrastructure issues.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 14
SOURCE: https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/
CONTENT:
LangGraph Studio¶

Prerequisites

LangGraph Platform
LangGraph Server

LangGraph Studio offers a new way to develop LLM applications by providing a specialized agent IDE that enables visualization, interaction, and debugging of complex agentic applications.
With visual graphs and the ability to edit state, you can better understand agent workflows and iterate faster. LangGraph Studio integrates with LangSmith allowing you to collaborate with teammates to debug failure modes.

Features¶
The key features of LangGraph Studio are:

Visualize your graphs
Test your graph by running it from the UI
Debug your agent by modifying its state and rerunning
Create and manage assistants
View and manage threads
View and manage long term memory
Add node input/outputs to LangSmith datasets for testing

Getting started¶
There are two ways to connect your LangGraph app with the studio:
Deployed Application¶
If you have deployed your LangGraph application on LangGraph Platform, you can access the studio as part of that deployment. To do so, navigate to the deployment in LangGraph Platform within the LangSmith UI and click the "LangGraph Studio" button.
Local Development Server¶
If you have a LangGraph application that is running locally in-memory, you can connect it to LangGraph Studio in the browser within LangSmith.
By default, starting the local server with langgraph dev will run the server at http://127.0.0.1:2024 and automatically open Studio in your browser. However, you can also manually connect to Studio by either:

In LangGraph Platform, clicking the "LangGraph Studio" button and entering the server URL in the dialog that appears.

or

Navigating to the URL in your browser:

https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024

Related¶
For more information please see the following:

LangGraph Studio how-to guides
LangGraph CLI Documentation

LangGraph Studio FAQs¶
Why is my project failing to start?¶
A project may fail to start if the configuration file is defined incorrectly, or if required environment variables are missing. See here for how your configuration file should be defined.
How does interrupt work?¶
When you select the Interrupts dropdown and select a node to interrupt the graph will pause execution before and after (unless the node goes straight to END) that node has run. This means that you will be able to both edit the state before the node is ran and the state after the node has ran. This is intended to allow developers more fine-grained control over the behavior of a node and make it easier to observe how the node is behaving. You will not be able to edit the state after the node has ran if the node is the final node in the graph.
For more information on interrupts and human in the loop, see here.
Why are extra edges showing up in my graph?¶
If you don't define your conditional edges carefully, you might notice extra edges appearing in your graph. This is because without proper definition, LangGraph Studio assumes the conditional edge could access all other nodes. In order for this to not be the case, you need to be explicit about how you define the nodes the conditional edge routes to. There are two ways you can do this:
Solution 1: Include a path map¶
The first way to solve this is to add path maps to your conditional edges. A path map is just a dictionary or array that maps the possible outputs of your router function with the names of the nodes that each output corresponds to. The path map is passed as the third argument to the add_conditional_edges function like so:
PythonJavascript

graph.add_conditional_edges("node_a", routing_function, {True: "node_b", False: "node_c"})

graph.addConditionalEdges("node_a", routingFunction, { true: "node_b", false: "node_c" });

In this case, the routing function returns either True or False, which map to node_b and node_c respectively.
Solution 2: Update the typing of the router (Python only)¶
Instead of passing a path map, you can also be explicit about the typing of your routing function by specifying the nodes it can map to using the Literal python definition. Here is an example of how to define a routing function in that way:
def routing_function(state: GraphState) -> Literal["node_b","node_c"]:
    if state['some_condition'] == True:
        return "node_b"
    else:
        return "node_c"

Studio Desktop FAQs¶

Deprecation Warning
In order to support a wider range of platforms and users, we now recommend following the above instructions to connect to LangGraph Studio using the development server instead of the desktop app.

The LangGraph Studio Desktop App is a standalone application that allows you to connect to your LangGraph application and visualize and interact with your graph. It is available for MacOS only and requires Docker to be installed.
Why is my project failing to start?¶
In addition to the reasons listed above, for the desktop app there are a few more reasons that your project might fail to start:

Note 
LangGraph Studio Desktop automatically populates LANGCHAIN_* environment variables for license verification and tracing, regardless of the contents of the .env file. All other environment variables defined in .env will be read as normal.

Docker issues¶
LangGraph Studio (desktop) requires Docker Desktop version 4.24 or higher. Please make sure you have a version of Docker installed that satisfies that requirement and also make sure you have the Docker Desktop app up and running before trying to use LangGraph Studio. In addition, make sure you have docker-compose updated to version 2.22.0 or higher.
Incorrect data region¶
If you receive a license verification error when attempting to start the LangGraph Server, you may be logged into the incorrect LangSmith data region. Ensure that you're logged into the correct LangSmith data region and ensure that the LangSmith account has access to LangGraph platform.

In the top right-hand corner, click the user icon and select Logout.
At the login screen, click the Data Region dropdown menu and select the appropriate data region. Then click Login to LangSmith.

How do I reload the app?¶
If you would like to reload the app, don't use Command+R as you might normally do. Instead, close and reopen the app for a full refresh.
How does automatic rebuilding work?¶
One of the key features of LangGraph Studio is that it automatically rebuilds your image when you change the source code. This allows for a super fast development and testing cycle which makes it easy to iterate on your graph. There are two different ways that LangGraph rebuilds your image: either by editing the image or completely rebuilding it.
Rebuilds from source code changes¶
If you modified the source code only (no configuration or dependency changes!) then the image does not require a full rebuild, and LangGraph Studio will only update the relevant parts. The UI status in the bottom left will switch from Online to Stopping temporarily while the image gets edited. The logs will be shown as this process is happening, and after the image has been edited the status will change back to Online and you will be able to run your graph with the modified code!
Rebuilds from configuration or dependency changes¶
If you edit your graph configuration file (langgraph.json) or the dependencies (either pyproject.toml or requirements.txt) then the entire image will be rebuilt. This will cause the UI to switch away from the graph view and start showing the logs of the new image building process. This can take a minute or two, and once it is done your updated image will be ready to use!
Why is my graph taking so long to startup?¶
The LangGraph Studio interacts with a local LangGraph API server. To stay aligned with ongoing updates, the LangGraph API requires regular rebuilding. As a result, you may occasionally experience slight delays when starting up your project.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 15
SOURCE: https://langchain-ai.github.io/langgraph/concepts/langgraph_self_hosted_data_plane/
CONTENT:
Self-Hosted Data Plane (Beta)¶
To deploy a LangGraph Server, follow the how-to guide for how to deploy the Self-Hosted Data Plane.
Overview¶
LangGraph Platform's Self-Hosted Data Plane deployment option is a "hybrid" model for deployemnt where we manage the control plane in our cloud and you manage the data plane in your cloud.

Control Plane
Data Plane

What is it?
Control Plane UI for creating deployments and revisionsControl Plane APIs for creating deployments and revisions
Data plane "listener" for reconciling deployments with control plane stateLangGraph ServersPostgres, Redis, etc

Where is it hosted?
LangChain's cloud
Your cloud

Who provisions and manages it?
LangChain
You

Architecture¶

Compute Platforms¶
Kubernetes¶
The Self-Hosted Data Plane deployment option supports deploying data plane infrastructure to any Kubernetes cluster.
Amazon ECS¶
Coming soon...

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 16
SOURCE: https://langchain-ai.github.io/langgraph/concepts/langgraph_cloud/
CONTENT:
Cloud SaaS (Beta)¶
To deploy a LangGraph Server, follow the how-to guide for how to deploy to Cloud SaaS.
Overview¶
The Cloud SaaS deployment option is a fully managed model for deployment where we manage the control plane and data plane in our cloud.

Control Plane
Data Plane

What is it?
Control Plane UI for creating deployments and revisionsControl Plane APIs for creating deployments and revisions
Data plane "listener" for reconciling deployments with control plane stateLangGraph ServersPostgres, Redis, etc

Where is it hosted?
LangChain's cloud
LangChain's cloud

Who provisions and manages it?
LangChain
LangChain

Architecture¶

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 17
SOURCE: https://langchain-ai.github.io/langgraph/concepts/platform_architecture/
CONTENT:
LangGraph Platform Architecture¶

How we use Postgres¶
Postgres is the persistence layer for all user, run, and long-term memory data in LGP. This stores both checkpoints (see more info here), server resources (threads, runs, assistants and crons), as well as items saved in the long-term memory store (see more info here).
How we use Redis¶
Redis is used in each LGP deployment as a way for server and queue workers to communicate, and to store ephemeral metadata, more details on both below. No user/run data is stored in Redis.
Communication¶
All runs in LGP are executed by the pool of background workers that are part of each deployment. In order to enable some features for those runs (such as cancellation and output streaming) we need a channel for two-way communication between the server and the worker handling a particular run. We use Redis to organize that communication.

A Redis list is used as a mechanism to wake up a worker as soon as a new run is created. Only a sentinel value is stored in this list, no actual run info. The run information is then retrieved from Postgres by the worker.
A combination of a Redis string and Redis PubSub channel is used for the server to communicate a run cancellation request to the appropriate worker.
A Redis PubSub channel is used by the worker to broadcast streaming output from an agent while the run is being handled. Any open /stream request in the server will subscribe to that channel and forward any events to the response as they arrive. No events are stored in Redis at any time.

Ephemeral metadata¶
Runs in an LGP deployment may be retried for specific failures (currently only for transient Postgres errors encountered during the run). In order to limit the number of retries (currently limited to 3 attempts per run) we record the attempt number in a Redis string when is picked up. This contains no run-specific info other than its ID, and expires after a short delay.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 18
SOURCE: https://langchain-ai.github.io/langgraph/concepts/bring_your_own_cloud/
CONTENT:
Bring Your Own Cloud (BYOC)¶

Note

LangGraph Platform
Deployment Options

Architecture¶
Split control plane (hosted by us) and data plane (hosted by you, managed by us).

Control Plane
Data Plane

What it does
Manages deployments, revisions.
Runs your LangGraph graphs, stores your data.

Where it is hosted
LangChain Cloud account
Your cloud account

Who provisions and monitors
LangChain
LangChain

LangChain has no direct access to the resources created in your cloud account, and can only interact with them via AWS APIs. Your data never leaves your cloud account / VPC at rest or in transit.

Requirements¶

You’re using AWS already.
You use langgraph-cli and/or LangGraph Studio app to test graph locally.
You use langgraph build command to build image and then push it to your AWS ECR repository (docker push).

How it works¶

We provide you a Terraform module which you run to set up our requirements
Creates an AWS role (which our control plane will later assume to provision and monitor resources)
https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonVPCReadOnlyAccess.html
Read VPCS to find subnets

https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonECS_FullAccess.html
Used to create/delete ECS resources for your LangGraph Cloud instances

https://docs.aws.amazon.com/aws-managed-policy/latest/reference/SecretsManagerReadWrite.html
Create secrets for your ECS resources

https://docs.aws.amazon.com/aws-managed-policy/latest/reference/CloudWatchReadOnlyAccess.html
Read CloudWatch metrics/logs to monitor your instances/push deployment logs

https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonRDSFullAccess.html
Provision RDS instances for your LangGraph Cloud instances
Alternatively, an externally managed Postgres instance can be used instead of the default RDS instance. LangChain does not monitor or manage the externally managed Postgres instance. See details for POSTGRES_URI_CUSTOM environment variable.

Either
Tags an existing vpc / subnets as langgraph-cloud-enabled
Creates a new vpc and subnets and tags them as langgraph-cloud-enabled

You create a LangGraph Cloud Project in smith.langchain.com providing
the ID of the AWS role created in the step above
the AWS ECR repo to pull the service image from

We provision the resources in your cloud account using the role above
We monitor those resources to ensure uptime and recovery from errors

Notes for customers using self-hosted LangSmith:

Creation of new LangGraph Cloud projects and revisions currently needs to be done on smith.langchain.com.
However, you can set up the project to trace to your self-hosted LangSmith instance if desired. See details for LANGSMITH_RUNS_ENDPOINTS environment variable.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 19
SOURCE: https://langchain-ai.github.io/langgraph/concepts/plans/
CONTENT:
LangGraph Platform Plans¶
Overview¶
LangGraph Platform is a commercial solution for deploying agentic applications in production.
There are three different plans for using it.

Developer: All LangSmith users have access to this plan. You can sign up for this plan simply by creating a LangSmith account. This gives you access to the Self-Hosted Lite deployment option.
Plus: All LangSmith users with a Plus account have access to this plan. You can sign up for this plan simply by upgrading your LangSmith account to the Plus plan type. This gives you access to the Cloud deployment option.
Enterprise: This is separate from LangSmith plans. You can sign up for this plan by contacting sales@langchain.dev. This gives you access to all deployment options: Cloud, Bring-Your-Own-Cloud, and Self Hosted Enterprise

Plan Details¶

Developer
Plus
Enterprise

Deployment Options
Self-Hosted Lite
Cloud
Self-Hosted Enterprise, Cloud, Bring-Your-Own-Cloud

Usage
Free, limited to 1M nodes executed per year
Free while in Beta, will be charged per node executed
Custom

APIs for retrieving and updating state and conversational history
✅
✅
✅

APIs for retrieving and updating long-term memory
✅
✅
✅

Horizontally scalable task queues and servers
✅
✅
✅

Real-time streaming of outputs and intermediate steps
✅
✅
✅

Assistants API (configurable templates for LangGraph apps)
✅
✅
✅

Cron scheduling
--
✅
✅

LangGraph Studio for prototyping
✅
✅
✅

Authentication & authorization to call the LangGraph APIs
--
Coming Soon!
Coming Soon!

Smart caching to reduce traffic to LLM API
--
Coming Soon!
Coming Soon!

Publish/subscribe API for state
--
Coming Soon!
Coming Soon!

Scheduling prioritization
--
Coming Soon!
Coming Soon!

Please see the LangGraph Platform Pricing for information on pricing.
Related¶
For more information, please see:

Deployment Options conceptual guide
LangGraph Platform Pricing
LangSmith Plans

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 20
SOURCE: https://langchain-ai.github.io/langgraph/concepts/scalability_and_resilience/
CONTENT:
LangGraph Platform: Scalability & Resilience¶
LangGraph Platform is designed to scale horizontally with your workload. Each instance of the service is stateless, and keeps no resources in memory. The service is designed to gracefully handle new instances being added or removed, including hard shutdown cases.
Server scalability¶
As you add more instances to a service, they will share the HTTP load as long as an appropriate load balancer mechanism is placed in front of them. In most deployment modalities we configure a load balancer for the service automatically. In the “self-hosted without control plane” modality it’s your responsibility to add a load balancer. Since the instances are stateless any load balancing strategy will work, no session stickiness is needed, or recommended. Any instance of the server can communicate with any queue instance (through Redis PubSub), meaning that requests to cancel or stream an in-progress run can be handled by any arbitrary instance.
Queue scalability¶
As you add more instances to a service, they will increase run throughput linearly, as each instance is configured to handle a set number of concurrent runs (by default 10). Each attempt for each run will be handled by a single instance, with exactly-once semantics enforced through Postgres’s MVCC model (refer to section below for crash resilience details). Attempts that fail due to transient database errors are retried up to 3 times. We do not make use of long-lived transactions or locks, this enables us to make more efficient use of Postgres resources.
Resilience¶
While a run is being handled by a queue instance, a periodic heartbeat timestamp will be recorded in Redis by that queue worker.
When a graceful shutdown request is received (SIGINT) an instance enters shutdown mode, which

stops accepting new HTTP requests
gives any in-progress runs a limited number of seconds to finish (if not finished it will be put back in the queue)
stops the instance from picking up more runs from the queue

If a hard shutdown occurs, eg. due to a server crash, or an infra failure, any runs that were in progress will be picked up by a periodic sweeper task that looks for in-progress runs that have breached their heartbeat window, which will put them back in the queue for another instance to pick them up.
Postgres resilience¶
For deployment modalities where we manage the Postgres database we have periodic backups, continuously replicated standby replicas for automatic failover. Optionally, on request, we can also setup read replicas as well as other advanced failover capabilities.
All communication with Postgres implements retries for retry-able errors. If Postgres is momentarily unavailable, such as during a database restart, most/all traffic should continue to succeed. Prolonged failure of the Postgres instance will switch traffic to the failover replica. If the failover replica also fails before the primary is brought back online the service would become unavailable.
Redis resilience¶
All data that requires durable storage is stored in Postgres, not Redis. Redis is used only for ephemeral metadata, and communication between instances. Refer to the architecture page for more details on how we use Redis. Therefore we place no durability requirements on Redis.
All communication with Redis implements retries for retry-able errors. If Redis is momentarily unavailable, such as during a database restart, most/all traffic should continue to succeed. Prolonged failure of Redis will render the LGP service unavailable.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 21
SOURCE: https://langchain-ai.github.io/langgraph/concepts/template_applications/
CONTENT:
Template Applications¶
Templates are open source reference applications designed to help you get started quickly when building with LangGraph. They provide working examples of common agentic workflows that can be customized to your needs.
You can create an application from a template using the LangGraph CLI.

Requirements

Python >= 3.11
LangGraph CLI: Requires langchain-cli[inmem] >= 0.1.58

Install the LangGraph CLI¶
pip install "langgraph-cli[inmem]" --upgrade

Available Templates¶

Template
Description
Python
JS/TS

New LangGraph Project
A simple, minimal chatbot with memory.
Repo
Repo

ReAct Agent
A simple agent that can be flexibly extended to many tools.
Repo
Repo

Memory Agent
A ReAct-style agent with an additional tool to store memories for use across threads.
Repo
Repo

Retrieval Agent
An agent that includes a retrieval-based question-answering system.
Repo
Repo

Data-Enrichment Agent
An agent that performs web searches and organizes its findings into a structured format.
Repo
Repo

üå± Create a LangGraph App¶
To create a new app from a template, use the langgraph new command.
langgraph new

Next Steps¶
Review the README.md file in the root of your new LangGraph app for more information about the template and how to customize it.
After configuring the app properly and adding your API keys, you can start the app using the LangGraph CLI:
langgraph dev 

See the following guides for more information on how to deploy your app:

Launch Local LangGraph Server: This quick start guide shows how to start a LangGraph Server locally for the ReAct Agent template. The steps are similar for other templates.
Deploy to LangGraph Cloud: Deploy your LangGraph app using LangGraph Cloud.

LangGraph Framework¶

LangGraph Concepts: Learn the foundational concepts of LangGraph.
LangGraph How-to Guides: Guides for common tasks with LangGraph.

üìö Learn More about LangGraph Platform¶
Expand your knowledge with these resources:

LangGraph Platform Concepts: Understand the foundational concepts of the LangGraph Platform.
LangGraph Platform How-to Guides: Discover step-by-step guides to build and deploy applications.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 22
SOURCE: https://langchain-ai.github.io/langgraph/concepts/persistence/
CONTENT:
Persistence¶
LangGraph has a built-in persistence layer, implemented through checkpointers. When you compile graph with a checkpointer, the checkpointer saves a checkpoint of the graph state at every super-step. Those checkpoints are saved to a thread, which can be accessed after graph execution. Because threads allow access to graph's state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. See this how-to guide for an end-to-end example on how to add and use checkpointers with your graph. Below, we'll discuss each of these concepts in more detail. 

LangGraph API handles checkpointing automatically
When using the LangGraph API, you don't need to implement or configure checkpointers manually. The API handles all persistence infrastructure for you behind the scenes.

Threads¶
A thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. When invoking graph with a checkpointer, you must specify a thread_id as part of the configurable portion of the config:
{"configurable": {"thread_id": "1"}}

Checkpoints¶
Checkpoint is a snapshot of the graph state saved at each super-step and is represented by StateSnapshot object with the following key properties:

config: Config associated with this checkpoint. 
metadata: Metadata associated with this checkpoint.
values: Values of the state channels at this point in time.
next A tuple of the node names to execute next in the graph.
tasks: A tuple of PregelTask objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted dynamically from within a node, tasks will contain additional data associated with interrupts.

Let's see what checkpoints are saved when a simple graph is invoked as follows:
API Reference: StateGraph | START | END | InMemorySaver
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver
from typing import Annotated
from typing_extensions import TypedDict
from operator import add

class State(TypedDict):
    foo: str
    bar: Annotated[list[str], add]

def node_a(state: State):
    return {"foo": "a", "bar": ["a"]}

def node_b(state: State):
    return {"foo": "b", "bar": ["b"]}

workflow = StateGraph(State)
workflow.add_node(node_a)
workflow.add_node(node_b)
workflow.add_edge(START, "node_a")
workflow.add_edge("node_a", "node_b")
workflow.add_edge("node_b", END)

checkpointer = InMemorySaver()
graph = workflow.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "1"}}
graph.invoke({"foo": ""}, config)

After we run the graph, we expect to see exactly 4 checkpoints:

empty checkpoint with START as the next node to be executed
checkpoint with the user input {'foo': '', 'bar': []} and node_a as the next node to be executed
checkpoint with the outputs of node_a {'foo': 'a', 'bar': ['a']} and node_b as the next node to be executed
checkpoint with the outputs of node_b {'foo': 'b', 'bar': ['a', 'b']} and no next nodes to be executed

Note that we bar channel values contain outputs from both nodes as we have a reducer for bar channel.
Get state¶
When interacting with the saved graph state, you must specify a thread identifier. You can view the latest state of the graph by calling graph.get_state(config). This will return a StateSnapshot object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.
# get the latest state snapshot
config = {"configurable": {"thread_id": "1"}}
graph.get_state(config)

# get a state snapshot for a specific checkpoint_id
config = {"configurable": {"thread_id": "1", "checkpoint_id": "1ef663ba-28fe-6528-8002-5a559208592c"}}
graph.get_state(config)

In our example, the output of get_state will look like this:
StateSnapshot(
    values={'foo': 'b', 'bar': ['a', 'b']},
    next=(),
    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},
    metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},
    created_at='2024-08-29T19:19:38.821749+00:00',
    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=()
)

Get state history¶
You can get the full history of the graph execution for a given thread by calling graph.get_state_history(config). This will return a list of StateSnapshot objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / StateSnapshot being the first in the list.
config = {"configurable": {"thread_id": "1"}}
list(graph.get_state_history(config))

In our example, the output of get_state_history will look like this:
[
    StateSnapshot(
        values={'foo': 'b', 'bar': ['a', 'b']},
        next=(),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},
        metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},
        created_at='2024-08-29T19:19:38.821749+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},
        tasks=(),
    ),
    StateSnapshot(
        values={'foo': 'a', 'bar': ['a']}, next=('node_b',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},
        metadata={'source': 'loop', 'writes': {'node_a': {'foo': 'a', 'bar': ['a']}}, 'step': 1},
        created_at='2024-08-29T19:19:38.819946+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},
        tasks=(PregelTask(id='6fb7314f-f114-5413-a1f3-d37dfe98ff44', name='node_b', error=None, interrupts=()),),
    ),
    StateSnapshot(
        values={'foo': '', 'bar': []},
        next=('node_a',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},
        metadata={'source': 'loop', 'writes': None, 'step': 0},
        created_at='2024-08-29T19:19:38.817813+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},
        tasks=(PregelTask(id='f1b14528-5ee5-579c-949b-23ef9bfbed58', name='node_a', error=None, interrupts=()),),
    ),
    StateSnapshot(
        values={'bar': []},
        next=('__start__',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},
        metadata={'source': 'input', 'writes': {'foo': ''}, 'step': -1},
        created_at='2024-08-29T19:19:38.816205+00:00',
        parent_config=None,
        tasks=(PregelTask(id='6d27aa2e-d72b-5504-a36f-8620e54a76dd', name='__start__', error=None, interrupts=()),),
    )
]

Replay¶
It's also possible to play-back a prior graph execution. If we invoke a graph with a thread_id and a checkpoint_id, then we will re-play the previously executed steps before a checkpoint that corresponds to the checkpoint_id, and only execute the steps after the checkpoint.

thread_id is the ID of a thread.
checkpoint_id is an identifier that refers to a specific checkpoint within a thread.

You must pass these when invoking the graph as part of the configurable portion of the config:
config = {"configurable": {"thread_id": "1", "checkpoint_id": "0c62ca34-ac19-445d-bbb0-5b4984975b2a"}}
graph.invoke(None, config=config)

Importantly, LangGraph knows whether a particular step has been executed previously. If it has, LangGraph simply re-plays that particular step in the graph and does not re-execute the step, but only for the steps before the provided checkpoint_id. All of the steps after checkpoint_id will be executed (i.e., a new fork), even if they have been executed previously. See this how to guide on time-travel to learn more about replaying.

Update state¶
In addition to re-playing the graph from specific checkpoints, we can also edit the graph state. We do this using graph.update_state(). This method accepts three different arguments:
config¶
The config should contain thread_id specifying which thread to update. When only the thread_id is passed, we update (or fork) the current state. Optionally, if we include checkpoint_id field, then we fork that selected checkpoint.
values¶
These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions, if they are defined for some of the channels in the graph state. This means that update_state does NOT automatically overwrite the channel values for every channel, but only for the channels without reducers. Let's walk through an example.
Let's assume you have defined the state of your graph with the following schema (see full example above):
from typing import Annotated
from typing_extensions import TypedDict
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]

Let's now assume the current state of the graph is
{"foo": 1, "bar": ["a"]}

If you update the state as below:
graph.update_state(config, {"foo": 2, "bar": ["b"]})

Then the new state of the graph will be:
{"foo": 2, "bar": ["a", "b"]}

The foo key (channel) is completely changed (because there is no reducer specified for that channel, so update_state overwrites it). However, there is a reducer specified for the bar key, and so it appends "b" to the state of bar.
as_node¶
The final thing you can optionally specify when calling update_state is as_node. If you provided it, the update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this how to guide on time-travel to learn more about forking state.

Memory Store¶

A state schema specifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence.
But, what if we want to retain some information across threads? Consider the case of a chatbot where we want to retain specific information about the user across all chat conversations (e.g., threads) with that user!
With checkpointers alone, we cannot share information across threads. This motivates the need for the Store interface. As an illustration, we can define an InMemoryStore to store information about a user across threads. We simply compile our graph with a checkpointer, as before, and with our new in_memory_store variable.
!!!  info "LangGraph API handles stores automatically"
When using the LangGraph API, you don't need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes.

Basic Usage¶
First, let's showcase this in isolation without using LangGraph.
from langgraph.store.memory import InMemoryStore
in_memory_store = InMemoryStore()

Memories are namespaced by a tuple, which in this specific example will be (<user_id>, "memories"). The namespace can be any length and represent anything, does not have to be user specific.
user_id = "1"
namespace_for_memory = (user_id, "memories")

We use the store.put method to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory (memory_id) and the value (a dictionary) is the memory itself.
memory_id = str(uuid.uuid4())
memory = {"food_preference" : "I like pizza"}
in_memory_store.put(namespace_for_memory, memory_id, memory)

We can read out memories in our namespace using the store.search method, which will return all memories for a given user as a list. The most recent memory is the last in the list.
memories = in_memory_store.search(namespace_for_memory)
memories[-1].dict()
{'value': {'food_preference': 'I like pizza'},
 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',
 'namespace': ['1', 'memories'],
 'created_at': '2024-10-02T17:22:31.590602+00:00',
 'updated_at': '2024-10-02T17:22:31.590605+00:00'}

Each memory type is a Python class (Item) with certain attributes. We can access it as a dictionary by converting via .dict as above.
The attributes it has are:

value: The value (itself a dictionary) of this memory
key: A unique key for this memory in this namespace
namespace: A list of strings, the namespace of this memory type
created_at: Timestamp for when this memory was created
updated_at: Timestamp for when this memory was updated

Semantic Search¶
Beyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model:
API Reference: init_embeddings
from langchain.embeddings import init_embeddings

store = InMemoryStore(
    index={
        "embed": init_embeddings("openai:text-embedding-3-small"),  # Embedding provider
        "dims": 1536,                              # Embedding dimensions
        "fields": ["food_preference", "$"]              # Fields to embed
    }
)

Now when searching, you can use natural language queries to find relevant memories:
# Find memories about food preferences
# (This can be done after putting memories into the store)
memories = store.search(
    namespace_for_memory,
    query="What does the user like to eat?",
    limit=3  # Return top 3 matches
)

You can control which parts of your memories get embedded by configuring the fields parameter or by specifying the index parameter when storing memories:
# Store with specific fields to embed
store.put(
    namespace_for_memory,
    str(uuid.uuid4()),
    {
        "food_preference": "I love Italian cuisine",
        "context": "Discussing dinner plans"
    },
    index=["food_preference"]  # Only embed "food_preferences" field
)

# Store without embedding (still retrievable, but not searchable)
store.put(
    namespace_for_memory,
    str(uuid.uuid4()),
    {"system_info": "Last updated: 2024-01-01"},
    index=False
)

Using in LangGraph¶
With this all in place, we use the in_memory_store in LangGraph. The in_memory_store works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the in_memory_store allows us to store arbitrary information for access across threads. We compile the graph with both the checkpointer and the in_memory_store as follows. 
API Reference: InMemorySaver
from langgraph.checkpoint.memory import InMemorySaver

# We need this because we want to enable threads (conversations)
checkpointer = InMemorySaver()

# ... Define the graph ...

# Compile the graph with the checkpointer and store
graph = graph.compile(checkpointer=checkpointer, store=in_memory_store)

We invoke the graph with a thread_id, as before, and also with a user_id, which we'll use to namespace our memories to this particular user as we showed above.
# Invoke the graph
user_id = "1"
config = {"configurable": {"thread_id": "1", "user_id": user_id}}

# First let's just say hi to the AI
for update in graph.stream(
    {"messages": [{"role": "user", "content": "hi"}]}, config, stream_mode="updates"
):
    print(update)

We can access the in_memory_store and the user_id in any node by passing store: BaseStore and config: RunnableConfig as node arguments. Here's how we might use semantic search in a node to find relevant memories:
def update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):

    # Get the user id from the config
    user_id = config["configurable"]["user_id"]

    # Namespace the memory
    namespace = (user_id, "memories")

    # ... Analyze conversation and create a new memory

    # Create a new memory ID
    memory_id = str(uuid.uuid4())

    # We create a new memory
    store.put(namespace, memory_id, {"memory": memory})

As we showed above, we can also access the store in any node and use the store.search method to get memories. Recall the the memories are returned as a list of objects that can be converted to a dictionary.
memories[-1].dict()
{'value': {'food_preference': 'I like pizza'},
 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',
 'namespace': ['1', 'memories'],
 'created_at': '2024-10-02T17:22:31.590602+00:00',
 'updated_at': '2024-10-02T17:22:31.590605+00:00'}

We can access the memories and use them in our model call.
def call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):
    # Get the user id from the config
    user_id = config["configurable"]["user_id"]

    # Namespace the memory
    namespace = (user_id, "memories")

    # Search based on the most recent message
    memories = store.search(
        namespace,
        query=state["messages"][-1].content,
        limit=3
    )
    info = "\n".join([d.value["memory"] for d in memories])

    # ... Use memories in the model call

If we create a new thread, we can still access the same memories so long as the user_id is the same. 
# Invoke the graph
config = {"configurable": {"thread_id": "2", "user_id": "1"}}

# Let's say hi again
for update in graph.stream(
    {"messages": [{"role": "user", "content": "hi, tell me about my memories"}]}, config, stream_mode="updates"
):
    print(update)

When we use the LangGraph Platform, either locally (e.g., in LangGraph Studio) or with LangGraph Cloud, the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you do need to configure the indexing settings in your langgraph.json file. For example:
{
    ...
    "store": {
        "index": {
            "embed": "openai:text-embeddings-3-small",
            "dims": 1536,
            "fields": ["$"]
        }
    }
}

See the deployment guide for more details and configuration options.
Checkpointer libraries¶
Under the hood, checkpointing is powered by checkpointer objects that conform to BaseCheckpointSaver interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:

langgraph-checkpoint: The base interface for checkpointer savers (BaseCheckpointSaver) and serialization/deserialization interface (SerializerProtocol). Includes in-memory checkpointer implementation (InMemorySaver) for experimentation. LangGraph comes with langgraph-checkpoint included.
langgraph-checkpoint-sqlite: An implementation of LangGraph checkpointer that uses SQLite database (SqliteSaver / AsyncSqliteSaver). Ideal for experimentation and local workflows. Needs to be installed separately.
langgraph-checkpoint-postgres: An advanced checkpointer that uses Postgres database (PostgresSaver / AsyncPostgresSaver), used in LangGraph Cloud. Ideal for using in production. Needs to be installed separately.

Checkpointer interface¶
Each checkpointer conforms to BaseCheckpointSaver interface and implements the following methods:

.put - Store a checkpoint with its configuration and metadata.  
.put_writes - Store intermediate writes linked to a checkpoint (i.e. pending writes).  
.get_tuple - Fetch a checkpoint tuple using for a given configuration (thread_id and checkpoint_id). This is used to populate StateSnapshot in graph.get_state().  
.list - List checkpoints that match a given configuration and filter criteria. This is used to populate state history in graph.get_state_history()

If the checkpointer is used with asynchronous graph execution (i.e. executing the graph via .ainvoke, .astream, .abatch), asynchronous versions of the above methods will be used (.aput, .aput_writes, .aget_tuple, .alist).

Note
For running your graph asynchronously, you can use InMemorySaver, or async versions of Sqlite/Postgres checkpointers -- AsyncSqliteSaver / AsyncPostgresSaver checkpointers.

Serializer¶
When checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects. 
langgraph_checkpoint defines protocol for implementing serializers provides a default implementation (JsonPlusSerializer) that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more.
Capabilities¶
Human-in-the-loop¶
First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. See these how-to guides for concrete examples.
Memory¶
Second, checkpointers allow for "memory" between interactions.  In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See this how-to guide for an end-to-end example on how to add and manage conversation memory using checkpointers.
Time Travel¶
Third, checkpointers allow for "time travel", allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories.
Fault-tolerance¶
Lastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes.
Pending writes¶
Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 23
SOURCE: https://langchain-ai.github.io/langgraph/concepts/durable_execution/
CONTENT:
Durable Execution¶
Durable execution is a technique in which a process or workflow saves its progress at key points, allowing it to pause and later resume exactly where it left off. This is particularly useful in scenarios that require human-in-the-loop, where users can inspect, validate, or modify the process before continuing, and in long-running tasks that might encounter interruptions or errors (e.g., calls to an LLM timing out). By preserving completed work, durable execution enables a process to resume without reprocessing previous steps -- even after a significant delay (e.g., a week later). 
LangGraph's built-in persistence layer provides durable execution for workflows, ensuring that the state of each execution step is saved to a durable store. This capability guarantees that if a workflow is interrupted -- whether by a system failure or for human-in-the-loop interactions -- it can be resumed from its last recorded state.

Tip
If you are using LangGraph with a checkpointer, you already have durable execution enabled. You can pause and resume workflows at any point, even after interruptions or failures.
To make the most of durable execution, ensure that your workflow is designed to be deterministic and idempotent and wrap any side effects or non-deterministic operations inside tasks. You can use tasks from both the StateGraph (Graph API) and the Functional API.

Requirements¶
To leverage durable execution in LangGraph, you need to:

Enable persistence in your workflow by specifying a checkpointer that will save workflow progress.
Specify a thread identifier when executing a workflow. This will track the execution history for a particular instance of the workflow.
Wrap any non-deterministic operations (e.g., random number generation) or operations with side effects (e.g., file writes, API calls) inside tasks to ensure that when a workflow is resumed, these operations are not repeated for the particular run, and instead their results are retrieved from the persistence layer. For more information, see Determinism and Consistent Replay.

Determinism and Consistent Replay¶
When you resume a workflow run, the code does NOT resume from the same line of code where execution stopped; instead, it will identify an appropriate starting point from which to pick up where it left off. This means that the workflow will replay all steps from the starting point until it reaches the point where it was stopped.
As a result, when you are writing a workflow for durable execution, you must wrap any non-deterministic operations (e.g., random number generation) and any operations with side effects (e.g., file writes, API calls) inside tasks or nodes.
To ensure that your workflow is deterministic and can be consistently replayed, follow these guidelines:

Avoid Repeating Work:  If a node contains multiple operations with side effects (e.g., logging, file writes, or network calls), wrap each operation in a separate task. This ensures that when the workflow is resumed, the operations are not repeated, and their results are retrieved from the persistence layer.
Encapsulate Non-Deterministic Operations:  Wrap any code that might yield non-deterministic results (e.g., random number generation) inside tasks or nodes. This ensures that, upon resumption, the workflow follows the exact recorded sequence of steps with the same outcomes.
Use Idempotent Operations: When possible ensure that side effects (e.g., API calls, file writes) are idempotent. This means that if an operation is retried after a failure in the workflow, it will have the same effect as the first time it was executed. This is particularly important for operations that result in data writes. In the event that a task starts but fails to complete successfully, the workflow's resumption will re-run the task, relying on recorded outcomes to maintain consistency. Use idempotency keys or verify existing results to avoid unintended duplication, ensuring a smooth and predictable workflow execution.

For some examples of pitfalls to avoid, see the Common Pitfalls section in the functional API, which shows
how to structure your code using tasks to avoid these issues. The same principles apply to the StateGraph (Graph API).
Using tasks in nodes¶
If a node contains multiple operations, you may find it easier to convert each operation into a task rather than refactor the operations into individual nodes.
OriginalWith task

from typing import NotRequired
from typing_extensions import TypedDict
import uuid

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, END
import requests

# Define a TypedDict to represent the state
class State(TypedDict):
    url: str
    result: NotRequired[str]

def call_api(state: State):
    """Example node that makes an API request."""
    result = requests.get(state['url']).text[:100]  # Side-effect
    return {
        "result": result
    }

# Create a StateGraph builder and add a node for the call_api function
builder = StateGraph(State)
builder.add_node("call_api", call_api)

# Connect the start and end nodes to the call_api node
builder.add_edge(START, "call_api")
builder.add_edge("call_api", END)

# Specify a checkpointer
checkpointer = MemorySaver()

# Compile the graph with the checkpointer
graph = builder.compile(checkpointer=checkpointer)

# Define a config with a thread ID.
thread_id = uuid.uuid4()
config = {"configurable": {"thread_id": thread_id}}

# Invoke the graph
graph.invoke({"url": "https://www.example.com"}, config)

from typing import NotRequired
from typing_extensions import TypedDict
import uuid

from langgraph.checkpoint.memory import MemorySaver
from langgraph.func import task
from langgraph.graph import StateGraph, START, END
import requests

# Define a TypedDict to represent the state
class State(TypedDict):
    urls: list[str]
    result: NotRequired[list[str]]

@task
def _make_request(url: str):
    """Make a request."""
    return requests.get(url).text[:100]

def call_api(state: State):
    """Example node that makes an API request."""
    requests = [_make_request(url) for url in state['urls']]
    results = [request.result() for request in requests]
    return {
        "results": results
    }

# Create a StateGraph builder and add a node for the call_api function
builder = StateGraph(State)
builder.add_node("call_api", call_api)

# Connect the start and end nodes to the call_api node
builder.add_edge(START, "call_api")
builder.add_edge("call_api", END)

# Specify a checkpointer
checkpointer = MemorySaver()

# Compile the graph with the checkpointer
graph = builder.compile(checkpointer=checkpointer)

# Define a config with a thread ID.
thread_id = uuid.uuid4()
config = {"configurable": {"thread_id": thread_id}}

# Invoke the graph
graph.invoke({"urls": ["https://www.example.com"]}, config)

Resuming Workflows¶
Once you have enabled durable execution in your workflow, you can resume execution for the following scenarios:

Pausing and Resuming Workflows: Use the interrupt function to pause a workflow at specific points and the Command primitive to resume it with updated state. See Human-in-the-Loop for more details.
Recovering from Failures: Automatically resume workflows from the last successful checkpoint after an exception (e.g., LLM provider outage). This involves executing the workflow with the same thread identifier by providing it with a None as the input value (see this example with the functional API).

Starting Points for Resuming Workflows¶

If you're using a StateGraph (Graph API), the starting point is the beginning of the node where execution stopped. 
If you're making a subgraph call inside a node, the starting point will be the parent node that called the subgraph that was halted.
Inside the subgraph, the starting point will be the specific node where execution stopped.
If you're using the Functional API, the starting point is the beginning of the entrypoint where execution stopped.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 24
SOURCE: https://langchain-ai.github.io/langgraph/concepts/v0-human-in-the-loop/
CONTENT:
Human-in-the-loop¶

Use the interrupt function instead.
As of LangGraph 0.2.57, the recommended way to set breakpoints is using the interrupt function as it simplifies human-in-the-loop patterns.
Please see the revised human-in-the-loop guide for the latest version that uses the interrupt function.

Human-in-the-loop (or "on-the-loop") enhances agent capabilities through several common user interaction patterns.
Common interaction patterns include:
(1) Approval - We can interrupt our agent, surface the current state to a user, and allow the user to accept an action. 
(2) Editing - We can interrupt our agent, surface the current state to a user, and allow the user to edit the agent state. 
(3) Input - We can explicitly create a graph node to collect human input and pass that input directly to the agent state.
Use-cases for these interaction patterns include:
(1) Reviewing tool calls - We can interrupt an agent to review and edit the results of tool calls.
(2) Time Travel - We can manually re-play and / or fork past actions of an agent.
Persistence¶
All of these interaction patterns are enabled by LangGraph's built-in persistence layer, which will write a checkpoint of the graph state at each step. Persistence allows the graph to stop so that a human can review and / or edit the current state of the graph and then resume with the human's input.
Breakpoints¶
Adding a breakpoint a specific location in the graph flow is one way to enable human-in-the-loop. In this case, the developer knows where in the workflow human input is needed and simply places a breakpoint prior to or following that particular graph node.
Here, we compile our graph with a checkpointer and a breakpoint at the node we want to interrupt before, step_for_human_in_the_loop. We then perform one of the above interaction patterns, which will create a new checkpoint if a human edits the graph state. The new checkpoint is saved to the thread and we can resume the graph execution from there by passing in None as the input.
# Compile our graph with a checkpointer and a breakpoint before "step_for_human_in_the_loop"
graph = builder.compile(checkpointer=checkpointer, interrupt_before=["step_for_human_in_the_loop"])

# Run the graph up to the breakpoint
thread_config = {"configurable": {"thread_id": "1"}}
for event in graph.stream(inputs, thread_config, stream_mode="values"):
    print(event)

# Perform some action that requires human in the loop

# Continue the graph execution from the current checkpoint 
for event in graph.stream(None, thread_config, stream_mode="values"):
    print(event)

Dynamic Breakpoints¶
Alternatively, the developer can define some condition that must be met for a breakpoint to be triggered. This concept of dynamic breakpoints is useful when the developer wants to halt the graph under a particular condition. This uses a NodeInterrupt, which is a special type of exception that can be raised from within a node based upon some condition. As an example, we can define a dynamic breakpoint that triggers when the input is longer than 5 characters.
def my_node(state: State) -> State:
    if len(state['input']) > 5:
        raise NodeInterrupt(f"Received input that is longer than 5 characters: {state['input']}")
    return state

Let's assume we run the graph with an input that triggers the dynamic breakpoint and then attempt to resume the graph execution simply by passing in None for the input. 
# Attempt to continue the graph execution with no change to state after we hit the dynamic breakpoint 
for event in graph.stream(None, thread_config, stream_mode="values"):
    print(event)

The graph will interrupt again because this node will be re-run with the same graph state. We need to change the graph state such that the condition that triggers the dynamic breakpoint is no longer met. So, we can simply edit the graph state to an input that meets the condition of our dynamic breakpoint (< 5 characters) and re-run the node.
# Update the state to pass the dynamic breakpoint
graph.update_state(config=thread_config, values={"input": "foo"})
for event in graph.stream(None, thread_config, stream_mode="values"):
    print(event)

Alternatively, what if we want to keep our current input and skip the node (my_node) that performs the check? To do this, we can simply perform the graph update with as_node="my_node" and pass in None for the values. This will make no update the graph state, but run the update as my_node, effectively skipping the node and bypassing the dynamic breakpoint.
# This update will skip the node `my_node` altogether
graph.update_state(config=thread_config, values=None, as_node="my_node")
for event in graph.stream(None, thread_config, stream_mode="values"):
    print(event)

See our guide for a detailed how-to on doing this!
Interaction Patterns¶
Approval¶

Sometimes we want to approve certain steps in our agent's execution. 
We can interrupt our agent at a breakpoint prior to the step that we want to approve.
This is generally recommend for sensitive actions (e.g., using external APIs or writing to a database).
With persistence, we can surface the current agent state as well as the next step to a user for review and approval. 
If approved, the graph resumes execution from the last saved checkpoint, which is saved to the thread:
# Compile our graph with a checkpointer and a breakpoint before the step to approve
graph = builder.compile(checkpointer=checkpointer, interrupt_before=["node_2"])

# Run the graph up to the breakpoint
for event in graph.stream(inputs, thread, stream_mode="values"):
    print(event)

# ... Get human approval ...

# If approved, continue the graph execution from the last saved checkpoint
for event in graph.stream(None, thread, stream_mode="values"):
    print(event)

See our guide for a detailed how-to on doing this!
Editing¶

Sometimes we want to review and edit the agent's state. 
As with approval, we can interrupt our agent at a breakpoint prior to the step we want to check. 
We can surface the current state to a user and allow the user to edit the agent state.
This can, for example, be used to correct the agent if it made a mistake (e.g., see the section on tool calling below).
We can edit the graph state by forking the current checkpoint, which is saved to the thread.
We can then proceed with the graph from our forked checkpoint as done before. 
# Compile our graph with a checkpointer and a breakpoint before the step to review
graph = builder.compile(checkpointer=checkpointer, interrupt_before=["node_2"])

# Run the graph up to the breakpoint
for event in graph.stream(inputs, thread, stream_mode="values"):
    print(event)

# Review the state, decide to edit it, and create a forked checkpoint with the new state
graph.update_state(thread, {"state": "new state"})

# Continue the graph execution from the forked checkpoint
for event in graph.stream(None, thread, stream_mode="values"):
    print(event)

See this guide for a detailed how-to on doing this!
Input¶

Sometimes we want to explicitly get human input at a particular step in the graph. 
We can create a graph node designated for this (e.g., human_input in our example diagram).
As with approval and editing, we can interrupt our agent at a breakpoint prior to this node.
We can then perform a state update that includes the human input, just as we did with editing state.
But, we add one thing: 
We can use as_node=human_input with the state update to specify that the state update should be treated as a node.
The is subtle, but important: 
With editing, the user makes a decision about whether or not to edit the graph state.
With input, we explicitly define a node in our graph for collecting human input!
The state update with the human input then runs as this node.
# Compile our graph with a checkpointer and a breakpoint before the step to to collect human input
graph = builder.compile(checkpointer=checkpointer, interrupt_before=["human_input"])

# Run the graph up to the breakpoint
for event in graph.stream(inputs, thread, stream_mode="values"):
    print(event)

# Update the state with the user input as if it was the human_input node
graph.update_state(thread, {"user_input": user_input}, as_node="human_input")

# Continue the graph execution from the checkpoint created by the human_input node
for event in graph.stream(None, thread, stream_mode="values"):
    print(event)

See this guide for a detailed how-to on doing this!
Use-cases¶
Reviewing Tool Calls¶
Some user interaction patterns combine the above ideas.
For example, many agents use tool calling to make decisions. 
Tool calling presents a challenge because the agent must get two things right: 
(1) The name of the tool to call 
(2) The arguments to pass to the tool
Even if the tool call is correct, we may also want to apply discretion: 
(3) The tool call may be a sensitive operation that we want to approve 
With these points in mind, we can combine the above ideas to create a human-in-the-loop review of a tool call.
# Compile our graph with a checkpointer and a breakpoint before the step to to review the tool call from the LLM 
graph = builder.compile(checkpointer=checkpointer, interrupt_before=["human_review"])

# Run the graph up to the breakpoint
for event in graph.stream(inputs, thread, stream_mode="values"):
    print(event)

# Review the tool call and update it, if needed, as the human_review node
graph.update_state(thread, {"tool_call": "updated tool call"}, as_node="human_review")

# Otherwise, approve the tool call and proceed with the graph execution with no edits 

# Continue the graph execution from either: 
# (1) the forked checkpoint created by human_review or 
# (2) the checkpoint saved when the tool call was originally made (no edits in human_review)
for event in graph.stream(None, thread, stream_mode="values"):
    print(event)

See this guide for a detailed how-to on doing this!
Time Travel¶
When working with agents, we often want closely examine their decision making process: 
(1) Even when they arrive a desired final result, the reasoning that led to that result is often important to examine.
(2) When agents make mistakes, it is often valuable to understand why.
(3) In either of the above cases, it is useful to manually explore alternative decision making paths.
Collectively, we call these debugging concepts time-travel and they are composed of replaying and forking.
Replaying¶

Sometimes we want to simply replay past actions of an agent. 
Above, we showed the case of executing an agent from the current state (or checkpoint) of the graph.
We by simply passing in None for the input with a thread.
thread = {"configurable": {"thread_id": "1"}}
for event in graph.stream(None, thread, stream_mode="values"):
    print(event)

Now, we can modify this to replay past actions from a specific checkpoint by passing in the checkpoint ID.
To get a specific checkpoint ID, we can easily get all of the checkpoints in the thread and filter to the one we want.
all_checkpoints = []
for state in app.get_state_history(thread):
    all_checkpoints.append(state)

Each checkpoint has a unique ID, which we can use to replay from a specific checkpoint.
Assume from reviewing the checkpoints that we want to replay from one, xxx.
We just pass in the checkpoint ID when we run the graph.
config = {'configurable': {'thread_id': '1', 'checkpoint_id': 'xxx'}}
for event in graph.stream(None, config, stream_mode="values"):
    print(event)

Importantly, the graph knows which checkpoints have been previously executed. 
So, it will re-play any previously executed nodes rather than re-executing them.
See this additional conceptual guide for related context on replaying.
See see this guide for a detailed how-to on doing time-travel!
Forking¶

Sometimes we want to fork past actions of an agent, and explore different paths through the graph.
Editing, as discussed above, is exactly how we do this for the current state of the graph! 
But, what if we want to fork past states of the graph?
For example, let's say we want to edit a particular checkpoint, xxx.
We pass this checkpoint_id when we update the state of the graph.
config = {"configurable": {"thread_id": "1", "checkpoint_id": "xxx"}}
graph.update_state(config, {"state": "updated state"}, )

This creates a new forked checkpoint, xxx-fork, which we can then run the graph from.
config = {'configurable': {'thread_id': '1', 'checkpoint_id': 'xxx-fork'}}
for event in graph.stream(None, config, stream_mode="values"):
    print(event)

See this additional conceptual guide for related context on forking.
See this guide for a detailed how-to on doing time-travel!

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 25
SOURCE: https://langchain-ai.github.io/langgraph/concepts/breakpoints/
CONTENT:
Breakpoints¶
Breakpoints pause graph execution at specific points and enable stepping through execution step by step. Breakpoints are powered by LangGraph's persistence layer, which saves the state after each graph step. Breakpoints can also be used to enable human-in-the-loop workflows, though we recommend using the interrupt function for this purpose.
Requirements¶
To use breakpoints, you will need to:

Specify a checkpointer to save the graph state after each step.
Set breakpoints to specify where execution should pause.
Run the graph with a thread ID to pause execution at the breakpoint.
Resume execution using invoke/ainvoke/stream/astream (see The Command primitive).

Setting breakpoints¶
There are two places where you can set breakpoints:

Before or after a node executes by setting breakpoints at compile time or run time. We call these static breakpoints.
Inside a node using the NodeInterrupt exception.

Static breakpoints¶
Static breakpoints are triggered either before or after a node executes. You can set static breakpoints by specifying interrupt_before and interrupt_after at "compile" time or run time.
Compile timeRun time

graph = graph_builder.compile(
    interrupt_before=["node_a"], 
    interrupt_after=["node_b", "node_c"],
    checkpointer=..., # Specify a checkpointer
)

thread_config = {
    "configurable": {
        "thread_id": "some_thread"
    }
}

# Run the graph until the breakpoint
graph.invoke(inputs, config=thread_config)

# Optionally update the graph state based on user input
graph.update_state(update, config=thread_config)

# Resume the graph
graph.invoke(None, config=thread_config)

graph.invoke(
    inputs, 
    config={"configurable": {"thread_id": "some_thread"}}, 
    interrupt_before=["node_a"], 
    interrupt_after=["node_b", "node_c"]
)

thread_config = {
    "configurable": {
        "thread_id": "some_thread"
    }
}

# Run the graph until the breakpoint
graph.invoke(inputs, config=thread_config)

# Optionally update the graph state based on user input
graph.update_state(update, config=thread_config)

# Resume the graph
graph.invoke(None, config=thread_config)

Note
You cannot set static breakpoints at runtime for sub-graphs.
If you have a sub-graph, you must set the breakpoints at compilation time.

Static breakpoints can be especially useful for debugging if you want to step through the graph execution one
node at a time or if you want to pause the graph execution at specific nodes.
NodeInterrupt exception¶
We recommend that you use the interrupt function instead of the NodeInterrupt exception if you're trying to implement
human-in-the-loop workflows. The interrupt function is easier to use and more flexible.

NodeInterrupt exception
The developer can define some condition that must be met for a breakpoint to be triggered. This concept of dynamic breakpoints is useful when the developer wants to halt the graph under a particular condition. This uses a NodeInterrupt, which is a special type of exception that can be raised from within a node based upon some condition. As an example, we can define a dynamic breakpoint that triggers when the input is longer than 5 characters.
def my_node(state: State) -> State:
    if len(state['input']) > 5:
        raise NodeInterrupt(f"Received input that is longer than 5 characters: {state['input']}")

    return state

Let's assume we run the graph with an input that triggers the dynamic breakpoint and then attempt to resume the graph execution simply by passing in None for the input.
# Attempt to continue the graph execution with no change to state after we hit the dynamic breakpoint 
for event in graph.stream(None, thread_config, stream_mode="values"):
    print(event)

The graph will interrupt again because this node will be re-run with the same graph state. We need to change the graph state such that the condition that triggers the dynamic breakpoint is no longer met. So, we can simply edit the graph state to an input that meets the condition of our dynamic breakpoint (< 5 characters) and re-run the node.
# Update the state to pass the dynamic breakpoint
graph.update_state(config=thread_config, values={"input": "foo"})
for event in graph.stream(None, thread_config, stream_mode="values"):
    print(event)

Alternatively, what if we want to keep our current input and skip the node (my_node) that performs the check? To do this, we can simply perform the graph update with as_node="my_node" and pass in None for the values. This will make no update the graph state, but run the update as my_node, effectively skipping the node and bypassing the dynamic breakpoint.
# This update will skip the node `my_node` altogether
graph.update_state(config=thread_config, values=None, as_node="my_node")
for event in graph.stream(None, thread_config, stream_mode="values"):
    print(event)

Additional Resources üìö¶

Conceptual Guide: Persistence: Read the persistence guide for more context about persistence.
Conceptual Guide: Human-in-the-loop: Read the human-in-the-loop guide for more context on integrating human feedback into LangGraph applications using breakpoints.
How to View and Update Past Graph State: Step-by-step instructions for working with graph state that demonstrate the replay and fork actions.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 26
SOURCE: https://langchain-ai.github.io/langgraph/concepts/memory/
CONTENT:
Memory¶
What is Memory?¶
Memory is a cognitive function that allows people to store, retrieve, and use information to understand their present and future. Consider the frustration of working with a colleague who forgets everything you tell them, requiring constant repetition! As AI agents undertake more complex tasks involving numerous user interactions, equipping them with memory becomes equally crucial for efficiency and user satisfaction. With memory, agents can learn from feedback and adapt to users' preferences. This guide covers two types of memory based on recall scope:
Short-term memory, or thread-scoped memory, can be recalled at any time from within a single conversational thread with a user. LangGraph manages short-term memory as a part of your agent's state. State is persisted to a database using a checkpointer so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step.
Long-term memory is shared across conversational threads. It can be recalled at any time and in any thread. Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides stores (reference doc) to let you save and recall long-term memories.
Both are important to understand and implement for your application.

Short-term memory¶
Short-term memory lets your application remember previous interactions within a single thread or conversation. A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.
LangGraph manages short-term memory as part of the agent's state, persisted via thread-scoped checkpoints. This state can normally include the conversation history along with other stateful data, such as uploaded files, retrieved documents, or generated artifacts. By storing these in the graph's state, the bot can access the full context for a given conversation while maintaining separation between different threads.
Since conversation history is the most common form of representing short-term memory, in the next section, we will cover techniques for managing conversation history when the list of messages becomes long. If you want to stick to the high-level concepts, continue on to the long-term memory section.
Managing long conversation history¶
Long conversations pose a challenge to today's LLMs. The full history may not even fit inside an LLM's context window, resulting in an irrecoverable error. Even if your LLM technically supports the full context length, most LLMs still perform poorly over long contexts. They get "distracted" by stale or off-topic content, all while suffering from slower response times and higher costs.
Managing short-term memory is an exercise of balancing precision & recall with your application's other performance requirements (latency & cost). As always, it's important to think critically about how you represent information for your LLM and to look at your data. We cover a few common techniques for managing message lists below and hope to provide sufficient context for you to pick the best tradeoffs for your application:

Editing message lists: How to think about trimming and filtering a list of messages before passing to language model.
Summarizing past conversations: A common technique to use when you don't just want to filter the list of messages.

Editing message lists¶
Chat models accept context using messages, which include developer provided instructions (a system message) and user inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited and token-rich message lists can be costly, many applications can benefit from using techniques to manually remove or forget stale information.

The most direct approach is to remove old messages from a list (similar to a least-recently used cache).
The typical technique for deleting content from a list in LangGraph is to return an update from a node telling the system to delete some portion of the list. You get to define what this update looks like, but a common approach would be to let you return an object or dictionary specifying which values to retain.
def manage_list(existing: list, updates: Union[list, dict]):
    if isinstance(updates, list):
        # Normal case, add to the history
        return existing + updates
    elif isinstance(updates, dict) and updates["type"] == "keep":
        # You get to decide what this looks like.
        # For example, you could simplify and just accept a string "DELETE"
        # and clear the entire list.
        return existing[updates["from"]:updates["to"]]
    # etc. We define how to interpret updates

class State(TypedDict):
    my_list: Annotated[list, manage_list]

def my_node(state: State):
    return {
        # We return an update for the field "my_list" saying to
        # keep only values from index -5 to the end (deleting the rest)
        "my_list": {"type": "keep", "from": -5, "to": None}
    }

LangGraph will call the manage_list "reducer" function any time an update is returned under the key "my_list". Within that function, we define what types of updates to accept. Typically, messages will be added to the existing list (the conversation will grow); however, we've also added support to accept a dictionary that lets you "keep" certain parts of the state. This lets you programmatically drop old message context.
Another common approach is to let you return a list of "remove" objects that specify the IDs of all messages to delete. If you're using the LangChain messages and the add_messages reducer (or MessagesState, which uses the same underlying functionality) in LangGraph, you can do this using a RemoveMessage.
API Reference: RemoveMessage | AIMessage | add_messages
from langchain_core.messages import RemoveMessage, AIMessage
from langgraph.graph import add_messages
# ... other imports

class State(TypedDict):
    # add_messages will default to upserting messages by ID to the existing list
    # if a RemoveMessage is returned, it will delete the message in the list by ID
    messages: Annotated[list, add_messages]

def my_node_1(state: State):
    # Add an AI message to the `messages` list in the state
    return {"messages": [AIMessage(content="Hi")]}

def my_node_2(state: State):
    # Delete all but the last 2 messages from the `messages` list in the state
    delete_messages = [RemoveMessage(id=m.id) for m in state['messages'][:-2]]
    return {"messages": delete_messages}

In the example above, the add_messages reducer allows us to append new messages to the messages state key as shown in my_node_1. When it sees a RemoveMessage, it will delete the message with that ID from the list (and the RemoveMessage will then be discarded). For more information on LangChain-specific message handling, check out this how-to on using RemoveMessage .
See this how-to guide and module 2 from our LangChain Academy course for example usage.
Summarizing past conversations¶
The problem with trimming or removing messages, as shown above, is that we may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.

Simple prompting and orchestration logic can be used to achieve this. As an example, in LangGraph we can extend the MessagesState to include a summary key.
from langgraph.graph import MessagesState
class State(MessagesState):
    summary: str

Then, we can generate a summary of the chat history, using any existing summary as context for the next summary. This summarize_conversation node can be called after some number of messages have accumulated in the messages state key.
def summarize_conversation(state: State):

    # First, we get any existing summary
    summary = state.get("summary", "")

    # Create our summarization prompt
    if summary:

        # A summary already exists
        summary_message = (
            f"This is a summary of the conversation to date: {summary}\n\n"
            "Extend the summary by taking into account the new messages above:"
        )

    else:
        summary_message = "Create a summary of the conversation above:"

    # Add prompt to our history
    messages = state["messages"] + [HumanMessage(content=summary_message)]
    response = model.invoke(messages)

    # Delete all but the 2 most recent messages
    delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]
    return {"summary": response.content, "messages": delete_messages}

See this how-to here and module 2 from our LangChain Academy course for example usage.
Knowing when to remove messages¶
Most LLMs have a maximum supported context window (denominated in tokens). A simple way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. Naive truncation is straightforward to implement on your own, though there are a few "gotchas". Some model APIs further restrict the sequence of message types (must start with human message, cannot have consecutive messages of the same type, etc.). If you're using LangChain, you can use the trim_messages utility and specify the number of tokens to keep from the list, as well as the strategy (e.g., keep the last max_tokens) to use for handling the boundary.
Below is an example.
API Reference: trim_messages
from langchain_core.messages import trim_messages
trim_messages(
    messages,
    # Keep the last <= n_count tokens of the messages.
    strategy="last",
    # Remember to adjust based on your model
    # or else pass a custom token_encoder
    token_counter=ChatOpenAI(model="gpt-4"),
    # Remember to adjust based on the desired conversation
    # length
    max_tokens=45,
    # Most chat models expect that chat history starts with either:
    # (1) a HumanMessage or
    # (2) a SystemMessage followed by a HumanMessage
    start_on="human",
    # Most chat models expect that chat history ends with either:
    # (1) a HumanMessage or
    # (2) a ToolMessage
    end_on=("human", "tool"),
    # Usually, we want to keep the SystemMessage
    # if it's present in the original history.
    # The SystemMessage has special instructions for the model.
    include_system=True,
)

Long-term memory¶
Long-term memory in LangGraph allows systems to retain information across different conversations or sessions. Unlike short-term memory, which is thread-scoped, long-term memory is saved within custom "namespaces."
Storing memories¶
LangGraph stores long-term memories as JSON documents in a store (reference doc). Each memory is organized under a custom namespace (similar to a folder) and a distinct key (like a filename). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters. See the example below for an example.
from langgraph.store.memory import InMemoryStore

def embed(texts: list[str]) -> list[list[float]]:
    # Replace with an actual embedding function or LangChain embeddings object
    return [[1.0, 2.0] * len(texts)]

# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.
store = InMemoryStore(index={"embed": embed, "dims": 2})
user_id = "my-user"
application_context = "chitchat"
namespace = (user_id, application_context)
store.put(
    namespace,
    "a-memory",
    {
        "rules": [
            "User likes short, direct language",
            "User only speaks English & python",
        ],
        "my-key": "my-value",
    },
)
# get the "memory" by ID
item = store.get(namespace, "a-memory")
# search for "memories" within this namespace, filtering on content equivalence, sorted by vector similarity
items = store.search(
    namespace, filter={"my-key": "my-value"}, query="language preferences"
)

Framework for thinking about long-term memory¶
Long-term memory is a complex challenge without a one-size-fits-all solution. However, the following questions provide a structure framework to help you navigate the different techniques:
What is the type of memory?
Humans use memories to remember facts, experiences, and rules. AI agents can use memory in the same ways. For example, AI agents can use memory to remember specific facts about a user to accomplish a task. We expand on several types of memories in the section below.
When do you want to update memories?
Memory can be updated as part of an agent's application logic (e.g. "on the hot path"). In this case, the agent typically decides to remember facts before responding to a user. Alternatively, memory can be updated as a background task (logic that runs in the background / asynchronously and generates memories). We explain the tradeoffs between these approaches in the section below.
Memory types¶
Different applications require various types of memory. Although the analogy isn't perfect, examining human memory types can be insightful. Some research (e.g., the CoALA paper) have even mapped these human memory types to those used in AI agents.

Memory Type
What is Stored
Human Example
Agent Example

Semantic
Facts
Things I learned in school
Facts about a user

Episodic
Experiences
Things I did
Past agent actions

Procedural
Instructions
Instincts or motor skills
Agent system prompt

Semantic Memory¶
Semantic memory, both in humans and AI agents, involves the retention of specific facts and concepts. In humans, it can include information learned in school and the understanding of concepts and their relationships. For AI agents, semantic memory is often used to personalize applications by remembering facts or concepts from past interactions. 

Note: Not to be confused with "semantic search" which is a technique for finding similar content using "meaning" (usually as embeddings). Semantic memory is a term from psychology, referring to storing facts and knowledge, while semantic search is a method for retrieving information based on meaning rather than exact matches.

Profile¶
Semantic memories can be managed in different ways. For example, memories can be a single, continuously updated "profile" of well-scoped and specific information about a user, organization, or other entity (including the agent itself). A profile is generally just a JSON document with various key-value pairs you've selected to represent your domain. 
When remembering a profile, you will want to make sure that you are updating the profile each time. As a result, you will want to pass in the previous profile and ask the model to generate a new profile (or some JSON patch to apply to the old profile). This can be become error-prone as the profile gets larger, and may benefit from splitting a profile into multiple documents or strict decoding when generating documents to ensure the memory schemas remains valid.

Collection¶
Alternatively, memories can be a collection of documents that are continuously updated and extended over time. Each individual memory can be more narrowly scoped and easier to generate, which means that you're less likely to lose information over time. It's easier for an LLM to generate new objects for new information than reconcile new information with an existing profile. As a result, a document collection tends to lead to higher recall downstream.
However, this shifts some complexity memory updating. The model must now delete or update existing items in the list, which can be tricky. In addition, some models may default to over-inserting and others may default to over-updating. See the Trustcall package for one way to manage this and consider evaluation (e.g., with a tool like LangSmith) to help you tune the behavior.
Working with document collections also shifts complexity to memory search over the list. The Store currently supports both semantic search and filtering by content.
Finally, using a collection of memories can make it challenging to provide comprehensive context to the model. While individual memories may follow a specific schema, this structure might not capture the full context or relationships between memories. As a result, when using these memories to generate responses, the model may lack important contextual information that would be more readily available in a unified profile approach.

Regardless of memory management approach, the central point is that the agent will use the semantic memories to ground its responses, which often leads to more personalized and relevant interactions.
Episodic Memory¶
Episodic memory, in both humans and AI agents, involves recalling past events or actions. The CoALA paper frames this well: facts can be written to semantic memory, whereas experiences can be written to episodic memory. For AI agents, episodic memory is often used to help an agent remember how to accomplish a task. 
In practice, episodic memories are often implemented through few-shot example prompting, where agents learn from past sequences to perform tasks correctly. Sometimes it's easier to "show" than "tell" and LLMs learn well from examples. Few-shot learning lets you "program" your LLM by updating the prompt with input-output examples to illustrate the intended behavior. While various best-practices can be used to generate few-shot examples, often the challenge lies in selecting the most relevant examples based on user input.
Note that the memory store is just one way to store data as few-shot examples. If you want to have more developer involvement, or tie few-shots more closely to your evaluation harness, you can also use a LangSmith Dataset to store your data. Then dynamic few-shot example selectors can be used out-of-the box to achieve this same goal. LangSmith will index the dataset for you and enable retrieval of few shot examples that are most relevant to the user input based upon keyword similarity (using a BM25-like algorithm for keyword based similarity). 
See this how-to video for example usage of dynamic few-shot example selection in LangSmith. Also, see this blog post showcasing few-shot prompting to improve tool calling performance and this blog post using few-shot example to align an LLMs to human preferences.
Procedural Memory¶
Procedural memory, in both humans and AI agents, involves remembering the rules used to perform tasks. In humans, procedural memory is like the internalized knowledge of how to perform tasks, such as riding a bike via basic motor skills and balance. Episodic memory, on the other hand, involves recalling specific experiences, such as the first time you successfully rode a bike without training wheels or a memorable bike ride through a scenic route. For AI agents, procedural memory is a combination of model weights, agent code, and agent's prompt that collectively determine the agent's functionality. 
In practice, it is fairly uncommon for agents to modify their model weights or rewrite their code. However, it is more common for agents to modify their own prompts. 
One effective approach to refining an agent's instructions is through "Reflection" or meta-prompting. This involves prompting the agent with its current instructions (e.g., the system prompt) along with recent conversations or explicit user feedback. The agent then refines its own instructions based on this input. This method is particularly useful for tasks where instructions are challenging to specify upfront, as it allows the agent to learn and adapt from its interactions.
For example, we built a Tweet generator using external feedback and prompt re-writing to produce high-quality paper summaries for Twitter. In this case, the specific summarization prompt was difficult to specify a priori, but it was fairly easy for a user to critique the generated Tweets and provide feedback on how to improve the summarization process. 
The below pseudo-code shows how you might implement this with the LangGraph memory store, using the store to save a prompt, the update_instructions node to get the current prompt (as well as feedback from the conversation with the user captured in state["messages"]), update the prompt, and save the new prompt back to the store. Then, the call_model get the updated prompt from the store and uses it to generate a response.
# Node that *uses* the instructions
def call_model(state: State, store: BaseStore):
    namespace = ("agent_instructions", )
    instructions = store.get(namespace, key="agent_a")[0]
    # Application logic
    prompt = prompt_template.format(instructions=instructions.value["instructions"])
    ...

# Node that updates instructions
def update_instructions(state: State, store: BaseStore):
    namespace = ("instructions",)
    current_instructions = store.search(namespace)[0]
    # Memory logic
    prompt = prompt_template.format(instructions=instructions.value["instructions"], conversation=state["messages"])
    output = llm.invoke(prompt)
    new_instructions = output['new_instructions']
    store.put(("agent_instructions",), "agent_a", {"instructions": new_instructions})
    ...

Writing memories¶
While humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: "on the hot path" and "in the background".

Writing memories in the hot path¶
Creating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.
However, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.
As an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.
Writing memories in the background¶
Creating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.
However, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.
See our memory-service template as an reference implementation.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 27
SOURCE: https://langchain-ai.github.io/langgraph/concepts/time-travel/
CONTENT:
Time Travel ‚è±Ô∏è¶

Prerequisites
This guide assumes that you are familiar with LangGraph's checkpoints and states. If not, please review the persistence concept first.

When working with non-deterministic systems that make model-based decisions (e.g., agents powered by LLMs), it can be useful to examine their decision-making process in detail:

ü§î Understand Reasoning: Analyze the steps that led to a successful result.
üêû Debug Mistakes: Identify where and why errors occurred.
üîç Explore Alternatives: Test different paths to uncover better solutions.

We call these debugging techniques Time Travel, composed of two key actions: Replaying üîÅ and Forking üîÄ .
Replaying¶

Replaying allows us to revisit and reproduce an agent's past actions, up to and including a specific step (checkpoint).
To replay actions before a specific checkpoint, start by retrieving all checkpoints for the thread:
all_checkpoints = []
for state in graph.get_state_history(thread):
    all_checkpoints.append(state)

Each checkpoint has a unique ID. After identifying the desired checkpoint, for instance, xyz, include its ID in the configuration:
config = {'configurable': {'thread_id': '1', 'checkpoint_id': 'xyz'}}
for event in graph.stream(None, config, stream_mode="values"):
    print(event)

The graph replays previously executed steps before the provided checkpoint_id and executes the steps after checkpoint_id (i.e., a new fork), even if they have been executed previously.
Forking¶

Forking allows you to revisit an agent's past actions and explore alternative paths within the graph.
To edit a specific checkpoint, such as xyz, provide its checkpoint_id when updating the graph's state:
config = {"configurable": {"thread_id": "1", "checkpoint_id": "xyz"}}
graph.update_state(config, {"state": "updated state"})

This creates a new forked checkpoint, xyz-fork, from which you can continue running the graph:
config = {'configurable': {'thread_id': '1', 'checkpoint_id': 'xyz-fork'}}
for event in graph.stream(None, config, stream_mode="values"):
    print(event)

Additional Resources üìö¶

Conceptual Guide: Persistence: Read the persistence guide for more context on replaying.
How to View and Update Past Graph State: Step-by-step instructions for working with graph state that demonstrate the replay and fork actions.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 28
SOURCE: https://langchain-ai.github.io/langgraph/concepts/functional_api/
CONTENT:
Functional API¶
Overview¶
The Functional API allows you to add LangGraph's key features -- persistence, memory, human-in-the-loop, and streaming — to your applications with minimal changes to your existing code.
It is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as if statements, for loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.  
The Functional API uses two key building blocks:  

@entrypoint – Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts.  
@task – Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.  

This provides a minimal abstraction for building workflows with state management and streaming.

Tip
For users who prefer a more declarative approach, LangGraph's Graph API allows you to define workflows using a Graph paradigm. Both APIs share the same underlying runtime, so you can use them together in the same application.
Please see the Functional API vs. Graph API section for a comparison of the two paradigms.

Example¶
Below we demonstrate a simple application that writes an essay and interrupts to request human review.
API Reference: MemorySaver | entrypoint | task | interrupt
from langgraph.checkpoint.memory import MemorySaver
from langgraph.func import entrypoint, task
from langgraph.types import interrupt

@task
def write_essay(topic: str) -> str:
    """Write an essay about the given topic."""
    time.sleep(1) # A placeholder for a long-running task.
    return f"An essay about topic: {topic}"

@entrypoint(checkpointer=MemorySaver())
def workflow(topic: str) -> dict:
    """A simple workflow that writes an essay and asks for a review."""
    essay = write_essay("cat").result()
    is_approved = interrupt({
        # Any json-serializable payload provided to interrupt as argument.
        # It will be surfaced on the client side as an Interrupt when streaming data
        # from the workflow.
        "essay": essay, # The essay we want reviewed.
        # We can add any additional information that we need.
        # For example, introduce a key called "action" with some instructions.
        "action": "Please approve/reject the essay",
    })

    return {
        "essay": essay, # The essay that was generated
        "is_approved": is_approved, # Response from HIL
    }

Detailed Explanation
This workflow will write an essay about the topic "cat" and then pause to get a review from a human. The workflow can be interrupted for an indefinite amount of time until a review is provided.
When the workflow is resumed, it executes from the very start, but because the result of the write_essay task was already saved, the task result will be loaded from the checkpoint instead of being recomputed.
import time
import uuid

from langgraph.func import entrypoint, task
from langgraph.types import interrupt
from langgraph.checkpoint.memory import MemorySaver

@task
def write_essay(topic: str) -> str:
    """Write an essay about the given topic."""
    time.sleep(1) # This is a placeholder for a long-running task.
    return f"An essay about topic: {topic}"

@entrypoint(checkpointer=MemorySaver())
def workflow(topic: str) -> dict:
    """A simple workflow that writes an essay and asks for a review."""
    essay = write_essay("cat").result()
    is_approved = interrupt({
        # Any json-serializable payload provided to interrupt as argument.
        # It will be surfaced on the client side as an Interrupt when streaming data
        # from the workflow.
        "essay": essay, # The essay we want reviewed.
        # We can add any additional information that we need.
        # For example, introduce a key called "action" with some instructions.
        "action": "Please approve/reject the essay",
    })

    return {
        "essay": essay, # The essay that was generated
        "is_approved": is_approved, # Response from HIL
    }

thread_id = str(uuid.uuid4())

config = {
    "configurable": {
        "thread_id": thread_id
    }
}

for item in workflow.stream("cat", config):
    print(item)

{'write_essay': 'An essay about topic: cat'}
{'__interrupt__': (Interrupt(value={'essay': 'An essay about topic: cat', 'action': 'Please approve/reject the essay'}, resumable=True, ns=['workflow:f7b8508b-21c0-8b4c-5958-4e8de74d2684'], when='during'),)}

An essay has been written and is ready for review. Once the review is provided, we can resume the workflow:
from langgraph.types import Command

# Get review from a user (e.g., via a UI)
# In this case, we're using a bool, but this can be any json-serializable value.
human_review = True

for item in workflow.stream(Command(resume=human_review), config):
    print(item)

{'workflow': {'essay': 'An essay about topic: cat', 'is_approved': False}}

The workflow has been completed and the review has been added to the essay.

Entrypoint¶
The @entrypoint decorator can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling long-running tasks and interrupts.
Definition¶
An entrypoint is defined by decorating a function with the @entrypoint decorator. 
The function must accept a single positional argument, which serves as the workflow input. If you need to pass multiple pieces of data, use a dictionary as the input type for the first argument.
Decorating a function with an entrypoint produces a Pregel instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing).
You will usually want to pass a checkpointer to the @entrypoint decorator to enable persistence and use features like human-in-the-loop.
SyncAsync

from langgraph.func import entrypoint

@entrypoint(checkpointer=checkpointer)
def my_workflow(some_input: dict) -> int:
    # some logic that may involve long-running tasks like API calls,
    # and may be interrupted for human-in-the-loop.
    ...
    return result

from langgraph.func import entrypoint

@entrypoint(checkpointer=checkpointer)
async def my_workflow(some_input: dict) -> int:
    # some logic that may involve long-running tasks like API calls,
    # and may be interrupted for human-in-the-loop
    ...
    return result 

Serialization
The inputs and outputs of entrypoints must be JSON-serializable to support checkpointing. Please see the serialization section for more details.

Injectable Parameters¶
When declaring an entrypoint, you can request access to additional parameters that will be injected automatically at run time. These parameters include:

Parameter
Description

previous
Access the the state associated with the previous checkpoint for the given thread. See state management.

store
An instance of BaseStore. Useful for long-term memory.

writer
For streaming custom data, to write custom data to the custom stream. Useful for streaming custom data.

config
For accessing run time configuration. See RunnableConfig for information.

Important
Declare the parameters with the appropriate name and type annotation.

Requesting Injectable Parameters
from langchain_core.runnables import RunnableConfig
from langgraph.func import entrypoint
from langgraph.store.base import BaseStore
from langgraph.store.memory import InMemoryStore

in_memory_store = InMemoryStore(...)  # An instance of InMemoryStore for long-term memory

@entrypoint(
    checkpointer=checkpointer,  # Specify the checkpointer
    store=in_memory_store  # Specify the store
)  
def my_workflow(
    some_input: dict,  # The input (e.g., passed via `invoke`)
    *,
    previous: Any = None, # For short-term memory
    store: BaseStore,  # For long-term memory
    writer: StreamWriter,  # For streaming custom data
    config: RunnableConfig  # For accessing the configuration passed to the entrypoint
) -> ...:

Executing¶
Using the @entrypoint yields a Pregel object that can be executed using the invoke, ainvoke, stream, and astream methods.
InvokeAsync InvokeStreamAsync Stream

config = {
    "configurable": {
        "thread_id": "some_thread_id"
    }
}
my_workflow.invoke(some_input, config)  # Wait for the result synchronously

config = {
    "configurable": {
        "thread_id": "some_thread_id"
    }
}
await my_workflow.ainvoke(some_input, config)  # Await result asynchronously

config = {
    "configurable": {
        "thread_id": "some_thread_id"
    }
}

for chunk in my_workflow.stream(some_input, config):
    print(chunk)

config = {
    "configurable": {
        "thread_id": "some_thread_id"
    }
}

async for chunk in my_workflow.astream(some_input, config):
    print(chunk)

Resuming¶
Resuming an execution after an interrupt can be done by passing a resume value to the Command primitive.
InvokeAsync InvokeStreamAsync Stream

from langgraph.types import Command

config = {
    "configurable": {
        "thread_id": "some_thread_id"
    }
}

my_workflow.invoke(Command(resume=some_resume_value), config)

from langgraph.types import Command

config = {
    "configurable": {
        "thread_id": "some_thread_id"
    }
}

await my_workflow.ainvoke(Command(resume=some_resume_value), config)

from langgraph.types import Command

config = {
    "configurable": {
        "thread_id": "some_thread_id"
    }
}

for chunk in my_workflow.stream(Command(resume=some_resume_value), config):
    print(chunk)

from langgraph.types import Command

config = {
    "configurable": {
        "thread_id": "some_thread_id"
    }
}

async for chunk in my_workflow.astream(Command(resume=some_resume_value), config):
    print(chunk)

Resuming after an error
To resume after an error, run the entrypoint with a None and the same thread id (config).
This assumes that the underlying error has been resolved and execution can proceed successfully.
InvokeAsync InvokeStreamAsync Stream

config = {
    "configurable": {
        "thread_id": "some_thread_id"
    }
}

my_workflow.invoke(None, config)

config = {
    "configurable": {
        "thread_id": "some_thread_id"
    }
}

await my_workflow.ainvoke(None, config)

config = {
    "configurable": {
        "thread_id": "some_thread_id"
    }
}

for chunk in my_workflow.stream(None, config):
    print(chunk)

config = {
    "configurable": {
        "thread_id": "some_thread_id"
    }
}

async for chunk in my_workflow.astream(None, config):
    print(chunk)

State Management¶
When an entrypoint is defined with a checkpointer, it stores information between successive invocations on the same thread id in checkpoints. 
This allows accessing the state from the previous invocation using the previous parameter.
By default, the previous parameter is the return value of the previous invocation.
@entrypoint(checkpointer=checkpointer)
def my_workflow(number: int, *, previous: Any = None) -> int:
    previous = previous or 0
    return number + previous

config = {
    "configurable": {
        "thread_id": "some_thread_id"
    }
}

my_workflow.invoke(1, config)  # 1 (previous was None)
my_workflow.invoke(2, config)  # 3 (previous was 1 from the previous invocation)

entrypoint.final¶
entrypoint.final is a special primitive that can be returned from an entrypoint and allows decoupling the value that is saved in the checkpoint from the return value of the entrypoint.
The first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is entrypoint.final[return_type, save_type].
@entrypoint(checkpointer=checkpointer)
def my_workflow(number: int, *, previous: Any = None) -> entrypoint.final[int, int]:
    previous = previous or 0
    # This will return the previous value to the caller, saving
    # 2 * number to the checkpoint, which will be used in the next invocation 
    # for the `previous` parameter.
    return entrypoint.final(value=previous, save=2 * number)

config = {
    "configurable": {
        "thread_id": "1"
    }
}

my_workflow.invoke(3, config)  # 0 (previous was None)
my_workflow.invoke(1, config)  # 6 (previous was 3 * 2 from the previous invocation)

Task¶
A task represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:

Asynchronous Execution: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.
Checkpointing: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See persistence for more details).

Definition¶
Tasks are defined using the @task decorator, which wraps a regular Python function.
API Reference: task
from langgraph.func import task

@task()
def slow_computation(input_value):
    # Simulate a long-running operation
    ...
    return result

Serialization
The outputs of tasks must be JSON-serializable to support checkpointing.

Execution¶
Tasks can only be called from within an entrypoint, another task, or a state graph node. 
Tasks cannot be called directly from the main application code. 
When you call a task, it returns immediately with a future object. A future is a placeholder for a result that will be available later.
To obtain the result of a task, you can either wait for it synchronously (using result()) or await it asynchronously (using await).
Synchronous InvocationAsynchronous Invocation

@entrypoint(checkpointer=checkpointer)
def my_workflow(some_input: int) -> int:
    future = slow_computation(some_input)
    return future.result()  # Wait for the result synchronously

@entrypoint(checkpointer=checkpointer)
async def my_workflow(some_input: int) -> int:
    return await slow_computation(some_input)  # Await result asynchronously

When to use a task¶
Tasks are useful in the following scenarios:

Checkpointing: When you need to save the result of a long-running operation to a checkpoint, so you don't need to recompute it when resuming the workflow.
Human-in-the-loop: If you're building a workflow that requires human intervention, you MUST use tasks to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the determinism section for more details.
Parallel Execution: For I/O-bound tasks, tasks enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).
Observability: Wrapping operations in tasks provides a way to track the progress of the workflow and monitor the execution of individual operations using LangSmith.
Retryable Work: When work needs to be retried to handle failures or inconsistencies, tasks provide a way to encapsulate and manage the retry logic.

Serialization¶
There are two key aspects to serialization in LangGraph:

@entrypoint inputs and outputs must be JSON-serializable.
@task outputs must be JSON-serializable.

These requirements are necessary for enabling checkpointing and workflow resumption. Use python primitives
like dictionaries, lists, strings, numbers, and booleans to ensure that your inputs and outputs are serializable.
Serialization ensures that workflow state, such as task results and intermediate values, can be reliably saved and restored. This is critical for enabling human-in-the-loop interactions, fault tolerance, and parallel execution.
Providing non-serializable inputs or outputs will result in a runtime error when a workflow is configured with a checkpointer.
Determinism¶
To utilize features like human-in-the-loop, any randomness should be encapsulated inside of tasks. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same sequence of steps, even if task results are non-deterministic.
LangGraph achieves this behavior by persisting task and subgraph results as they execute. A well-designed workflow ensures that resuming execution follows the same sequence of steps, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running tasks or tasks with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same 
While different runs of a workflow can produce different results, resuming a specific run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up task and subgraph results that were executed prior to the graph being interrupted and avoid recomputing them.
Idempotency¶
Idempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside tasks functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a task starts, but does not complete successfully. Then, if the workflow is resumed, the task will run again. Use idempotency keys or verify existing results to avoid duplication.
Functional API vs. Graph API¶
The Functional API and the Graph APIs (StateGraph) provide two different paradigms to create applications with LangGraph. Here are some key differences:

Control flow: The Functional API does not require thinking about graph structure. You can use standard Python constructs to define workflows. This will usually trim the amount of code you need to write.
State management: The GraphAPI requires declaring a State and may require defining reducers to manage updates to the graph state. @entrypoint and @tasks do not require explicit state management as their state is scoped to the function and is not shared across functions.
Checkpointing: Both APIs generate and use checkpoints. In the Graph API a new checkpoint is generated after every superstep. In the Functional API, when tasks are executed, their results are saved to an existing checkpoint associated with the given entrypoint instead of creating a new checkpoint.
Visualization: The Graph API makes it easy to visualize the workflow as a graph which can be useful for debugging, understanding the workflow, and sharing with others. The Functional API does not support visualization as the graph is dynamically generated during runtime.

Common Pitfalls¶
Handling side effects¶
Encapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.
IncorrectCorrect

In this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow.
@entrypoint(checkpointer=checkpointer)
def my_workflow(inputs: dict) -> int:
    # This code will be executed a second time when resuming the workflow.
    # Which is likely not what you want.
    with open("output.txt", "w") as f:
        f.write("Side effect executed")
    value = interrupt("question")
    return value

In this example, the side effect is encapsulated in a task, ensuring consistent execution upon resumption.
from langgraph.func import task

@task
def write_to_file():
    with open("output.txt", "w") as f:
        f.write("Side effect executed")

@entrypoint(checkpointer=checkpointer)
def my_workflow(inputs: dict) -> int:
    # The side effect is now encapsulated in a task.
    write_to_file().result()
    value = interrupt("question")
    return value

Non-deterministic control flow¶
Operations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.

In a task: Get random number (5) → interrupt → resume → (returns 5 again) → ...
Not in a task: Get random number (5) → interrupt → resume → get new random number (7) → ...

This is especially important when using human-in-the-loop workflows with multiple interrupts calls. LangGraph keeps a list
of resume values for each task/entrypoint. When an interrupt is encountered, it's matched with the corresponding resume value.
This matching is strictly index-based, so the order of the resume values should match the order of the interrupts.
If order of execution is not maintained when resuming, one interrupt call may be matched with the wrong resume value, leading to incorrect results.
Please read the section on determinism for more details.
IncorrectCorrect

In this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.
from langgraph.func import entrypoint

@entrypoint(checkpointer=checkpointer)
def my_workflow(inputs: dict) -> int:
    t0 = inputs["t0"]
    t1 = time.time()

    delta_t = t1 - t0

    if delta_t > 1:
        result = slow_task(1).result()
        value = interrupt("question")
    else:
        result = slow_task(2).result()
        value = interrupt("question")

    return {
        "result": result,
        "value": value
    }

In this example, the workflow uses the input t0 to determine which task to execute. This is deterministic because the result of the workflow depends only on the input.
import time

from langgraph.func import task

@task
def get_time() -> float:
    return time.time()

@entrypoint(checkpointer=checkpointer)
def my_workflow(inputs: dict) -> int:
    t0 = inputs["t0"]
    t1 = get_time().result()

    delta_t = t1 - t0

    if delta_t > 1:
        result = slow_task(1).result()
        value = interrupt("question")
    else:
        result = slow_task(2).result()
        value = interrupt("question")

    return {
        "result": result,
        "value": value
    }

Patterns¶
Below are a few simple patterns that show examples of how to use the Functional API.
When defining an entrypoint, input is restricted to the first argument of the function. To pass multiple inputs, you can use a dictionary.
@entrypoint(checkpointer=checkpointer)
def my_workflow(inputs: dict) -> int:
    value = inputs["value"]
    another_value = inputs["another_value"]
    ...

my_workflow.invoke({"value": 1, "another_value": 2})  

Parallel execution¶
Tasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs).
@task
def add_one(number: int) -> int:
    return number + 1

@entrypoint(checkpointer=checkpointer)
def graph(numbers: list[int]) -> list[str]:
    futures = [add_one(i) for i in numbers]
    return [f.result() for f in futures]

Calling subgraphs¶
The Functional API and the Graph API can be used together in the same application as they share the same underlying runtime.
API Reference: entrypoint | StateGraph
from langgraph.func import entrypoint
from langgraph.graph import StateGraph

builder = StateGraph()
...
some_graph = builder.compile()

@entrypoint()
def some_workflow(some_input: dict) -> int:
    # Call a graph defined using the graph API
    result_1 = some_graph.invoke(...)
    # Call another graph defined using the graph API
    result_2 = another_graph.invoke(...)
    return {
        "result_1": result_1,
        "result_2": result_2
    }

Calling other entrypoints¶
You can call other entrypoints from within an entrypoint or a task.
@entrypoint() # Will automatically use the checkpointer from the parent entrypoint
def some_other_workflow(inputs: dict) -> int:
    return inputs["value"]

@entrypoint(checkpointer=checkpointer)
def my_workflow(inputs: dict) -> int:
    value = some_other_workflow.invoke({"value": 1})
    return value

Streaming custom data¶
You can stream custom data from an entrypoint by using the StreamWriter type. This allows you to write custom data to the custom stream.
API Reference: MemorySaver | entrypoint | task | StreamWriter
from langgraph.checkpoint.memory import MemorySaver
from langgraph.func import entrypoint, task
from langgraph.types import StreamWriter

@task
def add_one(x):
    return x + 1

@task
def add_two(x):
    return x + 2

checkpointer = MemorySaver()

@entrypoint(checkpointer=checkpointer)
def main(inputs, writer: StreamWriter) -> int:
    """A simple workflow that adds one and two to a number."""
    writer("hello") # Write some data to the `custom` stream
    add_one(inputs['number']).result() # Will write data to the `updates` stream
    writer("world") # Write some more data to the `custom` stream
    add_two(inputs['number']).result() # Will write data to the `updates` stream
    return 5 

config = {
    "configurable": {
        "thread_id": "1"
    }
}

for chunk in main.stream({"number": 1}, stream_mode=["custom", "updates"], config=config):
    print(chunk)

('updates', {'add_one': 2})
('updates', {'add_two': 3})
('custom', 'hello')
('custom', 'world')
('updates', {'main': 5})

Important
The writer parameter is automatically injected at run time. It will only be injected if the 
parameter name appears in the function signature with that exact name.

Retry policy¶
API Reference: MemorySaver | entrypoint | task | RetryPolicy
from langgraph.checkpoint.memory import MemorySaver
from langgraph.func import entrypoint, task
from langgraph.types import RetryPolicy

attempts = 0

# Let's configure the RetryPolicy to retry on ValueError.
# The default RetryPolicy is optimized for retrying specific network errors.
retry_policy = RetryPolicy(retry_on=ValueError)

@task(retry=retry_policy) 
def get_info():
    global attempts
    attempts += 1

    if attempts < 2:
        raise ValueError('Failure')
    return "OK"

checkpointer = MemorySaver()

@entrypoint(checkpointer=checkpointer)
def main(inputs, writer):
    return get_info().result()

config = {
    "configurable": {
        "thread_id": "1"
    }
}

main.invoke({'any_input': 'foobar'}, config=config)

'OK'

Resuming after an error¶
API Reference: MemorySaver | entrypoint | task | StreamWriter
import time
from langgraph.checkpoint.memory import MemorySaver
from langgraph.func import entrypoint, task
from langgraph.types import StreamWriter

# This variable is just used for demonstration purposes to simulate a network failure.
# It's not something you will have in your actual code.
attempts = 0

@task()
def get_info():
    """
    Simulates a task that fails once before succeeding.
    Raises an exception on the first attempt, then returns "OK" on subsequent tries.
    """
    global attempts
    attempts += 1

    if attempts < 2:
        raise ValueError("Failure")  # Simulate a failure on the first attempt
    return "OK"

# Initialize an in-memory checkpointer for persistence
checkpointer = MemorySaver()

@task
def slow_task():
    """
    Simulates a slow-running task by introducing a 1-second delay.
    """
    time.sleep(1)
    return "Ran slow task."

@entrypoint(checkpointer=checkpointer)
def main(inputs, writer: StreamWriter):
    """
    Main workflow function that runs the slow_task and get_info tasks sequentially.

    Parameters:
    - inputs: Dictionary containing workflow input values.
    - writer: StreamWriter for streaming custom data.

    The workflow first executes `slow_task` and then attempts to execute `get_info`,
    which will fail on the first invocation.
    """
    slow_task_result = slow_task().result()  # Blocking call to slow_task
    get_info().result()  # Exception will be raised here on the first attempt
    return slow_task_result

# Workflow execution configuration with a unique thread identifier
config = {
    "configurable": {
        "thread_id": "1"  # Unique identifier to track workflow execution
    }
}

# This invocation will take ~1 second due to the slow_task execution
try:
    # First invocation will raise an exception due to the `get_info` task failing
    main.invoke({'any_input': 'foobar'}, config=config)
except ValueError:
    pass  # Handle the failure gracefully

When we resume execution, we won't need to re-run the slow_task as its result is already saved in the checkpoint.
main.invoke(None, config=config)

'Ran slow task.'

Human-in-the-loop¶
The functional API supports human-in-the-loop workflows using the interrupt function and the Command primitive.
Please see the following examples for more details:

How to wait for user input (Functional API): Shows how to implement a simple human-in-the-loop workflow using the functional API.
How to review tool calls (Functional API): Guide demonstrates how to implement human-in-the-loop workflows in a ReAct agent using the LangGraph Functional API.

Short-term memory¶
State management using the previous parameter and optionally using the entrypoint.final primitive can be used to implement short term memory.
Please see the following how-to guides for more details:

How to add thread-level persistence (functional API): Shows how to add thread-level persistence to a functional API workflow and implements a simple chatbot.

Long-term memory¶
long-term memory allows storing information across different thread ids. This could be useful for learning information
about a given user in one conversation and using it in another.
Please see the following how-to guides for more details:

How to add cross-thread persistence (functional API): Shows how to add cross-thread persistence to a functional API workflow and implements a simple chatbot.

Workflows¶

Workflows and agent guide for more examples of how to build workflows using the Functional API.

Agents¶

How to create a React agent from scratch (Functional API): Shows how to create a simple React agent from scratch using the functional API.
How to build a multi-agent network: Shows how to build a multi-agent network using the functional API.
How to add multi-turn conversation in a multi-agent application (functional API): allow an end-user to engage in a multi-turn conversation with one or more agents.  

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 29
SOURCE: https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/
CONTENT:
Agent architectures¶
Many LLM applications implement a particular control flow of steps before and / or after LLM calls. As an example, RAG performs retrieval of documents relevant to a user question, and passes those documents to an LLM in order to ground the model's response in the provided document context. 
Instead of hard-coding a fixed control flow, we sometimes want LLM systems that can pick their own control flow to solve more complex problems! This is one definition of an agent: an agent is a system that uses an LLM to decide the control flow of an application. There are many ways that an LLM can control application:

An LLM can route between two potential paths
An LLM can decide which of many tools to call
An LLM can decide whether the generated answer is sufficient or more work is needed

As a result, there are many different types of agent architectures, which give an LLM varying levels of control. 

Router¶
A router allows an LLM to select a single step from a specified set of options. This is an agent architecture that exhibits a relatively limited level of control because the LLM usually focuses on making a single decision and produces a specific output from a limited set of pre-defined options. Routers typically employ a few different concepts to achieve this.
Structured Output¶
Structured outputs with LLMs work by providing a specific format or schema that the LLM should follow in its response. This is similar to tool calling, but more general. While tool calling typically involves selecting and using predefined functions, structured outputs can be used for any type of formatted response. Common methods to achieve structured outputs include:

Prompt engineering: Instructing the LLM to respond in a specific format via the system prompt.
Output parsers: Using post-processing to extract structured data from LLM responses.
Tool calling: Leveraging built-in tool calling capabilities of some LLMs to generate structured outputs.

Structured outputs are crucial for routing as they ensure the LLM's decision can be reliably interpreted and acted upon by the system. Learn more about structured outputs in this how-to guide.
Tool calling agent¶
While a router allows an LLM to make a single decision, more complex agent architectures expand the LLM's control in two key ways:

Multi-step decision making: The LLM can make a series of decisions, one after another, instead of just one.
Tool access: The LLM can choose from and use a variety of tools to accomplish tasks.

ReAct is a popular general purpose agent architecture that combines these expansions, integrating three core concepts. 

Tool calling: Allowing the LLM to select and use various tools as needed.
Memory: Enabling the agent to retain and use information from previous steps.
Planning: Empowering the LLM to create and follow multi-step plans to achieve goals.

This architecture allows for more complex and flexible agent behaviors, going beyond simple routing to enable dynamic problem-solving with multiple steps. You can use it with create_react_agent.
Tool calling¶
Tools are useful whenever you want an agent to interact with external systems. External systems (e.g., APIs) often require a particular input schema or payload, rather than natural language. When we bind an API, for example, as a tool, we give the model awareness of the required input schema. The model will choose to call a tool based upon the natural language input from the user and it will return an output that adheres to the tool's required schema. 
Many LLM providers support tool calling and tool calling interface in LangChain is simple: you can simply pass any Python function into ChatModel.bind_tools(function).

Memory¶
Memory is crucial for agents, enabling them to retain and utilize information across multiple steps of problem-solving. It operates on different scales:

Short-term memory: Allows the agent to access information acquired during earlier steps in a sequence.
Long-term memory: Enables the agent to recall information from previous interactions, such as past messages in a conversation.

LangGraph provides full control over memory implementation:

State: User-defined schema specifying the exact structure of memory to retain.
Checkpointers: Mechanism to store state at every step across different interactions.

This flexible approach allows you to tailor the memory system to your specific agent architecture needs. For a practical guide on adding memory to your graph, see this tutorial.
Effective memory management enhances an agent's ability to maintain context, learn from past experiences, and make more informed decisions over time.
Planning¶
In the ReAct architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it has enough information to solve the user request and it is not worth calling any more tools.
ReAct implementation¶
There are several differences between this paper and the pre-built create_react_agent implementation:

First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Fourth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a "Thought" step before deciding which tools to call. This is the "Reasoning" part of "ReAct". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.

Custom agent architectures¶
While routers and tool-calling agents (like ReAct) are common, customizing agent architectures often leads to better performance for specific tasks. LangGraph offers several powerful features for building tailored agent systems:
Human-in-the-loop¶
Human involvement can significantly enhance agent reliability, especially for sensitive tasks. This can involve:

Approving specific actions
Providing feedback to update the agent's state
Offering guidance in complex decision-making processes

Human-in-the-loop patterns are crucial when full automation isn't feasible or desirable. Learn more in our human-in-the-loop guide.
Parallelization¶
Parallel processing is vital for efficient multi-agent systems and complex tasks. LangGraph supports parallelization through its Send API, enabling:

Concurrent processing of multiple states
Implementation of map-reduce-like operations
Efficient handling of independent subtasks

For practical implementation, see our map-reduce tutorial.
Subgraphs¶
Subgraphs are essential for managing complex agent architectures, particularly in multi-agent systems. They allow:

Isolated state management for individual agents
Hierarchical organization of agent teams
Controlled communication between agents and the main system

Subgraphs communicate with the parent graph through overlapping keys in the state schema. This enables flexible, modular agent design. For implementation details, refer to our subgraph how-to guide.
Reflection¶
Reflection mechanisms can significantly improve agent reliability by:

Evaluating task completion and correctness
Providing feedback for iterative improvement
Enabling self-correction and learning

While often LLM-based, reflection can also use deterministic methods. For instance, in coding tasks, compilation errors can serve as feedback. This approach is demonstrated in this video using LangGraph for self-corrective code generation.
By leveraging these features, LangGraph enables the creation of sophisticated, task-specific agent architectures that can handle complex workflows, collaborate effectively, and continuously improve their performance.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 30
SOURCE: https://langchain-ai.github.io/langgraph/concepts/multi_agent/
CONTENT:
Multi-agent Systems¶
An agent is a system that uses an LLM to decide the control flow of an application. As you develop these systems, they might grow more complex over time, making them harder to manage and scale. For example, you might run into the following problems:

agent has too many tools at its disposal and makes poor decisions about which tool to call next
context grows too complex for a single agent to keep track of
there is a need for multiple specialization areas in the system (e.g. planner, researcher, math expert, etc.)

To tackle these, you might consider breaking your application into multiple smaller, independent agents and composing them into a multi-agent system. These independent agents can be as simple as a prompt and an LLM call, or as complex as a ReAct agent (and more!).
The primary benefits of using multi-agent systems are:

Modularity: Separate agents make it easier to develop, test, and maintain agentic systems.
Specialization: You can create expert agents focused on specific domains, which helps with the overall system performance.
Control: You can explicitly control how agents communicate (as opposed to relying on function calling).

Multi-agent architectures¶

There are several ways to connect agents in a multi-agent system:

Network: each agent can communicate with every other agent. Any agent can decide which other agent to call next.
Supervisor: each agent communicates with a single supervisor agent. Supervisor agent makes decisions on which agent should be called next.
Supervisor (tool-calling): this is a special case of supervisor architecture. Individual agents can be represented as tools. In this case, a supervisor agent uses a tool-calling LLM to decide which of the agent tools to call, as well as the arguments to pass to those agents.
Hierarchical: you can define a multi-agent system with a supervisor of supervisors. This is a generalization of the supervisor architecture and allows for more complex control flows.
Custom multi-agent workflow: each agent communicates with only a subset of agents. Parts of the flow are deterministic, and only some agents can decide which other agents to call next.

Handoffs¶
In multi-agent architectures, agents can be represented as graph nodes. Each agent node executes its step(s) and decides whether to finish execution or route to another agent, including potentially routing to itself (e.g., running in a loop). A common pattern in multi-agent interactions is handoffs, where one agent hands off control to another. Handoffs allow you to specify:

destination: target agent to navigate to (e.g., name of the node to go to)
payload: information to pass to that agent (e.g., state update)

To implement handoffs in LangGraph, agent nodes can return Command object that allows you to combine both control flow and state updates:
def agent(state) -> Command[Literal["agent", "another_agent"]]:
    # the condition for routing/halting can be anything, e.g. LLM tool call / structured output, etc.
    goto = get_next_agent(...)  # 'agent' / 'another_agent'
    return Command(
        # Specify which agent to call next
        goto=goto,
        # Update the graph state
        update={"my_state_key": "my_state_value"}
    )

In a more complex scenario where each agent node is itself a graph (i.e., a subgraph), a node in one of the agent subgraphs might want to navigate to a different agent. For example, if you have two agents, alice and bob (subgraph nodes in a parent graph), and alice needs to navigate to bob, you can set graph=Command.PARENT in the Command object:
def some_node_inside_alice(state):
    return Command(
        goto="bob",
        update={"my_state_key": "my_state_value"},
        # specify which graph to navigate to (defaults to the current graph)
        graph=Command.PARENT,
    )

Note
If you need to support visualization for subgraphs communicating using Command(graph=Command.PARENT) you would need to wrap them in a node function with Command annotation, e.g. instead of this:
builder.add_node(alice)

you would need to do this:
def call_alice(state) -> Command[Literal["bob"]]:
    return alice.invoke(state)

builder.add_node("alice", call_alice)

Handoffs as tools¶
One of the most common agent types is a ReAct-style tool-calling agents. For those types of agents, a common pattern is wrapping a handoff in a tool call, e.g.:
def transfer_to_bob(state):
    """Transfer to bob."""
    return Command(
        goto="bob",
        update={"my_state_key": "my_state_value"},
        graph=Command.PARENT,
    )

This is a special case of updating the graph state from tools where, in addition to the state update, the control flow is included as well.

Important
If you want to use tools that return Command, you can either use prebuilt create_react_agent / ToolNode components, or implement your own tool-executing node that collects Command objects returned by the tools and returns a list of them, e.g.:
def call_tools(state):
    ...
    commands = [tools_by_name[tool_call["name"]].invoke(tool_call) for tool_call in tool_calls]
    return commands

Let's now take a closer look at the different multi-agent architectures.
Network¶
In this architecture, agents are defined as graph nodes. Each agent can communicate with every other agent (many-to-many connections) and can decide which agent to call next. This architecture is good for problems that do not have a clear hierarchy of agents or a specific sequence in which agents should be called.
API Reference: ChatOpenAI | Command | StateGraph | START | END
from typing import Literal
from langchain_openai import ChatOpenAI
from langgraph.types import Command
from langgraph.graph import StateGraph, MessagesState, START, END

model = ChatOpenAI()

def agent_1(state: MessagesState) -> Command[Literal["agent_2", "agent_3", END]]:
    # you can pass relevant parts of the state to the LLM (e.g., state["messages"])
    # to determine which agent to call next. a common pattern is to call the model
    # with a structured output (e.g. force it to return an output with a "next_agent" field)
    response = model.invoke(...)
    # route to one of the agents or exit based on the LLM's decision
    # if the LLM returns "__end__", the graph will finish execution
    return Command(
        goto=response["next_agent"],
        update={"messages": [response["content"]]},
    )

def agent_2(state: MessagesState) -> Command[Literal["agent_1", "agent_3", END]]:
    response = model.invoke(...)
    return Command(
        goto=response["next_agent"],
        update={"messages": [response["content"]]},
    )

def agent_3(state: MessagesState) -> Command[Literal["agent_1", "agent_2", END]]:
    ...
    return Command(
        goto=response["next_agent"],
        update={"messages": [response["content"]]},
    )

builder = StateGraph(MessagesState)
builder.add_node(agent_1)
builder.add_node(agent_2)
builder.add_node(agent_3)

builder.add_edge(START, "agent_1")
network = builder.compile()

Supervisor¶
In this architecture, we define agents as nodes and add a supervisor node (LLM) that decides which agent nodes should be called next. We use Command to route execution to the appropriate agent node based on supervisor's decision. This architecture also lends itself well to running multiple agents in parallel or using map-reduce pattern.
API Reference: ChatOpenAI | Command | StateGraph | START | END
from typing import Literal
from langchain_openai import ChatOpenAI
from langgraph.types import Command
from langgraph.graph import StateGraph, MessagesState, START, END

model = ChatOpenAI()

def supervisor(state: MessagesState) -> Command[Literal["agent_1", "agent_2", END]]:
    # you can pass relevant parts of the state to the LLM (e.g., state["messages"])
    # to determine which agent to call next. a common pattern is to call the model
    # with a structured output (e.g. force it to return an output with a "next_agent" field)
    response = model.invoke(...)
    # route to one of the agents or exit based on the supervisor's decision
    # if the supervisor returns "__end__", the graph will finish execution
    return Command(goto=response["next_agent"])

def agent_1(state: MessagesState) -> Command[Literal["supervisor"]]:
    # you can pass relevant parts of the state to the LLM (e.g., state["messages"])
    # and add any additional logic (different models, custom prompts, structured output, etc.)
    response = model.invoke(...)
    return Command(
        goto="supervisor",
        update={"messages": [response]},
    )

def agent_2(state: MessagesState) -> Command[Literal["supervisor"]]:
    response = model.invoke(...)
    return Command(
        goto="supervisor",
        update={"messages": [response]},
    )

builder = StateGraph(MessagesState)
builder.add_node(supervisor)
builder.add_node(agent_1)
builder.add_node(agent_2)

builder.add_edge(START, "supervisor")

supervisor = builder.compile()

Check out this tutorial for an example of supervisor multi-agent architecture.
Supervisor (tool-calling)¶
In this variant of the supervisor architecture, we define individual agents as tools and use a tool-calling LLM in the supervisor node. This can be implemented as a ReAct-style agent with two nodes — an LLM node (supervisor) and a tool-calling node that executes tools (agents in this case).
API Reference: ChatOpenAI | InjectedState | create_react_agent
from typing import Annotated
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import InjectedState, create_react_agent

model = ChatOpenAI()

# this is the agent function that will be called as tool
# notice that you can pass the state to the tool via InjectedState annotation
def agent_1(state: Annotated[dict, InjectedState]):
    # you can pass relevant parts of the state to the LLM (e.g., state["messages"])
    # and add any additional logic (different models, custom prompts, structured output, etc.)
    response = model.invoke(...)
    # return the LLM response as a string (expected tool response format)
    # this will be automatically turned to ToolMessage
    # by the prebuilt create_react_agent (supervisor)
    return response.content

def agent_2(state: Annotated[dict, InjectedState]):
    response = model.invoke(...)
    return response.content

tools = [agent_1, agent_2]
# the simplest way to build a supervisor w/ tool-calling is to use prebuilt ReAct agent graph
# that consists of a tool-calling LLM node (i.e. supervisor) and a tool-executing node
supervisor = create_react_agent(model, tools)

Hierarchical¶
As you add more agents to your system, it might become too hard for the supervisor to manage all of them. The supervisor might start making poor decisions about which agent to call next, or the context might become too complex for a single supervisor to keep track of. In other words, you end up with the same problems that motivated the multi-agent architecture in the first place.
To address this, you can design your system hierarchically. For example, you can create separate, specialized teams of agents managed by individual supervisors, and a top-level supervisor to manage the teams.
API Reference: ChatOpenAI | StateGraph | START | END | Command
from typing import Literal
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.types import Command
model = ChatOpenAI()

# define team 1 (same as the single supervisor example above)

def team_1_supervisor(state: MessagesState) -> Command[Literal["team_1_agent_1", "team_1_agent_2", END]]:
    response = model.invoke(...)
    return Command(goto=response["next_agent"])

def team_1_agent_1(state: MessagesState) -> Command[Literal["team_1_supervisor"]]:
    response = model.invoke(...)
    return Command(goto="team_1_supervisor", update={"messages": [response]})

def team_1_agent_2(state: MessagesState) -> Command[Literal["team_1_supervisor"]]:
    response = model.invoke(...)
    return Command(goto="team_1_supervisor", update={"messages": [response]})

team_1_builder = StateGraph(Team1State)
team_1_builder.add_node(team_1_supervisor)
team_1_builder.add_node(team_1_agent_1)
team_1_builder.add_node(team_1_agent_2)
team_1_builder.add_edge(START, "team_1_supervisor")
team_1_graph = team_1_builder.compile()

# define team 2 (same as the single supervisor example above)
class Team2State(MessagesState):
    next: Literal["team_2_agent_1", "team_2_agent_2", "__end__"]

def team_2_supervisor(state: Team2State):
    ...

def team_2_agent_1(state: Team2State):
    ...

def team_2_agent_2(state: Team2State):
    ...

team_2_builder = StateGraph(Team2State)
...
team_2_graph = team_2_builder.compile()

# define top-level supervisor

builder = StateGraph(MessagesState)
def top_level_supervisor(state: MessagesState) -> Command[Literal["team_1_graph", "team_2_graph", END]]:
    # you can pass relevant parts of the state to the LLM (e.g., state["messages"])
    # to determine which team to call next. a common pattern is to call the model
    # with a structured output (e.g. force it to return an output with a "next_team" field)
    response = model.invoke(...)
    # route to one of the teams or exit based on the supervisor's decision
    # if the supervisor returns "__end__", the graph will finish execution
    return Command(goto=response["next_team"])

builder = StateGraph(MessagesState)
builder.add_node(top_level_supervisor)
builder.add_node("team_1_graph", team_1_graph)
builder.add_node("team_2_graph", team_2_graph)
builder.add_edge(START, "top_level_supervisor")
builder.add_edge("team_1_graph", "top_level_supervisor")
builder.add_edge("team_2_graph", "top_level_supervisor")
graph = builder.compile()

Custom multi-agent workflow¶
In this architecture we add individual agents as graph nodes and define the order in which agents are called ahead of time, in a custom workflow. In LangGraph the workflow can be defined in two ways:

Explicit control flow (normal edges): LangGraph allows you to explicitly define the control flow of your application (i.e. the sequence of how agents communicate) explicitly, via normal graph edges. This is the most deterministic variant of this architecture above — we always know which agent will be called next ahead of time.

Dynamic control flow (Command): in LangGraph you can allow LLMs to decide parts of your application control flow. This can be achieved by using Command. A special case of this is a supervisor tool-calling architecture. In that case, the tool-calling LLM powering the supervisor agent will make decisions about the order in which the tools (agents) are being called.

API Reference: ChatOpenAI | StateGraph | START
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState, START

model = ChatOpenAI()

def agent_1(state: MessagesState):
    response = model.invoke(...)
    return {"messages": [response]}

def agent_2(state: MessagesState):
    response = model.invoke(...)
    return {"messages": [response]}

builder = StateGraph(MessagesState)
builder.add_node(agent_1)
builder.add_node(agent_2)
# define the flow explicitly
builder.add_edge(START, "agent_1")
builder.add_edge("agent_1", "agent_2")

Communication between agents¶
The most important thing when building multi-agent systems is figuring out how the agents communicate. There are a few different considerations:

Do agents communicate via graph state or via tool calls?
What if two agents have different state schemas?
How to communicate over a shared message list?

Graph state vs tool calls¶
What is the "payload" that is being passed around between agents? In most of the architectures discussed above the agents communicate via the graph state. In the case of the supervisor with tool-calling, the payloads are tool call arguments.

Graph state¶
To communicate via graph state, individual agents need to be defined as graph nodes. These can be added as functions or as entire subgraphs. At each step of the graph execution, agent node receives the current state of the graph, executes the agent code and then passes the updated state to the next nodes.
Typically agent nodes share a single state schema. However, you might want to design agent nodes with different state schemas.
Different state schemas¶
An agent might need to have a different state schema from the rest of the agents. For example, a search agent might only need to keep track of queries and retrieved documents. There are two ways to achieve this in LangGraph:

Define subgraph agents with a separate state schema. If there are no shared state keys (channels) between the subgraph and the parent graph, it’s important to add input / output transformations so that the parent graph knows how to communicate with the subgraphs.
Define agent node functions with a private input state schema that is distinct from the overall graph state schema. This allows passing information that is only needed for executing that particular agent.

Shared message list¶
The most common way for the agents to communicate is via a shared state channel, typically a list of messages. This assumes that there is always at least a single channel (key) in the state that is shared by the agents. When communicating via a shared message list there is an additional consideration: should the agents share the full history of their thought process or only the final result?

Share full history¶
Agents can share the full history of their thought process (i.e. "scratchpad") with all other agents. This "scratchpad" would typically look like a list of messages. The benefit of sharing full thought process is that it might help other agents make better decisions and improve reasoning ability for the system as a whole. The downside is that as the number of agents and their complexity grows, the "scratchpad" will grow quickly and might require additional strategies for memory management.
Share final result¶
Agents can have their own private "scratchpad" and only share the final result with the rest of the agents. This approach might work better for systems with many agents or agents that are more complex. In this case, you would need to define agents with different state schemas
For agents called as tools, the supervisor determines the inputs based on the tool schema. Additionally, LangGraph allows passing state to individual tools at runtime, so subordinate agents can access parent state, if needed.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 31
SOURCE: https://langchain-ai.github.io/langgraph/concepts/streaming/
CONTENT:
Streaming¶
Building a responsive app for end-users? Real-time updates are key to keeping users engaged as your app progresses.
There are three main types of data you’ll want to stream:

Workflow progress (e.g., get state updates after each graph node is executed).
LLM tokens as they’re generated.
Custom updates (e.g., "Fetched 10/100 records").

Streaming graph outputs (.stream and .astream)¶
.stream and .astream are sync and async methods for streaming back outputs from a graph run.
There are several different modes you can specify when calling these methods (e.g. `graph.stream(..., mode="...")):

"values": This streams the full value of the state after each step of the graph.
"updates": This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
"custom": This streams custom data from inside your graph nodes.
"messages": This streams LLM tokens and metadata for the graph node where LLM is invoked.
"debug": This streams as much information as possible throughout the execution of the graph.

You can also specify multiple streaming modes at the same time by passing them as a list. When you do this, the streamed outputs will be tuples (stream_mode, data). For example:
graph.stream(..., stream_mode=["updates", "messages"])

...
('messages', (AIMessageChunk(content='Hi'), {'langgraph_step': 3, 'langgraph_node': 'agent', ...}))
...
('updates', {'agent': {'messages': [AIMessage(content="Hi, how can I help you?")]}})

The below visualization shows the difference between the values and updates modes:

LangGraph Platform¶
Streaming is critical for making LLM applications feel responsive to end users. When creating a streaming run, the streaming mode determines what data is streamed back to the API client. LangGraph Platform supports five streaming modes:

values: Stream the full state of the graph after each super-step is executed. See the how-to guide for streaming values.
messages-tuple: Stream LLM tokens for any messages generated inside a node. This mode is primarily meant for powering chat applications. See the how-to guide for streaming messages.
updates: Streams updates to the state of the graph after each node is executed. See the how-to guide for streaming updates.
debug: Stream debug events throughout graph execution. See the how-to guide for streaming debug events.
events: Stream all events (including the state of the graph) that occur during graph execution. See the how-to guide for streaming events. This mode is only useful for users migrating large LCEL applications to LangGraph. Generally, this mode is not necessary for most applications.

You can also specify multiple streaming modes at the same time. See the how-to guide for configuring multiple streaming modes at the same time.
See the API reference for how to create streaming runs.
Streaming modes values, updates, messages-tuple and debug are very similar to modes available in the LangGraph library - for a deeper conceptual explanation of those, you can see the previous section.
Streaming mode events is the same as using .astream_events in the LangGraph library - for a deeper conceptual explanation of this, you can see the previous section.
All events emitted have two attributes:

event: This is the name of the event
data: This is data associated with the event

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 32
SOURCE: https://langchain-ai.github.io/langgraph/concepts/low_level/
CONTENT:
LangGraph Glossary¶
Graphs¶
At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code.
In short: nodes do the work. edges tell what to do next.
LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete "super-steps."
A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or "channels"). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.
StateGraph¶
The StateGraph class is the main graph class to use. This is parameterized by a user defined State object.
Compiling your graph¶
To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed?
Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method:
graph = graph_builder.compile(...)

You MUST compile your graph before you can use it.
State¶
The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.
Schema¶
The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation.
By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use.
Multiple schemas¶
Typically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:

Internal nodes can pass information that is not required in the graph's input / output.
We may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.

It is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, PrivateState. See this notebook for more detail.
It is also possible to define explicit input and output schemas for a graph. In these cases, we define an "internal" schema that contains all keys relevant to graph operations. But, we also define input and output schemas that are sub-sets of the "internal" schema to constrain the input and output of the graph. See this notebook for more detail.
Let's look at an example:
class InputState(TypedDict):
    user_input: str

class OutputState(TypedDict):
    graph_output: str

class OverallState(TypedDict):
    foo: str
    user_input: str
    graph_output: str

class PrivateState(TypedDict):
    bar: str

def node_1(state: InputState) -> OverallState:
    # Write to OverallState
    return {"foo": state["user_input"] + " name"}

def node_2(state: OverallState) -> PrivateState:
    # Read from OverallState, write to PrivateState
    return {"bar": state["foo"] + " is"}

def node_3(state: PrivateState) -> OutputState:
    # Read from PrivateState, write to OutputState
    return {"graph_output": state["bar"] + " Lance"}

builder = StateGraph(OverallState,input=InputState,output=OutputState)
builder.add_node("node_1", node_1)
builder.add_node("node_2", node_2)
builder.add_node("node_3", node_3)
builder.add_edge(START, "node_1")
builder.add_edge("node_1", "node_2")
builder.add_edge("node_2", "node_3")
builder.add_edge("node_3", END)

graph = builder.compile()
graph.invoke({"user_input":"My"})
{'graph_output': 'My name is Lance'}

There are two subtle and important points to note here:

We pass state: InputState as the input schema to node_1. But, we write out to foo, a channel in OverallState. How can we write out to a state channel that is not included in the input schema? This is because a node can write to any state channel in the graph state. The graph state is the union of of the state channels defined at initialization, which includes OverallState and the filters InputState and OutputState.

We initialize the graph with StateGraph(OverallState,input=InputState,output=OutputState). So, how can we write to PrivateState in node_2? How does the graph gain access to this schema if it was not passed in the StateGraph initialization? We can do this because nodes can also declare additional state channels as long as the state schema definition exists. In this case, the PrivateState schema is defined, so we can add bar as a new state channel in the graph and write to it.

Reducers¶
Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:
Default Reducer¶
These two examples show how to use the default reducer:
Example A:
from typing_extensions import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]

In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {"foo": 1, "bar": ["hi"]}. Let's then assume the first Node returns {"foo": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {"foo": 2, "bar": ["hi"]}. If the second node returns {"bar": ["bye"]} then the State would then be {"foo": 2, "bar": ["bye"]}
Example B:
from typing import Annotated
from typing_extensions import TypedDict
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]

In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {"foo": 1, "bar": ["hi"]}. Let's then assume the first Node returns {"foo": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {"foo": 2, "bar": ["hi"]}. If the second node returns {"bar": ["bye"]} then the State would then be {"foo": 2, "bar": ["hi", "bye"]}. Notice here that the bar key is updated by adding the two lists together.
Working with Messages in Graph State¶
Why use messages?¶
Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide.
Using Messages in your Graph¶
In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer.
However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.
Serialization¶
In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format:
# this is supported
{"messages": [HumanMessage(content="message")]}

# and this is also supported
{"messages": [{"type": "human", "content": "message"}]}

Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state["messages"][-1].content. Below is an example of a graph that uses add_messages as it's reducer function.
API Reference: AnyMessage | add_messages
from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated
from typing_extensions import TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]

MessagesState¶
Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:
from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]

Nodes¶
In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a "config", containing optional configurable parameters (such as a thread_id).
Similar to NetworkX, you add these nodes to a graph using the add_node method:
API Reference: RunnableConfig | StateGraph
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print("In node: ", config["configurable"]["user_id"])
    return {"results": f"Hello, {state['input']}!"}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node("my_node", my_node)
builder.add_node("other_node", my_other_node)
...

Behind the scenes, functions are converted to RunnableLambdas, which add batch and async support to your function, along with native tracing and debugging.
If you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.
builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `"my_node"`

START Node¶
The START Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.
API Reference: START
from langgraph.graph import START

graph.add_edge(START, "node_a")

END Node¶
The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.
from langgraph.graph import END

graph.add_edge("node_a", END)

Edges¶
Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:

Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.

A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.
Normal Edges¶
If you always want to go from node A to node B, you can use the add_edge method directly.
graph.add_edge("node_a", "node_b")

Conditional Edges¶
If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a "routing function" to call after that node is executed:
graph.add_conditional_edges("node_a", routing_function)

Similar to nodes, the routing_function accepts the current state of the graph and returns a value.
By default, the return value routing_function is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.
You can optionally provide a dictionary that maps the routing_function's output to the name of the next node.
graph.add_conditional_edges("node_a", routing_function, {True: "node_b", False: "node_c"})

Tip
Use Command instead of conditional edges if you want to combine state updates and routing in a single function.

Entry Point¶
The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph.
API Reference: START
from langgraph.graph import START

graph.add_edge(START, "node_a")

Conditional Entry Point¶
A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this.
API Reference: START
from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)

You can optionally provide a dictionary that maps the routing_function's output to the name of the next node.
graph.add_conditional_edges(START, routing_function, {True: "node_b", False: "node_c"})

Send¶
By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object).
To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node.
def continue_to_jokes(state: OverallState):
    return [Send("generate_joke", {"subject": s}) for s in state['subjects']]

graph.add_conditional_edges("node_a", continue_to_jokes)

Command¶
It can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:
def my_node(state: State) -> Command[Literal["my_other_node"]]:
    return Command(
        # state update
        update={"foo": "bar"},
        # control flow
        goto="my_other_node"
    )

With Command you can also achieve dynamic control flow behavior (identical to conditional edges):
def my_node(state: State) -> Command[Literal["my_other_node"]]:
    if state["foo"] == "bar":
        return Command(update={"foo": "baz"}, goto="my_other_node")

Important
When returning Command in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. Command[Literal["my_other_node"]]. This is necessary for the graph rendering and tells LangGraph that my_node can navigate to my_other_node.

Check out this how-to guide for an end-to-end example of how to use Command.
When should I use Command instead of conditional edges?¶
Use Command when you need to both update the graph state and route to a different node. For example, when implementing multi-agent handoffs where it's important to route to a different agent and pass some information to that agent.
Use conditional edges to route between nodes conditionally without updating the state.
Navigating to a node in a parent graph¶
If you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:
def my_node(state: State) -> Command[Literal["other_subgraph"]]:
    return Command(
        update={"foo": "bar"},
        goto="other_subgraph",  # where `other_subgraph` is a node in the parent graph
        graph=Command.PARENT
    )

Note
Setting graph to Command.PARENT will navigate to the closest parent graph.

State updates with Command.PARENT
When you send updates from a subgraph node to a parent graph node for a key that's shared by both parent and subgraph state schemas, you must define a reducer for the key you're updating in the parent graph state. See this example.

This is particularly useful when implementing multi-agent handoffs.
Using inside tools¶
A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return Command(update={"my_custom_key": "foo", "messages": [...]}) from the tool:
@tool
def lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):
    """Use this to look up user information to better assist them with their questions."""
    user_info = get_user_info(config.get("configurable", {}).get("user_id"))
    return Command(
        update={
            # update the state keys
            "user_info": user_info,
            # update the message history
            "messages": [ToolMessage("Successfully looked up user information", tool_call_id=tool_call_id)]
        }
    )

Important
You MUST include messages (or any state key used for the message history) in Command.update when returning Command from a tool and the list of messages in messages MUST contain a ToolMessage. This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).

If you are using tools that update state via Command, we recommend using prebuilt ToolNode which automatically handles tools returning Command objects and propagates them to the graph state. If you're writing a custom node that calls tools, you would need to manually propagate Command objects returned by the tools as the update from the node.
Human-in-the-loop¶
Command is an important part of human-in-the-loop workflows: when using interrupt() to collect user input, Command is then used to supply the input and resume execution via Command(resume="User input"). Check out this conceptual guide for more information.
Persistence¶
LangGraph provides built-in persistence for your agent's state using checkpointers. Checkpointers save snapshots of the graph state at every superstep, allowing resumption at any time. This enables features like human-in-the-loop interactions, memory management, and fault-tolerance. You can even directly manipulate a graph's state after its execution using the
appropriate get and update methods. For more details, see the persistence conceptual guide.
Threads¶
Threads in LangGraph represent individual sessions or conversations between your graph and a user. When using checkpointing, turns in a single conversation (and even steps within a single graph execution) are organized by a unique thread ID.
Storage¶
LangGraph provides built-in document storage through the BaseStore interface. Unlike checkpointers, which save state by thread ID, stores use custom namespaces for organizing data. This enables cross-thread persistence, allowing agents to maintain long-term memories, learn from past interactions, and accumulate knowledge over time. Common use cases include storing user profiles, building knowledge bases, and managing global preferences across all threads.
Graph Migrations¶
LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.

For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.

Configuration¶
When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single "cognitive architecture" (the graph) but have multiple different instance of it.
You can optionally specify a config_schema when creating a graph.
class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)

You can then pass this configuration into the graph using the configurable config field.
config = {"configurable": {"llm": "anthropic"}}

graph.invoke(inputs, config=config)

You can then access and use this configuration inside a node:
def node_a(state, config):
    llm_type = config.get("configurable", {}).get("llm", "openai")
    llm = get_llm(llm_type)
    ...

See this guide for a full breakdown on configuration.
Recursion Limit¶
The recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raise GraphRecursionError. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to .invoke/.stream via the config dictionary. Importantly, recursion_limit is a standalone config key and should not be passed inside the configurable key as all other user-defined configuration. See the example below:
graph.invoke(inputs, config={"recursion_limit": 5, "configurable":{"llm": "anthropic"}})

Read this how-to to learn more about how the recursion limit works.
interrupt¶
Use the interrupt function to pause the graph at specific points to collect user input. The interrupt function surfaces interrupt information to the client, allowing the developer to collect user input, validate the graph state, or make decisions before resuming execution.
API Reference: interrupt
from langgraph.types import interrupt

def human_approval_node(state: State):
    ...
    answer = interrupt(
        # This value will be sent to the client.
        # It can be any JSON serializable value.
        {"question": "is it ok to continue?"},
    )
    ...

Resuming the graph is done by passing a Command object to the graph with the resume key set to the value returned by the interrupt function.
Read more about how the interrupt is used for human-in-the-loop workflows in the Human-in-the-loop conceptual guide.
Breakpoints¶
Breakpoints pause graph execution at specific points and enable stepping through execution step by step. Breakpoints are powered by LangGraph's persistence layer, which saves the state after each graph step. Breakpoints can also be used to enable human-in-the-loop workflows, though we recommend using the interrupt function for this purpose.
Read more about breakpoints in the Breakpoints conceptual guide.
Subgraphs¶
A subgraph is a graph that is used as a node in another graph. This is nothing more than the age-old concept of encapsulation, applied to LangGraph. Some reasons for using subgraphs are:

building multi-agent systems

when you want to reuse a set of nodes in multiple graphs, which maybe share some state, you can define them once in a subgraph and then use them in multiple parent graphs

when you want different teams to work on different parts of the graph independently, you can define each part as a subgraph, and as long as the subgraph interface (the input and output schemas) is respected, the parent graph can be built without knowing any details of the subgraph

There are two ways to add subgraphs to a parent graph:

add a node with the compiled subgraph: this is useful when the parent graph and the subgraph share state keys and you don't need to transform state on the way in or out

builder.add_node("subgraph", subgraph_builder.compile())

add a node with a function that invokes the subgraph: this is useful when the parent graph and the subgraph have different state schemas and you need to transform state before or after calling the subgraph

subgraph = subgraph_builder.compile()

def call_subgraph(state: State):
    return subgraph.invoke({"subgraph_key": state["parent_key"]})

builder.add_node("subgraph", call_subgraph)

Let's take a look at examples for each.
As a compiled graph¶
The simplest way to create subgraph nodes is by using a compiled subgraph directly. When doing so, it is important that the parent graph and the subgraph state schemas share at least one key which they can use to communicate. If your graph and subgraph do not share any keys, you should write a function invoking the subgraph instead.

Note
If you pass extra keys to the subgraph node (i.e., in addition to the shared keys), they will be ignored by the subgraph node. Similarly, if you return extra keys from the subgraph, they will be ignored by the parent graph.

API Reference: StateGraph
from langgraph.graph import StateGraph
from typing import TypedDict

class State(TypedDict):
    foo: str

class SubgraphState(TypedDict):
    foo: str  # note that this key is shared with the parent graph state
    bar: str

# Define subgraph
def subgraph_node(state: SubgraphState):
    # note that this subgraph node can communicate with the parent graph via the shared "foo" key
    return {"foo": state["foo"] + "bar"}

subgraph_builder = StateGraph(SubgraphState)
subgraph_builder.add_node(subgraph_node)
...
subgraph = subgraph_builder.compile()

# Define parent graph
builder = StateGraph(State)
builder.add_node("subgraph", subgraph)
...
graph = builder.compile()

As a function¶
You might want to define a subgraph with a completely different schema. In this case, you can create a node function that invokes the subgraph. This function will need to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.
class State(TypedDict):
    foo: str

class SubgraphState(TypedDict):
    # note that none of these keys are shared with the parent graph state
    bar: str
    baz: str

# Define subgraph
def subgraph_node(state: SubgraphState):
    return {"bar": state["bar"] + "baz"}

subgraph_builder = StateGraph(SubgraphState)
subgraph_builder.add_node(subgraph_node)
...
subgraph = subgraph_builder.compile()

# Define parent graph
def node(state: State):
    # transform the state to the subgraph state
    response = subgraph.invoke({"bar": state["foo"]})
    # transform response back to the parent state
    return {"foo": response["bar"]}

builder = StateGraph(State)
# note that we are using `node` function instead of a compiled subgraph
builder.add_node(node)
...
graph = builder.compile()

Visualization¶
It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.
Streaming¶
LangGraph is built with first class support for streaming, including streaming updates from graph nodes during the execution, streaming tokens from LLM calls and more. See this conceptual guide for more information.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 33
SOURCE: https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/
CONTENT:
Human-in-the-loop¶

This guide uses the new interrupt function.
As of LangGraph 0.2.57, the recommended way to set breakpoints is using the interrupt function as it simplifies human-in-the-loop patterns.
If you're looking for the previous version of this conceptual guide, which relied on static breakpoints and NodeInterrupt exception, it is available here. 

A human-in-the-loop (or "on-the-loop") workflow integrates human input into automated processes, allowing for decisions, validation, or corrections at key stages. This is especially useful in LLM-based applications, where the underlying model may generate occasional inaccuracies. In low-error-tolerance scenarios like compliance, decision-making, or content generation, human involvement ensures reliability by enabling review, correction, or override of model outputs.
Use cases¶
Key use cases for human-in-the-loop workflows in LLM-based applications include:

üõ†Ô∏è Reviewing tool calls: Humans can review, edit, or approve tool calls requested by the LLM before tool execution.
‚úÖ Validating LLM outputs: Humans can review, edit, or approve content generated by the LLM.
üí° Providing context: Enable the LLM to explicitly request human input for clarification or additional details or to support multi-turn conversations.

interrupt¶
The interrupt function in LangGraph enables human-in-the-loop workflows by pausing the graph at a specific node, presenting information to a human, and resuming the graph with their input. This function is useful for tasks like approvals, edits, or collecting additional input. The interrupt function is used in conjunction with the Command object to resume the graph with a value provided by the human.
API Reference: interrupt
from langgraph.types import interrupt

def human_node(state: State):
    value = interrupt(
        # Any JSON serializable value to surface to the human.
        # For example, a question or a piece of text or a set of keys in the state
       {
          "text_to_revise": state["some_text"]
       }
    )
    # Update the state with the human's input or route the graph based on the input.
    return {
        "some_text": value
    }

graph = graph_builder.compile(
    checkpointer=checkpointer # Required for `interrupt` to work
)

# Run the graph until the interrupt
thread_config = {"configurable": {"thread_id": "some_id"}}
graph.invoke(some_input, config=thread_config)

# Resume the graph with the human's input
graph.invoke(Command(resume=value_from_human), config=thread_config)

{'some_text': 'Edited text'}

Warning
Interrupts are both powerful and ergonomic. However, while they may resemble Python's input() function in terms of developer experience, it's important to note that they do not automatically resume execution from the interruption point. Instead, they rerun the entire node where the interrupt was used.
  For this reason, interrupts are typically best placed at the start of a node or in a dedicated node. Please read the resuming from an interrupt section for more details. 

Full Code
Here's a full example of how to use interrupt in a graph, if you'd like
  to see the code in action.
from typing import TypedDict
import uuid

from langgraph.checkpoint.memory import MemorySaver
from langgraph.constants import START
from langgraph.graph import StateGraph
from langgraph.types import interrupt, Command

class State(TypedDict):
   """The graph state."""
   some_text: str

def human_node(state: State):
   value = interrupt(
      # Any JSON serializable value to surface to the human.
      # For example, a question or a piece of text or a set of keys in the state
      {
         "text_to_revise": state["some_text"]
      }
   )
   return {
      # Update the state with the human's input
      "some_text": value
   }

# Build the graph
graph_builder = StateGraph(State)
# Add the human-node to the graph
graph_builder.add_node("human_node", human_node)
graph_builder.add_edge(START, "human_node")

# A checkpointer is required for `interrupt` to work.
checkpointer = MemorySaver()
graph = graph_builder.compile(
   checkpointer=checkpointer
)

# Pass a thread ID to the graph to run it.
thread_config = {"configurable": {"thread_id": uuid.uuid4()}}

# Using stream() to directly surface the `__interrupt__` information.
for chunk in graph.stream({"some_text": "Original text"}, config=thread_config):
   print(chunk)

# Resume using Command
for chunk in graph.stream(Command(resume="Edited text"), config=thread_config):
   print(chunk)

{'__interrupt__': (
      Interrupt(
         value={'question': 'Please revise the text', 'some_text': 'Original text'}, 
         resumable=True, 
         ns=['human_node:10fe492f-3688-c8c6-0d0a-ec61a43fecd6'], 
         when='during'
      ),
   )
}
{'human_node': {'some_text': 'Edited text'}}

Requirements¶
To use interrupt in your graph, you need to:

Specify a checkpointer to save the graph state after each step.
Call interrupt() in the appropriate place. See the Design Patterns section for examples.
Run the graph with a thread ID until the interrupt is hit.
Resume execution using invoke/ainvoke/stream/astream (see The Command primitive).

Design Patterns¶
There are typically three different actions that you can do with a human-in-the-loop workflow:

Approve or Reject: Pause the graph before a critical step, such as an API call, to review and approve the action. If the action is rejected, you can prevent the graph from executing the step, and potentially take an alternative action. This pattern often involve routing the graph based on the human's input.
Edit Graph State: Pause the graph to review and edit the graph state. This is useful for correcting mistakes or updating the state with additional information. This pattern often involves updating the state with the human's input.
Get Input: Explicitly request human input at a particular step in the graph. This is useful for collecting additional information or context to inform the agent's decision-making process or for supporting multi-turn conversations.

Below we show different design patterns that can be implemented using these actions.
Approve or Reject¶

Depending on the human's approval or rejection, the graph can proceed with the action or take an alternative path.

Pause the graph before a critical step, such as an API call, to review and approve the action. If the action is rejected, you can prevent the graph from executing the step, and potentially take an alternative action.
API Reference: interrupt | Command
from typing import Literal
from langgraph.types import interrupt, Command

def human_approval(state: State) -> Command[Literal["some_node", "another_node"]]:
    is_approved = interrupt(
        {
            "question": "Is this correct?",
            # Surface the output that should be
            # reviewed and approved by the human.
            "llm_output": state["llm_output"]
        }
    )

    if is_approved:
        return Command(goto="some_node")
    else:
        return Command(goto="another_node")

# Add the node to the graph in an appropriate location
# and connect it to the relevant nodes.
graph_builder.add_node("human_approval", human_approval)
graph = graph_builder.compile(checkpointer=checkpointer)

# After running the graph and hitting the interrupt, the graph will pause.
# Resume it with either an approval or rejection.
thread_config = {"configurable": {"thread_id": "some_id"}}
graph.invoke(Command(resume=True), config=thread_config)

See how to review tool calls for a more detailed example.
Review & Edit State¶

A human can review and edit the state of the graph. This is useful for correcting mistakes or updating the state with additional information.

API Reference: interrupt
from langgraph.types import interrupt

def human_editing(state: State):
    ...
    result = interrupt(
        # Interrupt information to surface to the client.
        # Can be any JSON serializable value.
        {
            "task": "Review the output from the LLM and make any necessary edits.",
            "llm_generated_summary": state["llm_generated_summary"]
        }
    )

    # Update the state with the edited text
    return {
        "llm_generated_summary": result["edited_text"] 
    }

# Add the node to the graph in an appropriate location
# and connect it to the relevant nodes.
graph_builder.add_node("human_editing", human_editing)
graph = graph_builder.compile(checkpointer=checkpointer)

...

# After running the graph and hitting the interrupt, the graph will pause.
# Resume it with the edited text.
thread_config = {"configurable": {"thread_id": "some_id"}}
graph.invoke(
    Command(resume={"edited_text": "The edited text"}), 
    config=thread_config
)

See How to wait for user input using interrupt for a more detailed example.
Review Tool Calls¶

A human can review and edit the output from the LLM before proceeding. This is particularly
critical in applications where the tool calls requested by the LLM may be sensitive or require human oversight.

def human_review_node(state) -> Command[Literal["call_llm", "run_tool"]]:
    # This is the value we'll be providing via Command(resume=<human_review>)
    human_review = interrupt(
        {
            "question": "Is this correct?",
            # Surface tool calls for review
            "tool_call": tool_call
        }
    )

    review_action, review_data = human_review

    # Approve the tool call and continue
    if review_action == "continue":
        return Command(goto="run_tool")

    # Modify the tool call manually and then continue
    elif review_action == "update":
        ...
        updated_msg = get_updated_msg(review_data)
        # Remember that to modify an existing message you will need
        # to pass the message with a matching ID.
        return Command(goto="run_tool", update={"messages": [updated_message]})

    # Give natural language feedback, and then pass that back to the agent
    elif review_action == "feedback":
        ...
        feedback_msg = get_feedback_msg(review_data)
        return Command(goto="call_llm", update={"messages": [feedback_msg]})

See how to review tool calls for a more detailed example.
Multi-turn conversation¶

A multi-turn conversation architecture where an agent and human node cycle back and forth until the agent decides to hand off the conversation to another agent or another part of the system.

A multi-turn conversation involves multiple back-and-forth interactions between an agent and a human, which can allow the agent to gather additional information from the human in a conversational manner.
This design pattern is useful in an LLM application consisting of multiple agents. One or more agents may need to carry out multi-turn conversations with a human, where the human provides input or feedback at different stages of the conversation. For simplicity, the agent implementation below is illustrated as a single node, but in reality 
it may be part of a larger graph consisting of multiple nodes and include a conditional edge.
Using a human node per agentSharing human node across multiple agents

In this pattern, each agent has its own human node for collecting user input. 
This can be achieved by either naming the human nodes with unique names (e.g., "human for agent 1", "human for agent 2") or by
using subgraphs where a subgraph contains a human node and an agent node.
from langgraph.types import interrupt

def human_input(state: State):
    human_message = interrupt("human_input")
    return {
        "messages": [
            {
                "role": "human",
                "content": human_message
            }
        ]
    }

def agent(state: State):
    # Agent logic
    ...

graph_builder.add_node("human_input", human_input)
graph_builder.add_edge("human_input", "agent")
graph = graph_builder.compile(checkpointer=checkpointer)

# After running the graph and hitting the interrupt, the graph will pause.
# Resume it with the human's input.
graph.invoke(
    Command(resume="hello!"),
    config=thread_config
)

In this pattern, a single human node is used to collect user input for multiple agents. The active agent is determined from the state, so after human input is collected, the graph can route to the correct agent.
from langgraph.types import interrupt

def human_node(state: MessagesState) -> Command[Literal["agent_1", "agent_2", ...]]:
    """A node for collecting user input."""
    user_input = interrupt(value="Ready for user input.")

    # Determine the **active agent** from the state, so 
    # we can route to the correct agent after collecting input.
    # For example, add a field to the state or use the last active agent.
    # or fill in `name` attribute of AI messages generated by the agents.
    active_agent = ... 

    return Command(
        update={
            "messages": [{
                "role": "human",
                "content": user_input,
            }]
        },
        goto=active_agent,
    )

See how to implement multi-turn conversations for a more detailed example.
Validating human input¶
If you need to validate the input provided by the human within the graph itself (rather than on the client side), you can achieve this by using multiple interrupt calls within a single node.
API Reference: interrupt
from langgraph.types import interrupt

def human_node(state: State):
    """Human node with validation."""
    question = "What is your age?"

    while True:
        answer = interrupt(question)

        # Validate answer, if the answer isn't valid ask for input again.
        if not isinstance(answer, int) or answer < 0:
            question = f"'{answer} is not a valid age. What is your age?"
            answer = None
            continue
        else:
            # If the answer is valid, we can proceed.
            break

    print(f"The human in the loop is {answer} years old.")
    return {
        "age": answer
    }

The Command primitive¶
When using the interrupt function, the graph will pause at the interrupt and wait for user input.
Graph execution can be resumed using the Command primitive which can be passed through the invoke, ainvoke, stream or astream methods.
The Command primitive provides several options to control and modify the graph's state during resumption:

Pass a value to the interrupt: Provide data, such as a user's response, to the graph using Command(resume=value). Execution resumes from the beginning of the node where the interrupt was used, however, this time the interrupt(...) call will return the value passed in the Command(resume=value) instead of pausing the graph.
# Resume graph execution with the user's input.
graph.invoke(Command(resume={"age": "25"}), thread_config)

Update the graph state: Modify the graph state using Command(update=update). Note that resumption starts from the beginning of the node where the interrupt was used. Execution resumes from the beginning of the node where the interrupt was used, but with the updated state.
# Update the graph state and resume.
# You must provide a `resume` value if using an `interrupt`.
graph.invoke(Command(update={"foo": "bar"}, resume="Let's go!!!"), thread_config)

By leveraging Command, you can resume graph execution, handle user inputs, and dynamically adjust the graph's state.
How does resuming from an interrupt work?¶

Warning
Resuming from an interrupt is different from Python's input() function, where execution resumes from the exact point where the input() function was called.

A critical aspect of using interrupt is understanding how resuming works. When you resume execution after an interrupt, graph execution starts from the beginning of the graph node where the last interrupt was triggered.
All code from the beginning of the node to the interrupt will be re-executed.
counter = 0
def node(state: State):
    # All the code from the beginning of the node to the interrupt will be re-executed
    # when the graph resumes.
    global counter
    counter += 1
    print(f"> Entered the node: {counter} # of times")
    # Pause the graph and wait for user input.
    answer = interrupt()
    print("The value of counter is:", counter)
    ...

Upon resuming the graph, the counter will be incremented a second time, resulting in the following output:
> Entered the node: 2 # of times
The value of counter is: 2

Common Pitfalls¶
Side-effects¶
Place code with side effects, such as API calls, after the interrupt to avoid duplication, as these are re-triggered every time the node is resumed. 
Side effects before interrupt (BAD)Side effects after interrupt (OK)Side effects in a separate node (OK)

This code will re-execute the API call another time when the node is resumed from
the interrupt.
This can be problematic if the API call is not idempotent or is just expensive.
from langgraph.types import interrupt

def human_node(state: State):
    """Human node with validation."""
    api_call(...) # This code will be re-executed when the node is resumed.
    answer = interrupt(question)

from langgraph.types import interrupt

def human_node(state: State):
    """Human node with validation."""

    answer = interrupt(question)

    api_call(answer) # OK as it's after the interrupt

from langgraph.types import interrupt

def human_node(state: State):
    """Human node with validation."""

    answer = interrupt(question)

    return {
        "answer": answer
    }

def api_call_node(state: State):
    api_call(...) # OK as it's in a separate node

Subgraphs called as functions¶
When invoking a subgraph as a function, the parent graph will resume execution from the beginning of the node where the subgraph was invoked (and where an interrupt was triggered). Similarly, the subgraph, will resume from the beginning of the node where the interrupt() function was called.
For example,
def node_in_parent_graph(state: State):
    some_code()  # <-- This will re-execute when the subgraph is resumed.
    # Invoke a subgraph as a function.
    # The subgraph contains an `interrupt` call.
    subgraph_result = subgraph.invoke(some_input)
    ...

Example: Parent and Subgraph Execution Flow
Say we have a parent graph with 3 nodes:
Parent Graph: node_1 ‚Üí node_2 (subgraph call) ‚Üí node_3
And the subgraph has 3 nodes, where the second node contains an interrupt:
Subgraph: sub_node_1 ‚Üí sub_node_2 (interrupt) ‚Üí sub_node_3
When resuming the graph, the execution will proceed as follows:

Skip node_1 in the parent graph (already executed, graph state was saved in snapshot).
Re-execute node_2 in the parent graph from the start.
Skip sub_node_1 in the subgraph (already executed, graph state was saved in snapshot).
Re-execute sub_node_2 in the subgraph from the beginning.
Continue with sub_node_3 and subsequent nodes.

Here is abbreviated example code that you can use to understand how subgraphs work with interrupts.
  It counts the number of times each node is entered and prints the count.
import uuid
from typing import TypedDict

from langgraph.graph import StateGraph
from langgraph.constants import START
from langgraph.types import interrupt, Command
from langgraph.checkpoint.memory import MemorySaver

class State(TypedDict):
   """The graph state."""
   state_counter: int

counter_node_in_subgraph = 0

def node_in_subgraph(state: State):
   """A node in the sub-graph."""
   global counter_node_in_subgraph
   counter_node_in_subgraph += 1  # This code will **NOT** run again!
   print(f"Entered `node_in_subgraph` a total of {counter_node_in_subgraph} times")

counter_human_node = 0

def human_node(state: State):
   global counter_human_node
   counter_human_node += 1 # This code will run again!
   print(f"Entered human_node in sub-graph a total of {counter_human_node} times")
   answer = interrupt("what is your name?")
   print(f"Got an answer of {answer}")

checkpointer = MemorySaver()

subgraph_builder = StateGraph(State)
subgraph_builder.add_node("some_node", node_in_subgraph)
subgraph_builder.add_node("human_node", human_node)
subgraph_builder.add_edge(START, "some_node")
subgraph_builder.add_edge("some_node", "human_node")
subgraph = subgraph_builder.compile(checkpointer=checkpointer)

counter_parent_node = 0

def parent_node(state: State):
   """This parent node will invoke the subgraph."""
   global counter_parent_node

   counter_parent_node += 1 # This code will run again on resuming!
   print(f"Entered `parent_node` a total of {counter_parent_node} times")

   # Please note that we're intentionally incrementing the state counter
   # in the graph state as well to demonstrate that the subgraph update
   # of the same key will not conflict with the parent graph (until
   subgraph_state = subgraph.invoke(state)
   return subgraph_state

builder = StateGraph(State)
builder.add_node("parent_node", parent_node)
builder.add_edge(START, "parent_node")

# A checkpointer must be enabled for interrupts to work!
checkpointer = MemorySaver()
graph = builder.compile(checkpointer=checkpointer)

config = {
   "configurable": {
      "thread_id": uuid.uuid4(),
   }
}

for chunk in graph.stream({"state_counter": 1}, config):
   print(chunk)

print('--- Resuming ---')

for chunk in graph.stream(Command(resume="35"), config):
   print(chunk)

This will print out
Entered `parent_node` a total of 1 times
Entered `node_in_subgraph` a total of 1 times
Entered human_node in sub-graph a total of 1 times
{'__interrupt__': (Interrupt(value='what is your name?', resumable=True, ns=['parent_node:4c3a0248-21f0-1287-eacf-3002bc304db4', 'human_node:2fe86d52-6f70-2a3f-6b2f-b1eededd6348'], when='during'),)}
--- Resuming ---
Entered `parent_node` a total of 2 times
Entered human_node in sub-graph a total of 2 times
Got an answer of 35
{'parent_node': {'state_counter': 1}}

Using multiple interrupts¶
Using multiple interrupts within a single node can be helpful for patterns like validating human input. However, using multiple interrupts in the same node can lead to unexpected behavior if not handled carefully.
When a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task's resume list. Matching is strictly index-based, so the order of interrupt calls within the node is critical.
To avoid issues, refrain from dynamically changing the node's structure between executions. This includes adding, removing, or reordering interrupt calls, as such changes can result in mismatched indices. These problems often arise from unconventional patterns, such as mutating state via Command(resume=..., update=SOME_STATE_MUTATION) or relying on global variables to modify the node‚Äôs structure dynamically.

Example of incorrect code
import uuid
from typing import TypedDict, Optional

from langgraph.graph import StateGraph
from langgraph.constants import START 
from langgraph.types import interrupt, Command
from langgraph.checkpoint.memory import MemorySaver

class State(TypedDict):
    """The graph state."""

    age: Optional[str]
    name: Optional[str]

def human_node(state: State):
    if not state.get('name'):
        name = interrupt("what is your name?")
    else:
        name = "N/A"

    if not state.get('age'):
        age = interrupt("what is your age?")
    else:
        age = "N/A"

    print(f"Name: {name}. Age: {age}")

    return {
        "age": age,
        "name": name,
    }

builder = StateGraph(State)
builder.add_node("human_node", human_node)
builder.add_edge(START, "human_node")

# A checkpointer must be enabled for interrupts to work!
checkpointer = MemorySaver()
graph = builder.compile(checkpointer=checkpointer)

config = {
    "configurable": {
        "thread_id": uuid.uuid4(),
    }
}

for chunk in graph.stream({"age": None, "name": None}, config):
    print(chunk)

for chunk in graph.stream(Command(resume="John", update={"name": "foo"}), config):
    print(chunk)

{'__interrupt__': (Interrupt(value='what is your name?', resumable=True, ns=['human_node:3a007ef9-c30d-c357-1ec1-86a1a70d8fba'], when='during'),)}
Name: N/A. Age: John
{'human_node': {'age': 'John', 'name': 'N/A'}}

Additional Resources üìö¶

Conceptual Guide: Persistence: Read the persistence guide for more context on replaying.
How to Guides: Human-in-the-loop: Learn how to implement human-in-the-loop workflows in LangGraph.
How to implement multi-turn conversations: Learn how to implement multi-turn conversations in LangGraph.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 34
SOURCE: https://langchain-ai.github.io/langgraph/concepts/pregel/
CONTENT:
LangGraph's Runtime (Pregel)¶
Pregel implements LangGraph's runtime, managing the execution of LangGraph applications.
Compiling a StateGraph or creating an entrypoint produces a Pregel instance that can be invoked with input.
This guide explains the runtime at a high level and provides instructions for directly implementing applications with Pregel.

Note: The Pregel runtime is named after Google's Pregel algorithm, which describes an efficient method for large-scale parallel computation using graphs.

Overview¶
In LangGraph, Pregel combines actors and channels into a single application. Actors read data from channels and write data to channels. Pregel organizes the execution of the application into multiple steps, following the Pregel Algorithm/Bulk Synchronous Parallel model.
Each step consists of three phases:

Plan: Determine which actors to execute in this step. For example, in the first step, select the actors that subscribe to the special input channels; in subsequent steps, select the actors that subscribe to channels updated in the previous step.
Execution: Execute all selected actors in parallel, until all complete, or one fails, or a timeout is reached. During this phase, channel updates are invisible to actors until the next step.
Update: Update the channels with the values written by the actors in this step.

Repeat until no actors are selected for execution, or a maximum number of steps is reached.
Actors¶
An actor is a PregelNode. It subscribes to channels, reads data from them, and writes data to them. It can be thought of as an actor in the Pregel algorithm. PregelNodes implement LangChain's Runnable interface.
Channels¶
Channels are used to communicate between actors (PregelNodes). Each channel has a value type, an update type, and an update function – which takes a sequence of updates and modifies the stored value. Channels can be used to send data from one chain to another, or to send data from a chain to itself in a future step. LangGraph provides a number of built-in channels:
Basic channels: LastValue and Topic¶

LastValue: The default channel, stores the last value sent to the channel, useful for input and output values, or for sending data from one step to the next.
Topic: A configurable PubSub Topic, useful for sending multiple values between actors, or for accumulating output. Can be configured to deduplicate values or to accumulate values over the course of multiple steps.

Advanced channels: Context and BinaryOperatorAggregate¶

Context: exposes the value of a context manager, managing its lifecycle. Useful for accessing external resources that require setup and/or teardown; e.g., client = Context(httpx.Client).
BinaryOperatorAggregate: stores a persistent value, updated by applying a binary operator to the current value and each update sent to the channel, useful for computing aggregates over multiple steps; e.g.,total = BinaryOperatorAggregate(int, operator.add)

Examples¶
While most users will interact with Pregel through the StateGraph API or 
the entrypoint decorator, it is possible to interact with Pregel directly.
Below are a few different examples to give you a sense of the Pregel API.
Single nodeMultiple nodesTopicBinaryOperatorAggregateCycle

from langgraph.channels import EphemeralValue
from langgraph.pregel import Pregel, Channel 

node1 = (
    Channel.subscribe_to("a")
    | (lambda x: x + x)
    | Channel.write_to("b")
)

app = Pregel(
    nodes={"node1": node1},
    channels={
        "a": EphemeralValue(str),
        "b": EphemeralValue(str),
    },
    input_channels=["a"],
    output_channels=["b"],
)

app.invoke({"a": "foo"})

{'b': 'foofoo'}

from langgraph.channels import LastValue, EphemeralValue
from langgraph.pregel import Pregel, Channel 

node1 = (
    Channel.subscribe_to("a")
    | (lambda x: x + x)
    | Channel.write_to("b")
)

node2 = (
    Channel.subscribe_to("b")
    | (lambda x: x + x)
    | Channel.write_to("c")
)

app = Pregel(
    nodes={"node1": node1, "node2": node2},
    channels={
        "a": EphemeralValue(str),
        "b": LastValue(str),
        "c": EphemeralValue(str),
    },
    input_channels=["a"],
    output_channels=["b", "c"],
)

app.invoke({"a": "foo"})

{'b': 'foofoo', 'c': 'foofoofoofoo'}

from langgraph.channels import EphemeralValue, Topic
from langgraph.pregel import Pregel, Channel 

node1 = (
    Channel.subscribe_to("a")
    | (lambda x: x + x)
    | {
        "b": Channel.write_to("b"),
        "c": Channel.write_to("c")
    }
)

node2 = (
    Channel.subscribe_to("b")
    | (lambda x: x + x)
    | {
        "c": Channel.write_to("c"),
    }
)

app = Pregel(
    nodes={"node1": node1, "node2": node2},
    channels={
        "a": EphemeralValue(str),
        "b": EphemeralValue(str),
        "c": Topic(str, accumulate=True),
    },
    input_channels=["a"],
    output_channels=["c"],
)

app.invoke({"a": "foo"})

{'c': ['foofoo', 'foofoofoofoo']}

This examples demonstrates how to use the BinaryOperatorAggregate channel to implement a reducer.
from langgraph.channels import EphemeralValue, BinaryOperatorAggregate
from langgraph.pregel import Pregel, Channel

node1 = (
    Channel.subscribe_to("a")
    | (lambda x: x + x)
    | {
        "b": Channel.write_to("b"),
        "c": Channel.write_to("c")
    }
)

node2 = (
    Channel.subscribe_to("b")
    | (lambda x: x + x)
    | {
        "c": Channel.write_to("c"),
    }
)

def reducer(current, update):
    if current:
        return current + " | " + "update"
    else:
        return update

app = Pregel(
    nodes={"node1": node1, "node2": node2},
    channels={
        "a": EphemeralValue(str),
        "b": EphemeralValue(str),
        "c": BinaryOperatorAggregate(str, operator=reducer),
    },
    input_channels=["a"],
    output_channels=["c"],
)

app.invoke({"a": "foo"})

This example demonstrates how to introduce a cycle in the graph, by having
a chain write to a channel it subscribes to. Execution will continue
until a None value is written to the channel.
from langgraph.channels import EphemeralValue
from langgraph.pregel import Pregel, Channel, ChannelWrite, ChannelWriteEntry

example_node = (
    Channel.subscribe_to("value")
    | (lambda x: x + x if len(x) < 10 else None)
    | ChannelWrite(writes=[ChannelWriteEntry(channel="value", skip_none=True)])
)

app = Pregel(
    nodes={"example_node": example_node},
    channels={
        "value": EphemeralValue(str),
    },
    input_channels=["value"],
    output_channels=["value"],
)

app.invoke({"value": "a"})

{'value': 'aaaaaaaaaaaaaaaa'}

High-level API¶
LangGraph provides two high-level APIs for creating a Pregel application: the StateGraph (Graph API) and the Functional API.
StateGraph (Graph API)Functional API

The StateGraph (Graph API) is a higher-level abstraction that simplifies the creation of Pregel applications. It allows you to define a graph of nodes and edges. When you compile the graph, the StateGraph API automatically creates the Pregel application for you.
from typing import TypedDict, Optional

from langgraph.constants import START
from langgraph.graph import StateGraph

class Essay(TypedDict):
    topic: str
    content: Optional[str]
    score: Optional[float]

def write_essay(essay: Essay):
    return {
        "content": f"Essay about {essay['topic']}",
    }

def score_essay(essay: Essay):
    return {
        "score": 10
    }

builder = StateGraph(Essay)
builder.add_node(write_essay)
builder.add_node(score_essay)
builder.add_edge(START, "write_essay")

# Compile the graph. 
# This will return a Pregel instance.
graph = builder.compile()

The compiled Pregel instance will be associated with a list of nodes and channels. You can inspect the nodes and channels by printing them.
print(graph.nodes)

You will see something like this:
{'__start__': <langgraph.pregel.read.PregelNode at 0x7d05e3ba1810>,
 'write_essay': <langgraph.pregel.read.PregelNode at 0x7d05e3ba14d0>,
 'score_essay': <langgraph.pregel.read.PregelNode at 0x7d05e3ba1710>}

print(graph.channels)

You should see something like this
{'topic': <langgraph.channels.last_value.LastValue at 0x7d05e3294d80>,
 'content': <langgraph.channels.last_value.LastValue at 0x7d05e3295040>,
 'score': <langgraph.channels.last_value.LastValue at 0x7d05e3295980>,
 '__start__': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e3297e00>,
 'write_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e32960c0>,
 'score_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d8ab80>,
 'branch:__start__:__self__:write_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e32941c0>,
 'branch:__start__:__self__:score_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d88800>,
 'branch:write_essay:__self__:write_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e3295ec0>,
 'branch:write_essay:__self__:score_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d8ac00>,
 'branch:score_essay:__self__:write_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d89700>,
 'branch:score_essay:__self__:score_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d8b400>,
 'start:write_essay': <langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d8b280>}

In the Functional API, you can use an entrypoint to create
a Pregel application. The entrypoint decorator allows you to define a function that takes input and returns output. 
from typing import TypedDict, Optional

from langgraph.checkpoint.memory import InMemorySaver
from langgraph.func import entrypoint

class Essay(TypedDict):
    topic: str
    content: Optional[str]
    score: Optional[float]

checkpointer = InMemorySaver()

@entrypoint(checkpointer=checkpointer)
def write_essay(essay: Essay):
    return {
        "content": f"Essay about {essay['topic']}",
    }

print("Nodes: ")
print(write_essay.nodes)
print("Channels: ")
print(write_essay.channels)

Nodes: 
{'write_essay': <langgraph.pregel.read.PregelNode object at 0x7d05e2f9aad0>}
Channels: 
{'__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7d05e2c906c0>, '__end__': <langgraph.channels.last_value.LastValue object at 0x7d05e2c90c40>, '__previous__': <langgraph.channels.last_value.LastValue object at 0x7d05e1007280>}

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 35
SOURCE: https://langchain-ai.github.io/langgraph/concepts/sdk/
CONTENT:
LangGraph SDK¶

Prerequisites

LangGraph Platform
LangGraph Server

The LangGraph Platform provides both a Python and JS SDK for interacting with the LangGraph Server API. 
Installation¶
You can install the packages using the appropriate package manager for your language.
PythonJS

pip install langgraph-sdk

yarn add @langchain/langgraph-sdk

API Reference¶
You can find the API reference for the SDKs here:

Python SDK Reference
JS/TS SDK Reference

Python Sync vs. Async¶
The Python SDK provides both synchronous (get_sync_client) and asynchronous (get_client) clients for interacting with the LangGraph Server API.
AsyncSync

from langgraph_sdk import get_client

client = get_client(url=..., api_key=...)
await client.assistants.search()

from langgraph_sdk import get_sync_client

client = get_sync_client(url=..., api_key=...)
client.assistants.search()

Related¶

LangGraph CLI API Reference
Python SDK Reference
JS/TS SDK Reference

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 36
SOURCE: https://langchain-ai.github.io/langgraph/concepts/auth/
CONTENT:
Authentication & Access Control¶
LangGraph Platform provides a flexible authentication and authorization system that can integrate with most authentication schemes.
Core Concepts¶
Authentication vs Authorization¶
While often used interchangeably, these terms represent distinct security concepts:

Authentication ("AuthN") verifies who you are. This runs as middleware for every request.
Authorization ("AuthZ") determines what you can do. This validates the user's privileges and roles on a per-resource basis.

In LangGraph Platform, authentication is handled by your @auth.authenticate handler, and authorization is handled by your @auth.on handlers.
Default Security Models¶
LangGraph Platform provides different security defaults:
LangGraph Cloud¶

Uses LangSmith API keys by default
Requires valid API key in x-api-key header
Can be customized with your auth handler

Custom auth
Custom auth is supported for all plans in LangGraph Cloud.

Self-Hosted¶

No default authentication
Complete flexibility to implement your security model
You control all aspects of authentication and authorization

Custom auth
Custom auth is supported for Enterprise self-hosted plans.
Self-hosted lite plans do not support custom auth natively.

System Architecture¶
A typical authentication setup involves three main components:

Authentication Provider (Identity Provider/IdP)

A dedicated service that manages user identities and credentials
Handles user registration, login, password resets, etc.
Issues tokens (JWT, session tokens, etc.) after successful authentication
Examples: Auth0, Supabase Auth, Okta, or your own auth server

LangGraph Backend (Resource Server)

Your LangGraph application that contains business logic and protected resources
Validates tokens with the auth provider
Enforces access control based on user identity and permissions
Doesn't store user credentials directly

Client Application (Frontend)

Web app, mobile app, or API client
Collects time-sensitive user credentials and sends to auth provider
Receives tokens from auth provider
Includes these tokens in requests to LangGraph backend

Here's how these components typically interact:
sequenceDiagram
    participant Client as Client App
    participant Auth as Auth Provider
    participant LG as LangGraph Backend

    Client->>Auth: 1. Login (username/password)
    Auth-->>Client: 2. Return token
    Client->>LG: 3. Request with token
    Note over LG: 4. Validate token (@auth.authenticate)
    LG-->>Auth:  5. Fetch user info
    Auth-->>LG: 6. Confirm validity
    Note over LG: 7. Apply access control (@auth.on.*)
    LG-->>Client: 8. Return resources
Your @auth.authenticate handler in LangGraph handles steps 4-6, while your @auth.on handlers implement step 7.
Authentication¶
Authentication in LangGraph runs as middleware on every request. Your @auth.authenticate handler receives request information and should:

Validate the credentials
Return user info containing the user's identity and user information if valid
Raise an HTTP exception or AssertionError if invalid

from langgraph_sdk import Auth

auth = Auth()

@auth.authenticate
async def authenticate(headers: dict) -> Auth.types.MinimalUserDict:
    # Validate credentials (e.g., API key, JWT token)
    api_key = headers.get("x-api-key")
    if not api_key or not is_valid_key(api_key):
        raise Auth.exceptions.HTTPException(
            status_code=401,
            detail="Invalid API key"
        )

    # Return user info - only identity and is_authenticated are required
    # Add any additional fields you need for authorization
    return {
        "identity": "user-123",        # Required: unique user identifier
        "is_authenticated": True,      # Optional: assumed True by default
        "permissions": ["read", "write"] # Optional: for permission-based auth
        # You can add more custom fields if you want to implement other auth patterns
        "role": "admin",
        "org_id": "org-456"

    }

The returned user information is available:

To your authorization handlers via ctx.user
In your application via config["configuration"]["langgraph_auth_user"]

Supported Parameters
The @auth.authenticate handler can accept any of the following parameters by name:

request (Request): The raw ASGI request object
body (dict): The parsed request body
path (str): The request path, e.g., "/threads/abcd-1234-abcd-1234/runs/abcd-1234-abcd-1234/stream"
method (str): The HTTP method, e.g., "GET"
path_params (dict[str, str]): URL path parameters, e.g., {"thread_id": "abcd-1234-abcd-1234", "run_id": "abcd-1234-abcd-1234"}
query_params (dict[str, str]): URL query parameters, e.g., {"stream": "true"}
headers (dict[bytes, bytes]): Request headers
authorization (str | None): The Authorization header value (e.g., "Bearer ")

In many of our tutorials, we will just show the "authorization" parameter to be concise, but you can opt to accept more information as needed
to implement your custom authentication scheme.

Authorization¶
After authentication, LangGraph calls your @auth.on handlers to control access to specific resources (e.g., threads, assistants, crons). These handlers can:

Add metadata to be saved during resource creation by mutating the value["metadata"] dictionary directly. See the supported actions table for the list of types the value can take for each action.
Filter resources by metadata during search/list or read operations by returning a filter dictionary.
Raise an HTTP exception if access is denied.

If you want to just implement simple user-scoped access control, you can use a single @auth.on handler for all resources and actions. If you want to have different control depending on the resource and action, you can use resource-specific handlers. See the Supported Resources section for a full list of the resources that support access control.
@auth.on
async def add_owner(
    ctx: Auth.types.AuthContext,
    value: dict  # The payload being sent to this access method
) -> dict:  # Returns a filter dict that restricts access to resources
    """Authorize all access to threads, runs, crons, and assistants.

    This handler does two things:
        - Adds a value to resource metadata (to persist with the resource so it can be filtered later)
        - Returns a filter (to restrict access to existing resources)

    Args:
        ctx: Authentication context containing user info, permissions, the path, and
        value: The request payload sent to the endpoint. For creation
              operations, this contains the resource parameters. For read
              operations, this contains the resource being accessed.

    Returns:
        A filter dictionary that LangGraph uses to restrict access to resources.
        See [Filter Operations](#filter-operations) for supported operators.
    """
    # Create filter to restrict access to just this user's resources
    filters = {"owner": ctx.user.identity}

    # Get or create the metadata dictionary in the payload
    # This is where we store persistent info about the resource
    metadata = value.setdefault("metadata", {})

    # Add owner to metadata - if this is a create or update operation,
    # this information will be saved with the resource
    # So we can filter by it later in read operations
    metadata.update(filters)

    # Return filters to restrict access
    # These filters are applied to ALL operations (create, read, update, search, etc.)
    # to ensure users can only access their own resources
    return filters

Resource-Specific Handlers¶
You can register handlers for specific resources and actions by chaining the resource and action names together with the @auth.on decorator.
When a request is made, the most specific handler that matches that resource and action is called. Below is an example of how to register handlers for specific resources and actions. For the following setup:

Authenticated users are able to create threads, read thread, create runs on threads
Only users with the "assistants:create" permission are allowed to create new assistants
All other endpoints (e.g., e.g., delete assistant, crons, store) are disabled for all users.

Supported Handlers
For a full list of supported resources and actions, see the Supported Resources section below.

# Generic / global handler catches calls that aren't handled by more specific handlers
@auth.on
async def reject_unhandled_requests(ctx: Auth.types.AuthContext, value: Any) -> False:
    print(f"Request to {ctx.path} by {ctx.user.identity}")
    raise Auth.exceptions.HTTPException(
        status_code=403,
        detail="Forbidden"
    )

# Matches the "thread" resource and all actions - create, read, update, delete, search
# Since this is **more specific** than the generic @auth.on handler, it will take precedence
# over the generic handler for all actions on the "threads" resource
@auth.on.threads
async def on_thread_create(
    ctx: Auth.types.AuthContext,
    value: Auth.types.threads.create.value
):
    if "write" not in ctx.permissions:
        raise Auth.exceptions.HTTPException(
            status_code=403,
            detail="User lacks the required permissions."
        )
    # Setting metadata on the thread being created
    # will ensure that the resource contains an "owner" field
    # Then any time a user tries to access this thread or runs within the thread,
    # we can filter by owner
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    return {"owner": ctx.user.identity}

# Thread creation. This will match only on thread create actions
# Since this is **more specific** than both the generic @auth.on handler and the @auth.on.threads handler,
# it will take precedence for any "create" actions on the "threads" resources
@auth.on.threads.create
async def on_thread_create(
    ctx: Auth.types.AuthContext,
    value: Auth.types.threads.create.value
):
    # Setting metadata on the thread being created
    # will ensure that the resource contains an "owner" field
    # Then any time a user tries to access this thread or runs within the thread,
    # we can filter by owner
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    return {"owner": ctx.user.identity}

# Reading a thread. Since this is also more specific than the generic @auth.on handler, and the @auth.on.threads handler,
# it will take precedence for any "read" actions on the "threads" resource
@auth.on.threads.read
async def on_thread_read(
    ctx: Auth.types.AuthContext,
    value: Auth.types.threads.read.value
):
    # Since we are reading (and not creating) a thread,
    # we don't need to set metadata. We just need to
    # return a filter to ensure users can only see their own threads
    return {"owner": ctx.user.identity}

# Run creation, streaming, updates, etc.
# This takes precedenceover the generic @auth.on handler and the @auth.on.threads handler
@auth.on.threads.create_run
async def on_run_create(
    ctx: Auth.types.AuthContext,
    value: Auth.types.threads.create_run.value
):
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    # Inherit thread's access control
    return {"owner": ctx.user.identity}

# Assistant creation
@auth.on.assistants.create
async def on_assistant_create(
    ctx: Auth.types.AuthContext,
    value: Auth.types.assistants.create.value
):
    if "assistants:create" not in ctx.permissions:
        raise Auth.exceptions.HTTPException(
            status_code=403,
            detail="User lacks the required permissions."
        )

Notice that we are mixing global and resource-specific handlers in the above example. Since each request is handled by the most specific handler, a request to create a thread would match the on_thread_create handler but NOT the reject_unhandled_requests handler. A request to update a thread, however would be handled by the global handler, since we don't have a more specific handler for that resource and action.
Filter Operations¶
Authorization handlers can return None, a boolean, or a filter dictionary.
- None and True mean "authorize access to all underling resources"
- False means "deny access to all underling resources (raises a 403 exception)"
- A metadata filter dictionary will restrict access to resources
A filter dictionary is a dictionary with keys that match the resource metadata. It supports three operators:

The default value is a shorthand for exact match, or "$eq", below. For example, {"owner": user_id} will include only resources with metadata containing {"owner": user_id}
$eq: Exact match (e.g., {"owner": {"$eq": user_id}}) - this is equivalent to the shorthand above, {"owner": user_id}
$contains: List membership (e.g., {"allowed_users": {"$contains": user_id}}) The value here must be an element of the list. The metadata in the stored resource must be a list/container type.

A dictionary with multiple keys is treated using a logical AND filter. For example, {"owner": org_id, "allowed_users": {"$contains": user_id}} will only match resources with metadata whose "owner" is org_id and whose "allowed_users" list contains user_id.
See the reference here for more information.
Common Access Patterns¶
Here are some typical authorization patterns:
Single-Owner Resources¶
This common pattern lets you scope all threads, assistants, crons, and runs to a single user. It's useful for common single-user use cases like regular chatbot-style apps.
@auth.on
async def owner_only(ctx: Auth.types.AuthContext, value: dict):
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    return {"owner": ctx.user.identity}

Permission-based Access¶
This pattern lets you control access based on permissions. It's useful if you want certain roles to have broader or more restricted access to resources.
# In your auth handler:
@auth.authenticate
async def authenticate(headers: dict) -> Auth.types.MinimalUserDict:
    ...
    return {
        "identity": "user-123",
        "is_authenticated": True,
        "permissions": ["threads:write", "threads:read"]  # Define permissions in auth
    }

def _default(ctx: Auth.types.AuthContext, value: dict):
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    return {"owner": ctx.user.identity}

@auth.on.threads.create
async def create_thread(ctx: Auth.types.AuthContext, value: dict):
    if "threads:write" not in ctx.permissions:
        raise Auth.exceptions.HTTPException(
            status_code=403,
            detail="Unauthorized"
        )
    return _default(ctx, value)

@auth.on.threads.read
async def rbac_create(ctx: Auth.types.AuthContext, value: dict):
    if "threads:read" not in ctx.permissions and "threads:write" not in ctx.permissions:
        raise Auth.exceptions.HTTPException(
            status_code=403,
            detail="Unauthorized"
        )
    return _default(ctx, value)

Supported Resources¶
LangGraph provides three levels of authorization handlers, from most general to most specific:

Global Handler (@auth.on): Matches all resources and actions
Resource Handler (e.g., @auth.on.threads, @auth.on.assistants, @auth.on.crons): Matches all actions for a specific resource
Action Handler (e.g., @auth.on.threads.create, @auth.on.threads.read): Matches a specific action on a specific resource

The most specific matching handler will be used. For example, @auth.on.threads.create takes precedence over @auth.on.threads for thread creation.
If a more specific handler is registered, the more general handler will not be called for that resource and action.

Type Safety
Each handler has type hints available for its value parameter at Auth.types.on.<resource>.<action>.value. For example:
@auth.on.threads.create
async def on_thread_create(
    ctx: Auth.types.AuthContext,
    value: Auth.types.on.threads.create.value  # Specific type for thread creation
):
    ...

@auth.on.threads
async def on_threads(
    ctx: Auth.types.AuthContext,
    value: Auth.types.on.threads.value  # Union type of all thread actions
):
    ...

@auth.on
async def on_all(
    ctx: Auth.types.AuthContext,
    value: dict  # Union type of all possible actions
):
    ...

More specific handlers provide better type hints since they handle fewer action types.

Supported actions and types¶
Here are all the supported action handlers:

Resource
Handler
Description
Value Type

Threads
@auth.on.threads.create
Thread creation
ThreadsCreate

@auth.on.threads.read
Thread retrieval
ThreadsRead

@auth.on.threads.update
Thread updates
ThreadsUpdate

@auth.on.threads.delete
Thread deletion
ThreadsDelete

@auth.on.threads.search
Listing threads
ThreadsSearch

@auth.on.threads.create_run
Creating or updating a run
RunsCreate

Assistants
@auth.on.assistants.create
Assistant creation
AssistantsCreate

@auth.on.assistants.read
Assistant retrieval
AssistantsRead

@auth.on.assistants.update
Assistant updates
AssistantsUpdate

@auth.on.assistants.delete
Assistant deletion
AssistantsDelete

@auth.on.assistants.search
Listing assistants
AssistantsSearch

Crons
@auth.on.crons.create
Cron job creation
CronsCreate

@auth.on.crons.read
Cron job retrieval
CronsRead

@auth.on.crons.update
Cron job updates
CronsUpdate

@auth.on.crons.delete
Cron job deletion
CronsDelete

@auth.on.crons.search
Listing cron jobs
CronsSearch

About Runs
Runs are scoped to their parent thread for access control. This means permissions are typically inherited from the thread, reflecting the conversational nature of the data model. All run operations (reading, listing) except creation are controlled by the thread's handlers.
There is a specific create_run handler for creating new runs because it had more arguments that you can view in the handler.

Next Steps¶
For implementation details:

Check out the introductory tutorial on setting up authentication
See the how-to guide on implementing a custom auth handlers

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 37
SOURCE: https://langchain-ai.github.io/langgraph/concepts/double_texting/
CONTENT:
Double Texting¶

Prerequisites

LangGraph Server

Many times users might interact with your graph in unintended ways. 
For instance, a user may send one message and before the graph has finished running send a second message. 
More generally, users may invoke the graph a second time before the first run has finished.
We call this "double texting".
Currently, LangGraph only addresses this as part of LangGraph Platform, not in the open source.
The reason for this is that in order to handle this we need to know how the graph is deployed, and since LangGraph Platform deals with deployment the logic needs to live there.
If you do not want to use LangGraph Platform, we describe the options we have implemented in detail below.

Reject¶
This is the simplest option, this just rejects any follow-up runs and does not allow double texting. 
See the how-to guide for configuring the reject double text option.
Enqueue¶
This is a relatively simple option which continues the first run until it completes the whole run, then sends the new input as a separate run. 
See the how-to guide for configuring the enqueue double text option.
Interrupt¶
This option interrupts the current execution but saves all the work done up until that point. 
It then inserts the user input and continues from there. 
If you enable this option, your graph should be able to handle weird edge cases that may arise. 
For example, you could have called a tool but not yet gotten back a result from running that tool.
You may need to remove that tool call in order to not have a dangling tool call.
See the how-to guide for configuring the interrupt double text option.
Rollback¶
This option interrupts the current execution AND rolls back all work done up until that point, including the original run input. It then sends the new user input in, basically as if it was the original input.
See the how-to guide for configuring the rollback double text option.

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 38
SOURCE: https://langchain-ai.github.io/langgraph/concepts/assistants/
CONTENT:
Assistants¶

Prerequisites

LangGraph Server

When building agents, it is fairly common to make rapid changes that do not alter the graph logic. For example, simply changing prompts or the LLM selection can have significant impacts on the behavior of the agents. Assistants offer an easy way to make and save these types of changes to agent configuration. This can have at least two use-cases:

Assistants give developers a quick and easy way to modify and version agents for experimentation.
Assistants can be modified via LangGraph Studio, offering a no-code way to configure agents  (e.g., for business users). 

Assistants build off the concept of "configuration". 
While "configuration" is available in the open source LangGraph library as well,  assistants are only present in LangGraph Platform.
This is because Assistants are tightly coupled to your deployed graph, and so we can only make them available when we are also deploying the graphs.
Configuring Assistants¶
In practice, an assistant is just an instance of a graph with a specific configuration. Because of this, multiple assistants can reference the same graph but can contain different configurations, such as prompts, models, and other graph configuration options. The LangGraph Cloud API provides several endpoints for creating and managing assistants. See the API reference and this how-to for more details on how to create assistants.
Versioning Assistants¶
Once you've created an assistant, you can save and version it to track changes to the configuration over time. You can think about this at three levels:
1) The graph lays out the general agent application logic 
2) The agent configuration options represent parameters that can be changed 
3) Assistant versions save and track specific settings of the agent configuration options 
For example, let's imagine you have a general writing agent. You have created a general graph architecture that works well for writing. However, there are different types of writing, e.g. blogs vs tweets. In order to get the best performance on each use case, you need to make some minor changes to the models and prompts used. In this setup, you could create an assistant for each use case - one for blog writing and one for tweeting. These would share the same graph structure, but they may use different models and different prompts. Read this how-to to learn how you can use assistant versioning through both the Studio and the SDK.

Resources¶
For more information on assistants, see the following resources:

Assistants how-to guides

        Was this page helpful?
      

              
              
                
              
              Thanks for your feedback!
            

              
              
                
              
              Thanks for your feedback! Please help us improve this page by adding to the discussion below.
            

Comments

================================================================================

DOCUMENT 39
SOURCE: https://langchain-ai.github.io/langgraph/concepts/langgraph_data_plane/&
CONTENT:
404 - Not found

================================================================================

