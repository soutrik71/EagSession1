{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, os\n",
    "import tiktoken\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text, model=\"cl100k_base\"):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in the text using tiktoken.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to count tokens for\n",
    "        model (str): The tokenizer model to use (default: cl100k_base for GPT-4)\n",
    "\n",
    "    Returns:\n",
    "        int: Number of tokens in the text\n",
    "    \"\"\"\n",
    "    encoder = tiktoken.get_encoding(model)\n",
    "    return len(encoder.encode(text))\n",
    "\n",
    "\n",
    "def bs4_extractor(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text content from HTML using BeautifulSoup.\n",
    "\n",
    "    Args:\n",
    "        html (str): Raw HTML content to extract text from\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text content with excess whitespace removed\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Target the main article content for LangGraph documentation\n",
    "    main_content = soup.find(\"article\", class_=\"md-content__inner\")\n",
    "\n",
    "    # If found, use that, otherwise fall back to the whole document\n",
    "    content = main_content.get_text() if main_content else soup.text\n",
    "\n",
    "    # Clean up whitespace\n",
    "    content = re.sub(r\"\\n\\n+\", \"\\n\\n\", content).strip()\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def load_langgraph_docs():\n",
    "    \"\"\"\n",
    "    Load LangGraph documentation from the official website.\n",
    "\n",
    "    This function:\n",
    "    1. Uses RecursiveUrlLoader to fetch pages from the LangGraph website\n",
    "    2. Counts the total documents and tokens loaded\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Document objects containing the loaded content\n",
    "        list: A list of tokens per document\n",
    "    \"\"\"\n",
    "    print(\"Loading LangGraph documentation...\")\n",
    "\n",
    "    # Load the documentation\n",
    "    urls = [\n",
    "        \"https://langchain-ai.github.io/langgraph/concepts/\",\n",
    "    ]\n",
    "\n",
    "    docs = []\n",
    "    for url in urls:\n",
    "\n",
    "        loader = RecursiveUrlLoader(\n",
    "            url,\n",
    "            max_depth=7,\n",
    "            extractor=bs4_extractor,\n",
    "        )\n",
    "\n",
    "        # Load documents using lazy loading (memory efficient)\n",
    "        docs_lazy = loader.lazy_load()\n",
    "\n",
    "        # Load documents and track URLs\n",
    "        for d in docs_lazy:\n",
    "            docs.append(d)\n",
    "\n",
    "    print(f\"Loaded {len(docs)} documents from LangGraph documentation.\")\n",
    "    print(\"\\nLoaded URLs:\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"{i+1}. {doc.metadata.get('source', 'Unknown URL')}\")\n",
    "\n",
    "    # Count total tokens in documents\n",
    "    total_tokens = 0\n",
    "    tokens_per_doc = []\n",
    "    for doc in docs:\n",
    "        total_tokens += count_tokens(doc.page_content)\n",
    "        tokens_per_doc.append(count_tokens(doc.page_content))\n",
    "    print(f\"Total tokens in loaded documents: {total_tokens}\")\n",
    "\n",
    "    return docs, tokens_per_doc\n",
    "\n",
    "\n",
    "def save_llms_full(documents):\n",
    "    \"\"\"Save the documents to a file\"\"\"\n",
    "\n",
    "    # Open the output file\n",
    "    os.makedirs(os.path.join(os.getcwd(), \"saved_vector_stores\"), exist_ok=True)\n",
    "    output_filename = os.path.join(os.getcwd(), \"saved_vector_stores\", \"saved_llms_full.txt\")\n",
    "\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        # Write each document\n",
    "        for i, doc in enumerate(documents):\n",
    "            # Get the source (URL) from metadata\n",
    "            source = doc.metadata.get(\"source\", \"Unknown URL\")\n",
    "\n",
    "            # Write the document with proper formatting\n",
    "            f.write(f\"DOCUMENT {i+1}\\n\")\n",
    "            f.write(f\"SOURCE: {source}\\n\")\n",
    "            f.write(\"CONTENT:\\n\")\n",
    "            f.write(doc.page_content)\n",
    "            f.write(\"\\n\\n\" + \"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Documents concatenated into {output_filename}\")\n",
    "\n",
    "\n",
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for improved retrieval.\n",
    "\n",
    "    This function:\n",
    "    1. Uses RecursiveCharacterTextSplitter with tiktoken to create semantically meaningful chunks\n",
    "    2. Ensures chunks are appropriately sized for embedding and retrieval\n",
    "    3. Counts the resulting chunks and their total tokens\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of Document objects to split\n",
    "\n",
    "    Returns:\n",
    "        list: A list of split Document objects\n",
    "    \"\"\"\n",
    "    print(\"Splitting documents...\")\n",
    "\n",
    "    # Initialize text splitter using tiktoken for accurate token counting\n",
    "    # chunk_size=8,000 creates relatively large chunks for comprehensive context\n",
    "    # chunk_overlap=500 ensures continuity between chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=8000, chunk_overlap=500\n",
    "    )\n",
    "\n",
    "    # Split documents into chunks\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"Created {len(split_docs)} chunks from documents.\")\n",
    "\n",
    "    # Count total tokens in split documents\n",
    "    total_tokens = 0\n",
    "    for doc in split_docs:\n",
    "        total_tokens += count_tokens(doc.page_content)\n",
    "\n",
    "    print(f\"Total tokens in split documents: {total_tokens}\")\n",
    "\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(splits):\n",
    "    \"\"\"\n",
    "    Create a vector store from document chunks using SKLearnVectorStore.\n",
    "\n",
    "    This function:\n",
    "    1. Initializes an embedding model to convert text into vector representations\n",
    "    2. Creates a vector store from the document chunks\n",
    "\n",
    "    Args:\n",
    "        splits (list): List of split Document objects to embed\n",
    "\n",
    "    Returns:\n",
    "        SKLearnVectorStore: A vector store containing the embedded documents\n",
    "    \"\"\"\n",
    "    print(\"Creating SKLearnVectorStore...\")\n",
    "\n",
    "    # Initialize OpenAI embeddings\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "    # Create vector store from documents using SKLearn\n",
    "    os.makedirs(os.path.join(os.getcwd(), \"saved_vector_stores\"), exist_ok=True)\n",
    "    persist_path = os.getcwd() + \"/saved_vector_stores/sklearn_vectorstore.parquet\"\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embeddings,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\",\n",
    "    )\n",
    "    print(\"SKLearnVectorStore created successfully.\")\n",
    "\n",
    "    vectorstore.persist()\n",
    "    print(\"SKLearnVectorStore was persisted to\", persist_path)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LangGraph documentation...\n",
      "Loaded 41 documents from LangGraph documentation.\n",
      "\n",
      "Loaded URLs:\n",
      "1. https://langchain-ai.github.io/langgraph/concepts/\n",
      "2. https://langchain-ai.github.io/langgraph/concepts/langgraph_self_hosted_data_plane/\n",
      "3. https://langchain-ai.github.io/langgraph/concepts/langgraph_self_hosted_control_plane/\n",
      "4. https://langchain-ai.github.io/langgraph/concepts/langgraph_data_plane/\n",
      "5. https://langchain-ai.github.io/langgraph/concepts/langgraph_data_plane/&\n",
      "6. https://langchain-ai.github.io/langgraph/concepts/langgraph_cli/\n",
      "7. https://langchain-ai.github.io/langgraph/concepts/langgraph_control_plane/\n",
      "8. https://langchain-ai.github.io/langgraph/concepts/langgraph_control_plane/&\n",
      "9. https://langchain-ai.github.io/langgraph/concepts/application_structure/\n",
      "10. https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/\n",
      "11. https://langchain-ai.github.io/langgraph/concepts/sdk/\n",
      "12. https://langchain-ai.github.io/langgraph/concepts/template_applications/\n",
      "13. https://langchain-ai.github.io/langgraph/concepts/faq/\n",
      "14. https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/\n",
      "15. https://langchain-ai.github.io/langgraph/concepts/langgraph_server/\n",
      "16. https://langchain-ai.github.io/langgraph/concepts/langgraph_cloud/\n",
      "17. https://langchain-ai.github.io/langgraph/concepts/high_level/\n",
      "18. https://langchain-ai.github.io/langgraph/concepts/persistence/\n",
      "19. https://langchain-ai.github.io/langgraph/concepts/langgraph_standalone_container/\n",
      "20. https://langchain-ai.github.io/langgraph/concepts/self_hosted/\n",
      "21. https://langchain-ai.github.io/langgraph/concepts/deployment_options/\n",
      "22. https://langchain-ai.github.io/langgraph/concepts/auth/\n",
      "23. https://langchain-ai.github.io/langgraph/concepts/assistants/\n",
      "24. https://langchain-ai.github.io/langgraph/concepts/double_texting/\n",
      "25. https://langchain-ai.github.io/langgraph/concepts/low_level/\n",
      "26. https://langchain-ai.github.io/langgraph/concepts/platform_architecture/\n",
      "27. https://langchain-ai.github.io/langgraph/concepts/plans/\n",
      "28. https://langchain-ai.github.io/langgraph/concepts/plans/&\n",
      "29. https://langchain-ai.github.io/langgraph/concepts/bring_your_own_cloud/\n",
      "30. https://langchain-ai.github.io/langgraph/concepts/scalability_and_resilience/\n",
      "31. https://langchain-ai.github.io/langgraph/concepts/pregel/\n",
      "32. https://langchain-ai.github.io/langgraph/concepts/multi_agent/\n",
      "33. https://langchain-ai.github.io/langgraph/concepts/streaming/\n",
      "34. https://langchain-ai.github.io/langgraph/concepts/functional_api/\n",
      "35. https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\n",
      "36. https://langchain-ai.github.io/langgraph/concepts/v0-human-in-the-loop/\n",
      "37. https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/\n",
      "38. https://langchain-ai.github.io/langgraph/concepts/durable_execution/\n",
      "39. https://langchain-ai.github.io/langgraph/concepts/memory/\n",
      "40. https://langchain-ai.github.io/langgraph/concepts/breakpoints/\n",
      "41. https://langchain-ai.github.io/langgraph/concepts/time-travel/\n",
      "Total tokens in loaded documents: 65732\n",
      "Documents concatenated into c:\\workspace\\EagSession1\\mcp_full_course\\llm_codes\\saved_vector_stores\\saved_llms_full.txt\n",
      "Splitting documents...\n",
      "Created 42 chunks from documents.\n",
      "Total tokens in split documents: 66069\n"
     ]
    }
   ],
   "source": [
    "# Load the documents\n",
    "documents, tokens_per_doc = load_langgraph_docs()\n",
    "\n",
    "# Save the documents to a file\n",
    "save_llms_full(documents)\n",
    "\n",
    "# Split the documents\n",
    "split_docs = split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SKLearnVectorStore...\n",
      "SKLearnVectorStore created successfully.\n",
      "SKLearnVectorStore was persisted to c:\\workspace\\EagSession1\\mcp_full_course\\llm_codes/saved_vector_stores/sklearn_vectorstore.parquet\n"
     ]
    }
   ],
   "source": [
    "# Create the vector store\n",
    "vectorstore = create_vectorstore(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 relevant documents\n",
      "https://langchain-ai.github.io/langgraph/concepts/high_level/\n",
      "Why LangGraph?¶\n",
      "LLM applications¶\n",
      "LLMs make it possible to embed intelligence into a new class of applications. There are many patterns for building applications that use LLMs. Workflows have scaffolding of predefined code paths around LLM calls. LLMs can direct the control flow through these predefined code paths, which some consider to be an \"agentic system\". In other cases, it's possible to remove this scaffolding, creating autonomous agents that can plan, take actions via tool calls, and dir\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "https://langchain-ai.github.io/langgraph/concepts/faq/\n",
      "FAQ¶\n",
      "Common questions and their answers!\n",
      "Do I need to use LangChain to use LangGraph? What’s the difference?¶\n",
      "No. LangGraph is an orchestration framework for complex agentic systems and is more low-level and controllable than LangChain agents. LangChain provides a standard interface to interact with models and other components, useful for straight-forward chains and retrieval flows.\n",
      "How is LangGraph different from other agent frameworks?¶\n",
      "Other agentic frameworks can work for simple, generic tas\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/\n",
      "LangGraph Platform¶\n",
      "Watch this 4-minute overview of LangGraph Platform to see how it helps you build, deploy, and evaluate agentic applications.\n",
      "\n",
      "Overview¶\n",
      "LangGraph Platform is a commercial solution for deploying agentic applications to production, built on the open-source LangGraph framework.\n",
      "The LangGraph Platform consists of several components that work together to support the development, deployment, debugging, and monitoring of LangGraph applications:\n",
      "\n",
      "LangGraph Server: The server defines \n",
      "\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create retriever to get relevant documents (k=3 means return top 3 matches)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Get relevant documents for the query\n",
    "query = \"What is LangGraph?\"\n",
    "relevant_docs = retriever.invoke(query)\n",
    "print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
    "\n",
    "for d in relevant_docs:\n",
    "    print(d.metadata[\"source\"])\n",
    "    print(d.page_content[0:500])\n",
    "    print(\"\\n--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def langgraph_query_tool(query: str):\n",
    "    \"\"\"\n",
    "    Query the LangGraph documentation using a retriever.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to search the documentation with\n",
    "\n",
    "    Returns:\n",
    "        str: A str of the retrieved documents\n",
    "    \"\"\"\n",
    "    retriever = SKLearnVectorStore(\n",
    "        embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "        persist_path=os.getcwd() + \"/saved_vector_stores/sklearn_vectorstore.parquet\",\n",
    "        serializer=\"parquet\",\n",
    "    ).as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    relevant_docs = retriever.invoke(query)\n",
    "    print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
    "    formatted_context = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"==DOCUMENT {i+1}==\\n{doc.page_content}\"\n",
    "            for i, doc in enumerate(relevant_docs)\n",
    "        ]\n",
    "    )\n",
    "    return formatted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As of my last knowledge update in October 2023, LangGraph is not a widely recognized term or concept in mainstream technology, literature, or academia. It could potentially refer to a specific project, tool, or framework that has emerged recently or is niche in nature.\\n\\nIf LangGraph is a new development, it might be related to natural language processing, graph databases, or a combination of language models and graph theory. For the most accurate and up-to-date information, I recommend checking the latest resources, official documentation, or news articles related to LangGraph. If you have more context or details about what LangGraph pertains to, I would be happy to help clarify further!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 136, 'prompt_tokens': 12, 'total_tokens': 148, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BQTCAXSQglzgRQv58lAFgrJkT9nAt', 'finish_reason': 'stop', 'logprobs': None}, id='run-3ed3a04b-c639-408a-8c34-4d6882d73e7a-0', usage_metadata={'input_tokens': 12, 'output_tokens': 136, 'total_tokens': 148, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "llm.invoke(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  langgraph_query_tool (call_drIZYVyXMaIihX5sX7smvuyE)\n",
      " Call ID: call_drIZYVyXMaIihX5sX7smvuyE\n",
      "  Args:\n",
      "    query: What is LangGraph?\n"
     ]
    }
   ],
   "source": [
    "augmented_llm = llm.bind_tools([langgraph_query_tool])\n",
    "instructions = \"\"\"You are a helpful assistant that can answer questions about the LangGraph documentation. \n",
    "Use the langgraph_query_tool for any questions about the documentation.\n",
    "If you don't know the answer, say \"I don't know.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": instructions},\n",
    "    {\"role\": \"user\", \"content\": \"What is LangGraph?\"},\n",
    "]\n",
    "\n",
    "message = augmented_llm.invoke(messages)\n",
    "message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
