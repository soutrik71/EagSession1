{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e255cc8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38d9f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\workspace\\EagSession1\\eag_agentic_rag\\url_rag\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03bbeb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Markdown\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18949ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Clear SSL_CERT_FILE environment variable if set\n",
    "if \"SSL_CERT_FILE\" in os.environ:\n",
    "    del os.environ[\"SSL_CERT_FILE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "190e72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from client.llm_provider import default_llm\n",
    "from client.embedding_provider import OpenAIEmbeddingProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7a8ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = default_llm.chat_model\n",
    "embedder = OpenAIEmbeddingProvider().embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f94ef33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of France is Paris.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d8864f8b6b', 'id': 'chatcmpl-BTuiHUODFWzul7gHbJCtfJl8t2LS7', 'finish_reason': 'stop', 'logprobs': None} id='run-155e42e9-6360-42bc-8353-5cdb6aacfb85-0' usage_metadata={'input_tokens': 14, 'output_tokens': 8, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"what is the capital of France?\"))\n",
    "print(len(embedder.embed_query(\"what is the capital of France?\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7167601",
   "metadata": {},
   "source": [
    "#### Embedding Generation Steps\n",
    "- A function to which I will provide a url and it will scrape the content of the webpage\n",
    "- We will token chunk the content of the webpage\n",
    "- We will generate embeddings for each chunk\n",
    "- We will store the embeddings in a vector database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52350cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# website = \"https://geshan.com.np/blog/2018/11/4-ways-docker-changed-the-way-software-engineers-work-in-past-half-decade\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "647608e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "website = \"https://www.redhat.com/en/topics/devops/what-is-helm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30d7f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "from langchain_community.document_transformers import Html2TextTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32dd81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = AsyncHtmlLoader([website])\n",
    "# docs = loader.load()\n",
    "\n",
    "# html2text = Html2TextTransformer()\n",
    "# docs_transformed = html2text.transform_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49e0e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs_transformed[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72b88391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown(docs_transformed[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "577e631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ac8ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=50,  model_name=\"gpt-4o\",)\n",
    "\n",
    "# texts_chunked = text_splitter.split_documents(docs_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c876f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(texts_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46960fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts_chunked[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4749e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_url(url):\n",
    "    \"\"\"\n",
    "    Extract and return plain text Document(s) from a given URL using LangChain's AsyncHtmlLoader and Html2TextTransformer.\n",
    "    Returns a list of Document objects.\n",
    "    \"\"\"\n",
    "    loader = AsyncHtmlLoader([url])\n",
    "    docs = loader.load()\n",
    "    html2text = Html2TextTransformer()\n",
    "    docs_transformed = html2text.transform_documents(docs)\n",
    "    return docs_transformed\n",
    "\n",
    "\n",
    "def chunk_text_documents(\n",
    "    docs_transformed, chunk_size=500, chunk_overlap=50, model_name=\"gpt-4o\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Split the transformed documents into chunks using TokenTextSplitter.\n",
    "    Returns a list of chunked Document objects.\n",
    "    \"\"\"\n",
    "    text_splitter = TokenTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, model_name=model_name\n",
    "    )\n",
    "    texts_chunked = text_splitter.split_documents(docs_transformed)\n",
    "    print(\n",
    "        f\"Number of chunked documents: {len(texts_chunked)} extracted from {len(docs_transformed)} original documents.\"\n",
    "    )\n",
    "    return texts_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9ec8066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunked documents: 9 extracted from 1 original documents.\n"
     ]
    }
   ],
   "source": [
    "raw_docs = extract_text_from_url(website)\n",
    "chunked_docs = chunk_text_documents(\n",
    "    raw_docs, chunk_size=500, chunk_overlap=50, model_name=\"gpt-4o\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79910ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a261f315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents_to_faiss_index(documents, embedder, index_name):\n",
    "    \"\"\"\n",
    "    Adds documents to a FAISS vector store with the given index name.\n",
    "    If the index does not exist, it creates a new one.\n",
    "    If it exists, it loads and updates it.\n",
    "    Each document is assigned a new UUID as its ID.\n",
    "    \"\"\"\n",
    "    # Ensure the index folder exists or not\n",
    "    index_name = f\"./{index_name}\"\n",
    "    if not os.path.exists(index_name):\n",
    "        print(f\"Index folder '{index_name}' does not exist. Creating a new index.\")\n",
    "        # Create new FAISS index\n",
    "        # Get embedding size from a sample embedding\n",
    "        sample_embedding = embedder.embed_query(\"sample text\")\n",
    "        index = faiss.IndexFlatL2(len(sample_embedding))\n",
    "        vector_store = FAISS(\n",
    "            embedding_function=embedder,\n",
    "            index=index,\n",
    "            docstore=InMemoryDocstore(),\n",
    "            index_to_docstore_id={},\n",
    "        )\n",
    "        print(f\"Creating new FAISS index at '{index_name}'\")\n",
    "    else:\n",
    "        print(f\"Index folder '{index_name}' exists. Loading existing index.\")\n",
    "        # Load existing FAISS index\n",
    "        vector_store = FAISS.load_local(\n",
    "            index_name, embedder, allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(f\"Loaded existing FAISS index from '{index_name}'\")\n",
    "\n",
    "    # Generate unique IDs for each document\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in documents]\n",
    "    # Add documents to the vector store\n",
    "    vector_store.add_documents(documents=documents, ids=doc_ids)\n",
    "    print(f\"Added {len(documents)} documents to the vector store.\")\n",
    "\n",
    "    # Save the updated index\n",
    "    vector_store.save_local(folder_path=index_name)\n",
    "    print(f\"Index saved at '{index_name}'\")\n",
    "\n",
    "    return doc_ids  # Optionally return the IDs for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2528102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index folder './test_index' exists. Loading existing index.\n",
      "Loaded existing FAISS index from './test_index'\n",
      "Added 9 documents to the vector store.\n",
      "Index saved at './test_index'\n"
     ]
    }
   ],
   "source": [
    "index_name = \"test_index\"\n",
    "ids = add_documents_to_faiss_index(chunked_docs, embedder, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05c7f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pickle file\n",
    "path = \"./test_index/index.pkl\"\n",
    "import pickle\n",
    "\n",
    "with open(path, \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f18d0e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(index_name, embedder):\n",
    "    \"\"\"\n",
    "    Load a FAISS vector store from the specified index name.\n",
    "    \"\"\"\n",
    "    path = f\"./{index_name}\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Index folder '{index_name}' does not exist. Creating a new index.\")\n",
    "        raise FileNotFoundError(f\"Index folder '{index_name}' does not exist.\")\n",
    "    # Load existing FAISS index\n",
    "    vector_store = FAISS.load_local(\n",
    "        path, embedder, allow_dangerous_deserialization=True\n",
    "    )\n",
    "    print(f\"Loaded existing FAISS index from '{index_name}'\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c0a7601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing FAISS index from 'test_index'\n"
     ]
    }
   ],
   "source": [
    "vector_store = get_vector_store(index_name, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "167c964a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='dcf74748-0e94-4bf5-b563-35a5bf9fa5b0', metadata={'source': 'https://www.redhat.com/en/topics/devops/what-is-helm', 'title': 'What is Helm?', 'description': 'Helm is a package manager for Kubernetes that includes all the necessary code and resources needed to deploy an application to a cluster.', 'language': 'en'}, page_content='日本語한국어PortuguêsEspañol\\n\\nContact us\\n\\nEnglish\\n\\n### Select a language\\n\\n  * 简体中文\\n  * English\\n  * Français\\n  * Deutsch\\n  * Italiano\\n  * 日本語\\n  * 한국어\\n  * Português\\n  * Español\\n\\nSelect a language简体中文EnglishFrançaisDeutschItaliano日本語한국어PortuguêsEspañol\\n\\nRed Hat\\n\\n  * AI\\n  * Hybrid cloud\\n  * Products\\n  * Training & services\\n  * Resources\\n  * Partners\\n  * About\\n\\nMenu Search  For you  Contact us  English  Log In\\n\\n  * AI\\n  * Hybrid cloud\\n  * Products\\n  * Training & services\\n  * Resources\\n  * Partners\\n  * About\\n  * Contact us\\n\\n### Select a language\\n\\n  * 简体中文\\n  * English\\n  * Français\\n  * Deutsch\\n  * Italiano\\n  * 日本語\\n  * 한국어\\n  * Português\\n  * Español\\n\\n  * Topics \\n  * Open source \\n  * What is Helm? \\n\\n# What is Helm?\\n\\nPublished May 9, 2024• _3_ -minute read\\n\\nCopy URL\\n\\n## Overview\\n\\nApplication development in Kubernetes is inherently complex. For any given\\napplication, you might be installing, managing, and updating hundreds of\\nconfigurations.\\n\\nHelm simplifies this process by automating the distribution of your\\napplications using a packaging format called a **Helm chart**. Much like how\\n`yum` manages RPM packages for Red Hat® Enterprise Linux®, `helm` manages Helm\\ncharts for Kubernetes. Charts maintain consistency across containers while\\nalso determining how specific requirements for an application are met.\\n\\nAs the package manager for Kubernetes, Helm enables you to apply the same\\nconfiguration framework to multiple instances using variable overrides, all\\nbased on what matters most to your specific configuration.\\n\\n_Fun fact_ : Helm is an open-source project that graduated out of the Cloud\\nNative Computing Foundation (CNCF). It was first introduced in 2015 at the\\ninaugural KubeCon and migrated to the CNCF with Kubernetes.\\n\\n## How does Helm work?\\n\\nHelm works by describing the application from definition to upgrade in what is\\nknown as a **Helm chart**. Helm uses charts (similar to a template) to pass\\nresources to your Kubernetes cluster using the Kubernetes API.\\n\\nHelm uses a single command'),\n",
       " Document(id='bb79781e-f97b-46a6-9b28-552a3d31cef6', metadata={'source': 'https://www.redhat.com/en/topics/devops/what-is-helm', 'title': 'What is Helm?', 'description': 'Helm is a package manager for Kubernetes that includes all the necessary code and resources needed to deploy an application to a cluster.', 'language': 'en'}, page_content=' works by describing the application from definition to upgrade in what is\\nknown as a **Helm chart**. Helm uses charts (similar to a template) to pass\\nresources to your Kubernetes cluster using the Kubernetes API.\\n\\nHelm uses a single command line interface (CLI) tool called `helm` to manage\\nthe Helm chart, along with a handful of simple commands that allow you to\\ncreate, manage, and configure your application.\\n\\nLearn more about Helm and Red Hat OpenShift\\n\\n## Red Hat resources\\n\\nKeep reading\\n\\n## What is a Helm chart?\\n\\nHelm charts are a collection of files that describe a Kubernetes cluster’s\\nresources and package them together as an application. They comprise three\\nbasic components:\\n\\n  * **The chart** \\\\- `Chart.yaml` defines the application metadata like name, version, dependencies, etc. \\n  * **Values** \\\\- `values.yaml` sets values, which is how you will set variable substitutions for reusing your chart\\n    * You may also have a values JSON schema that describes a structure for the values file, which can help in creating dynamic forms and validating your values parameters.\\n  * **The templates directory** \\\\- `templates/` houses your templates and combines them with the values set in your values.yaml file to create manifests\\n  * **The charts directory** \\\\- `charts/` stores any chart dependencies you define in `Chart.yaml` and reconstruct with `helm dependency build` or `helm dependency update`.\\n\\nEach time you install a Helm chart, you also create an instance of it, called\\na **release**. Helm charts are maintained with each new release, and you can\\neasily use previous versions of the chart to roll back to your preferred\\nconfiguration.\\n\\nTo see Helm charts in action, check out this episode of Ask a Product Manager\\nOffice Hours that discusses using Helm with Red Hat OpenShift®.\\n\\n## How to use Helm charts\\n\\nAfter installing the Helm CLI, you have two options: use an existing chart\\nwith predefined resources and values or create a customized chart to package\\nyour own application resources.\\n\\n### Using a pre-existing chart\\n\\nUsing a pre-existing chart involves first adding the Helm repository to your\\nHelm client, then setting specific configuration parameters in your values\\nfiles at the time of install.\\n\\nYour values files are the keys to reusing Helm charts for individual\\nconfigurations. You can substitute any variable declared in `values.yaml`, and\\nHelm will create the'),\n",
       " Document(id='104127c2-9a0e-4864-b779-195466de9625', metadata={'source': 'https://www.redhat.com/en/topics/devops/what-is-helm', 'title': 'What is Helm?', 'description': 'Helm is a package manager for Kubernetes that includes all the necessary code and resources needed to deploy an application to a cluster.', 'language': 'en'}, page_content=' setting specific configuration parameters in your values\\nfiles at the time of install.\\n\\nYour values files are the keys to reusing Helm charts for individual\\nconfigurations. You can substitute any variable declared in `values.yaml`, and\\nHelm will create the `.values` structure to hold these variables in the\\ntemplate. This makes the variables available to substitute in later\\nconfigurations. You can pass as many values files to a chart as you want; Helm\\nwill combine them and render them together so it’s possible to reuse variable\\nfiles.\\n\\nOnce you have committed or pushed your values configurations, you will be able\\nto update, upgrade, and manage your application’s lifecycle through common\\n`helm` commands.\\n\\n### Creating a customized chart\\n\\nYou may need to create a custom chart to package applications you want to\\nreuse across your organization or within your specific workloads. This\\ninvolves defining your application’s resources in the chart’s `templates/`\\ndirectory, setting configuration parameters through values files, and adding\\nany metadata and documentation to the `Chart.yaml` file.\\n\\nYou can then package the chart using `helm package` and upload it to a public\\nor private Helm repository, or distribute it directly.\\n\\nRed Hat’s **validated patterns** are useful examples of existing Helm charts\\nthat you can also customize. Validated patterns are Helm charts that describe\\na full workload that has been deployed at a customer site and met a set of\\nrequirements for testing and maintenance. They can be used directly or\\nmodified to fit your own configuration’s needs.\\n\\nLearn more about validated patterns\\n\\n## Why use Helm?\\n\\nAs part of your overall GitOps strategy, Helm is a powerful tool that provides\\nagility, security, and consistency across multiple environments. When paired\\nwith a multi-cloud communication tool like Red Hat Service Interconnect, Helm\\nis especially useful for complex deployments that are using multiple\\nmicroservices across multiple cloud providers.\\n\\nFor Kubernetes developers, Helm charts are a simple, quick, and easy way to\\nspin up applications that can then be reused or shared with others.  \\n\\nFor system administrators and other IT operations professionals, Helm provides\\na consistent tool for implementing and streamlining continuous integration and\\ncontinuous development (CI/CD) into their application pipelines. It’s a tool\\nthat provides agility alongside consistency.\\n\\nTo try out the full process, from creating a chart to configuring values and\\ndeploying the chart, check out the following tutorials from Red Hat®\\nDeveloper.\\n\\nTry out')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\n",
    "    query=\"What is Helm?\",\n",
    "    k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91ec28dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5b28024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaissConversationStore:\n",
    "    \"\"\"\n",
    "    Store and retrieve conversations in a FAISS vector store using conversation IDs.\n",
    "    Each message is a Document with metadata: conversation_id, sender ('human' or 'ai'), and order.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedder, index_folder=\"history_index\"):\n",
    "        self.embedder = embedder\n",
    "        self.index_folder = index_folder\n",
    "        if not os.path.exists(index_folder):\n",
    "            sample_embedding = embedder.embed_query(\"sample text\")\n",
    "            index = faiss.IndexFlatL2(len(sample_embedding))\n",
    "            self.vector_store = FAISS(\n",
    "                embedding_function=embedder,\n",
    "                index=index,\n",
    "                docstore=InMemoryDocstore(),\n",
    "                index_to_docstore_id={},\n",
    "            )\n",
    "            self.vector_store.save_local(folder_path=index_folder)\n",
    "        else:\n",
    "            self.vector_store = FAISS.load_local(\n",
    "                index_folder, embedder, allow_dangerous_deserialization=True\n",
    "            )\n",
    "\n",
    "    def store_conversation(self, conversation_id: str, messages: list[dict]):\n",
    "        \"\"\"\n",
    "        Store a conversation as a list of messages.\n",
    "        Each message is a dict: {'sender': 'human'/'ai', 'content': str}\n",
    "        \"\"\"\n",
    "        docs = [\n",
    "            Document(\n",
    "                page_content=msg[\"content\"],\n",
    "                metadata={\n",
    "                    \"conversation_id\": conversation_id,\n",
    "                    \"sender\": msg[\"sender\"],\n",
    "                    \"order\": i,\n",
    "                },\n",
    "            )\n",
    "            for i, msg in enumerate(messages)\n",
    "        ]\n",
    "        ids = [f\"{conversation_id}_{i}\" for i in range(len(messages))]\n",
    "        self.vector_store.add_documents(docs, ids=ids)\n",
    "        self.vector_store.save_local(folder_path=self.index_folder)\n",
    "\n",
    "    def get_conversation(self, conversation_id: str) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Retrieve the conversation as a list of dicts with sender and content, ordered.\n",
    "        \"\"\"\n",
    "        # Get all docstore keys for this conversation\n",
    "        ids = [\n",
    "            k\n",
    "            for k in self.vector_store.docstore._dict.keys()\n",
    "            if k.startswith(conversation_id + \"_\")\n",
    "        ]\n",
    "        docs = self.vector_store.get_by_ids(ids)\n",
    "        # Sort by order\n",
    "        docs = sorted(\n",
    "            [doc for doc in docs if doc is not None],\n",
    "            key=lambda d: d.metadata.get(\"order\", 0),\n",
    "        )\n",
    "        return [\n",
    "            {\"sender\": doc.metadata[\"sender\"], \"content\": doc.page_content}\n",
    "            for doc in docs\n",
    "        ]\n",
    "\n",
    "    def count(self) -> int:\n",
    "        \"\"\"Return the number of messages stored.\"\"\"\n",
    "        return len(self.vector_store.docstore._dict)\n",
    "\n",
    "    def list_conversation_ids(self) -> list[str]:\n",
    "        \"\"\"List all unique conversation IDs stored.\"\"\"\n",
    "        ids = self.vector_store.docstore._dict.keys()\n",
    "        return list(set(k.split(\"_\")[0] for k in ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8b92ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sender': 'human', 'content': 'Hello!'}, {'sender': 'ai', 'content': 'Hi, how can I help you?'}, {'sender': 'human', 'content': 'Tell me a joke.'}, {'sender': 'ai', 'content': 'Why did the chicken cross the road?'}]\n"
     ]
    }
   ],
   "source": [
    "conv_id = uuid.uuid4()\n",
    "store = FaissConversationStore(embedder)\n",
    "messages = [\n",
    "    {\"sender\": \"human\", \"content\": \"Hello!\"},\n",
    "    {\"sender\": \"ai\", \"content\": \"Hi, how can I help you?\"},\n",
    "    {\"sender\": \"human\", \"content\": \"Tell me a joke.\"},\n",
    "    {\"sender\": \"ai\", \"content\": \"Why did the chicken cross the road?\"},\n",
    "]\n",
    "store.store_conversation(str(conv_id), messages)\n",
    "print(store.get_conversation(str(conv_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "282d68f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_index_name': 'test_index', 'history_index_name': 'history_index'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read a yaml file\n",
    "import yaml\n",
    "\n",
    "def read_yaml_file(file_path):\n",
    "    \"\"\"\n",
    "    Read a YAML file and return its contents.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data\n",
    "yaml_file_path = \"server/config.yaml\"\n",
    "yaml_data = read_yaml_file(yaml_file_path)\n",
    "yaml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9c2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
