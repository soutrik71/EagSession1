{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\workspace\\EagSession1\\eag_agentic_rag\\url_rag\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Markdown\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Clear SSL_CERT_FILE environment variable if set\n",
    "if \"SSL_CERT_FILE\" in os.environ:\n",
    "    del os.environ[\"SSL_CERT_FILE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urllib3 SSL patched successfully\n",
      "httpx Client patched to disable SSL verification\n",
      "httpx AsyncHTTPTransport patched to disable SSL verification\n",
      "✅ SSL verification completely disabled at all levels\n",
      "SSL_CERT_FILE: None\n",
      "PYTHONHTTPSVERIFY: 0\n",
      "OPENAI_VERIFY_SSL_CERTS: false\n",
      "OPENAI_API_SKIP_VERIFY_SSL: true\n",
      "✅ SSL test success: 401\n"
     ]
    }
   ],
   "source": [
    "from utility.llm_provider import default_llm\n",
    "from utility.embedding_provider import OpenAIEmbeddingProvider\n",
    "from utility.utils import read_yaml_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = default_llm.chat_model\n",
    "embedder = OpenAIEmbeddingProvider().embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of France is Paris.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f5bdcc3276', 'id': 'chatcmpl-BUHkVM02Ghurwph9B4PevIbc26eMI', 'finish_reason': 'stop', 'logprobs': None} id='run-ae0382ca-0281-4fe6-b295-748fc98de293-0' usage_metadata={'input_tokens': 14, 'output_tokens': 8, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"what is the capital of France?\"))\n",
    "print(len(embedder.embed_query(\"what is the capital of France?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read config\n",
    "config = read_yaml_file(\"utility/config.yaml\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_index_name = config[\"history_index_name\"]\n",
    "history_index_name = os.path.join(os.getcwd(), history_index_name)\n",
    "print(history_index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from client.memory import FaissConversationStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_id = uuid.uuid4()\n",
    "# store = FaissConversationStore(\n",
    "#     embedder, index_folder=history_index_name, reset_index=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\"sender\": \"human\", \"content\": \"Hello!\"},\n",
    "#     {\"sender\": \"ai\", \"content\": \"Hi, how can I help you?\"},\n",
    "#     {\"sender\": \"human\", \"content\": \"Tell me a joke.\"},\n",
    "#     {\"sender\": \"ai\", \"content\": \"Why did the chicken cross the road?\"},\n",
    "# ]\n",
    "# store.store_conversation(str(conv_id), messages)\n",
    "# print(store.get_conversation(str(conv_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # new conversation with same id\n",
    "# messages = [\n",
    "#     {\"sender\": \"human\", \"content\": \"Tell me a joke about a space alien\"},\n",
    "#     {\"sender\": \"ai\", \"content\": \"Why did the space alien cross the road?\"},\n",
    "#     {\"sender\": \"human\", \"content\": \"What is the capital of France?\"},\n",
    "#     {\"sender\": \"ai\", \"content\": \"The capital of France is Paris.\"},\n",
    "# ]\n",
    "# store.store_conversation(str(conv_id), messages)\n",
    "# print(store.get_conversation(str(conv_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next steps \n",
    "- Perception code which will be used prepare the question based on the user query and chat history\n",
    "- Decision making code which will be used to decide the action based on the question and chat history\n",
    "- Action code which will be used to execute the action\n",
    "- Memory code which will be used to store the chat history\n",
    "- Client code which will be used to coordinate the conversation between the perception, decision making, action and memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perception code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebContentSearch(BaseModel):\n",
    "    enhanced_user_query: str = Field(\n",
    "        description=\"The enhanced user query to be searched on the web for similar content stored in the memory\"\n",
    "    )\n",
    "    no_of_results: int = Field(\n",
    "        1,\n",
    "        description=\"The number of results to be returned from the web search corresponding to the user query\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an intelligent assistant designed to refine user queries using current input and prior context. Your goal is to construct a precise, context-aware enhanced query that reflects the user’s true intent.\n",
    "\n",
    "Your responsibilities are:\n",
    "1. Analyze the current user message to understand the core request.\n",
    "2. Examine the chat history (a list of messages with sender and content) to identify:\n",
    "   - Any contextually relevant information.\n",
    "   - Follow-ups or references to previous discussions.\n",
    "3. Determine whether the current query is:\n",
    "   a. A continuation of a previous topic.\n",
    "   b. A new, unrelated query.\n",
    "\n",
    "Based on this analysis, follow the appropriate path:\n",
    "\n",
    "**If the current message relates to previous topics:**\n",
    "- Incorporate relevant prior context into the enhanced query.\n",
    "- Clarify ambiguous references or pronouns using information from the chat history.\n",
    "- Resolve under-specified requests by grounding them in previous conversations.\n",
    "\n",
    "**If the message is a new topic:**\n",
    "- Focus solely on the current message.\n",
    "- Do not infer or inject unrelated context from chat history.\n",
    "\n",
    "**Process for forming the enhanced user query:**\n",
    "1. Carefully review the current message.\n",
    "2. Analyze the chat history (most recent last) to extract relevant past content.\n",
    "3. Identify if the user is building on a previous conversation.\n",
    "4. Integrate any necessary clarifications, references, or details from the chat history.\n",
    "5. Output a concise, context-enriched, unambiguous enhanced query.\n",
    "\n",
    "**Inputs:**\n",
    "- `chat_history`: A list of dictionaries, each with keys `sender` and `content`.\n",
    "    {chat_history}\n",
    "- `user_query`: The latest message from the user.\n",
    "    {user_query}\n",
    "\n",
    "**Output Format:**\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perception_chain(llm) -> RunnableSequence:\n",
    "    \"\"\"\n",
    "    Creates and returns the perception chain for extracting travel search parameters.\n",
    "\n",
    "    Args:\n",
    "        default_llm: The default LLM to use for the chain.\n",
    "\n",
    "    Returns:\n",
    "        A LangChain runnable sequence that takes a user query and chat history,\n",
    "        and returns a TravelSearch object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up a parser\n",
    "    parser = PydanticOutputParser(pydantic_object=WebContentSearch)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"human\", \"The user query is: {user_query}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "    # Create and return the chain\n",
    "    return prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_id = uuid.uuid4()\n",
    "store = FaissConversationStore(\n",
    "    embedder, index_folder=history_index_name, reset_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = store.get_conversation_as_lc_messages(str(conv_id))\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is role of helm in kubernetes?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "perception_chain = get_perception_chain(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = perception_chain.invoke(\n",
    "    {\"user_query\": user_query, \"chat_history\": chat_history}\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the history object\n",
    "messages = [\n",
    "    {\"sender\": \"human\", \"content\": user_query},\n",
    "    {\"sender\": \"ai\", \"content\": result.enhanced_user_query},\n",
    "]\n",
    "store.store_conversation(str(conv_id), messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Give me two articles on for the same topic?\"\n",
    "chat_history = store.get_conversation_as_lc_messages(str(conv_id))\n",
    "print(chat_history)\n",
    "\n",
    "result = perception_chain.invoke(\n",
    "    {\"user_query\": user_query, \"chat_history\": chat_history}\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the history object\n",
    "messages = [\n",
    "    {\"sender\": \"human\", \"content\": user_query},\n",
    "    {\"sender\": \"ai\", \"content\": result.enhanced_user_query},\n",
    "]\n",
    "store.store_conversation(str(conv_id), messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = store.get_conversation_as_lc_messages(str(conv_id))\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function where we will pass the history object and a list of conversation and the chain\n",
    "# and it will keep processing the conversation and return the enhanced user query\n",
    "\n",
    "\n",
    "def process_conversation_and_enhance_query(\n",
    "    history_store, conversation_list, chain, conv_id\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a list of user queries (conversation_list) using the provided chain and history store.\n",
    "    For each user query, it retrieves the chat history, invokes the chain, stores the conversation,\n",
    "    and returns a list of enhanced user queries.\n",
    "\n",
    "    Args:\n",
    "        history_store: The object responsible for storing and retrieving conversation history.\n",
    "        conversation_list: List of user queries (strings) to process.\n",
    "        chain: The chain object with an 'invoke' method.\n",
    "        conv_id: The conversation id to store the conversation.\n",
    "\n",
    "    Returns:\n",
    "        List of enhanced user queries (one for each user query in conversation_list).\n",
    "    \"\"\"\n",
    "    enhanced_queries = []\n",
    "    for user_query in conversation_list:\n",
    "        # Retrieve chat history for the conversation\n",
    "        chat_history = history_store.get_conversation_as_lc_messages(str(conv_id))\n",
    "        # Invoke the chain to get the result\n",
    "        result = chain.invoke({\"user_query\": user_query, \"chat_history\": chat_history})\n",
    "        # Store the conversation\n",
    "        messages = [\n",
    "            {\"sender\": \"human\", \"content\": user_query},\n",
    "            {\"sender\": \"ai\", \"content\": result.enhanced_user_query},\n",
    "        ]\n",
    "        history_store.store_conversation(str(conv_id), messages)\n",
    "        # Collect the enhanced user query\n",
    "        enhanced_queries.append(result.enhanced_user_query)\n",
    "    return enhanced_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_id = uuid.uuid4()\n",
    "memory_store = FaissConversationStore(\n",
    "    embedder, index_folder=history_index_name, reset_index=True\n",
    ")\n",
    "conversation_list = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Tell me more about its history.\",\n",
    "    \"List two famous landmarks in that city.\",\n",
    "]\n",
    "enhanced_queries = process_conversation_and_enhance_query(\n",
    "    memory_store, conversation_list, perception_chain, conv_id\n",
    ")\n",
    "print(enhanced_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision and action code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the constraints of mcp we cannot use the decision node in jupter notebook , so we have executed the decision node in the test_client.py file and have saved the output in the test_client_output.pkl file\n",
    "- We will try to debug the action node here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "decison_op = \"client/test_client_output.json\"\n",
    "import json\n",
    "with open(decison_op, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': {'content': '',\n",
       "  'tool_calls': [{'name': 'web_vector_search',\n",
       "    'id': 'call_FWHEcSZih0LGVsBIvpGLRC5w',\n",
       "    'type': 'tool_call',\n",
       "    'args': {'request': {'query': 'What is the purpose and benefits of using Helm in software development or Kubernetes management?',\n",
       "      'k': 1}}}],\n",
       "  'model': 'gpt-4o-2024-08-06',\n",
       "  'finish_reason': 'tool_calls',\n",
       "  'usage': {'total_tokens': 247, 'input_tokens': 210, 'output_tokens': 37}},\n",
       " 'messages': [{'type': 'HumanMessage',\n",
       "   'content': 'What is the object of using helm?',\n",
       "   'tool_calls': []},\n",
       "  {'type': 'AIMessage',\n",
       "   'content': '',\n",
       "   'tool_calls': [{'name': 'web_vector_search',\n",
       "     'id': 'call_FWHEcSZih0LGVsBIvpGLRC5w',\n",
       "     'type': 'tool_call',\n",
       "     'args': {'request': {'query': 'What is the purpose and benefits of using Helm in software development or Kubernetes management?',\n",
       "       'k': 1}}}]}],\n",
       " 'tools': [{'name': 'web_vector_search',\n",
       "   'description': '\\n    Perform advanced web search using vector database and return the results.\\n\\n    Args:\\n        request: The search parameters with query and k (number of results) in the format of WebsearchRequest pydantic model\\n    Returns:\\n        WebsearchResponse with web_url and page_content lists in the format of WebsearchResponse pydantic model\\n    ',\n",
       "   'args_schema': {'$defs': {'WebsearchRequest': {'description': 'Request model for web search.',\n",
       "      'properties': {'query': {'description': 'The search query',\n",
       "        'title': 'Query',\n",
       "        'type': 'string'},\n",
       "       'k': {'default': 1,\n",
       "        'description': 'The number of results to return',\n",
       "        'title': 'K',\n",
       "        'type': 'integer'}},\n",
       "      'required': ['query'],\n",
       "      'title': 'WebsearchRequest',\n",
       "      'type': 'object'}},\n",
       "    'properties': {'request': {'$ref': '#/$defs/WebsearchRequest'}},\n",
       "    'required': ['request'],\n",
       "    'title': 'web_vector_searchArguments',\n",
       "    'type': 'object'},\n",
       "   'response_format': 'content_and_artifact'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pickle file\n",
    "import pickle\n",
    "with open(\"client/test_client_output.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = data[\"response\"]\n",
    "msg_objs = data[\"msg_objs\"]\n",
    "tools = data[\"tools\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_FWHEcSZih0LGVsBIvpGLRC5w', 'function': {'arguments': '{\"request\":{\"query\":\"What is the purpose and benefits of using Helm in software development or Kubernetes management?\",\"k\":1}}', 'name': 'web_vector_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 210, 'total_tokens': 247, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f5bdcc3276', 'id': 'chatcmpl-BUI6CtcIxvtTuQ6RcYcaaa61VViIc', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-dfc381cd-69fe-42ce-8787-50be4eb0d4f6-0', tool_calls=[{'name': 'web_vector_search', 'args': {'request': {'query': 'What is the purpose and benefits of using Helm in software development or Kubernetes management?', 'k': 1}}, 'id': 'call_FWHEcSZih0LGVsBIvpGLRC5w', 'type': 'tool_call'}], usage_metadata={'input_tokens': 210, 'output_tokens': 37, 'total_tokens': 247, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.tool_example import web_vector_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [web_vector_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_dict = {tool.name: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'web_vector_search': StructuredTool(name='web_vector_search', description='Search the knowledge base for information on any topic.\\n\\n    This tool must be used for ANY request requiring factual information.\\n\\n    Args:\\n        request: The WebSearchRequest containing query and k parameters', args_schema=<class 'langchain_core.utils.pydantic.web_vector_search'>, func=<function web_vector_search at 0x00000219FB2A47C0>)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WebSearchResponse(results=['Found 1 results for: What is the capital of France? (schema validated). The knowledge base indicates What is the capital of France? is a package manager for Kubernetes that simplifies application deployment and management.'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_tool = tool_dict[\"web_vector_search\"]\n",
    "await selected_tool.ainvoke(\n",
    "    {\"request\": {\"query\": \"What is the capital of France?\", \"k\": 1}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool_dict = {tool.name: tool for tool in tools}\n",
    "# print(tool_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if hasattr(response, \"tool_calls\") and response.tool_calls:\n",
    "#     # Handle tool calls\n",
    "#     print(f\"Processing {len(response.tool_calls)} tool call(s)...\")\n",
    "#     print(response.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tool_call in response.tool_calls:\n",
    "#     print(tool_call)\n",
    "#     if tool_call[\"name\"] in tool_dict:\n",
    "#         selected_tool = tool_dict[tool_call[\"name\"]]\n",
    "#         print(f\"Executing tool: {tool_call['name']}\")\n",
    "\n",
    "#         # Process the arguments - make a copy to avoid modifying the original\n",
    "#         tool_args = dict(tool_call[\"args\"])\n",
    "\n",
    "#         # Ensure the arguments match the expected schema\n",
    "#         print(f\"Original tool args: {tool_args}\")\n",
    "\n",
    "#         # Get the schema from the tool if available\n",
    "#         tool_schema = None\n",
    "#         if hasattr(selected_tool, \"args_schema\"):\n",
    "#             tool_schema = selected_tool.args_schema\n",
    "#             schema_name = (\n",
    "#                 tool_schema.__name__\n",
    "#                 if hasattr(tool_schema, \"__name__\")\n",
    "#                 else type(tool_schema).__name__\n",
    "#             )\n",
    "#             print(\n",
    "#                 f\"Tool schema found: {schema_name} and the type is {type(tool_schema)}\"\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.messages import ToolMessage\n",
    "\n",
    "# messages = []\n",
    "# tool_output = selected_tool.invoke(tool_args)\n",
    "\n",
    "# messages.append(ToolMessage(content=tool_output, tool_call_id=tool_call[\"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "async def process_tool_calls(\n",
    "    tool_calls: List[Dict], tools: List[Any]\n",
    ") -> List[ToolMessage]:\n",
    "    \"\"\"\n",
    "    Process tool calls by invoking the appropriate tool with arguments.\n",
    "    Handles schema validation and nested argument structures.\n",
    "    If invocation with nested dict fails, fallback to simple dict.\n",
    "\n",
    "    Args:\n",
    "        tool_calls: List of tool calls from the LLM response.\n",
    "        tools: List of available tools.\n",
    "\n",
    "    Returns:\n",
    "        List of ToolMessage objects with the results.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    tool_dict = {tool.name: tool for tool in tools}\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call.get(\"name\")\n",
    "        tool_id = tool_call.get(\"id\")\n",
    "        tool_args = tool_call.get(\"args\", {})\n",
    "        print(f\"\\nProcessing tool call: {tool_name} (id: {tool_id})\")\n",
    "        if tool_name not in tool_dict:\n",
    "            print(f\"Tool {tool_name} not found in available tools\")\n",
    "            messages.append(\n",
    "                ToolMessage(content=f\"Tool {tool_name} not found.\", tool_call_id=tool_id)\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        selected_tool = tool_dict[tool_name]\n",
    "        print(f\"Executing tool: {tool_name}\")\n",
    "\n",
    "        # Print original tool args\n",
    "        print(f\"Original tool args: {tool_args}\")\n",
    "\n",
    "        # Handle schema if present\n",
    "        tool_schema = getattr(selected_tool, \"args_schema\", None)\n",
    "        if tool_schema is not None:\n",
    "            schema_name = getattr(tool_schema, \"__name__\", type(tool_schema).__name__)\n",
    "            print(f\"Tool schema found: {schema_name} ({type(tool_schema)})\")\n",
    "\n",
    "        # Try nested dict first, fallback to simple dict if it fails\n",
    "        tried_fallback = False\n",
    "        try:\n",
    "            print(f\"Calling tool with args: {tool_args}\")\n",
    "            if hasattr(selected_tool, \"ainvoke\"):\n",
    "                tool_output = await selected_tool.ainvoke(tool_args)\n",
    "            else:\n",
    "                tool_output = selected_tool.invoke(tool_args)\n",
    "            messages.append(\n",
    "                ToolMessage(content=tool_output, tool_call_id=tool_id)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Tool invocation failed with nested dict: {e}\")\n",
    "            # Fallback: If tool_args is a dict with a single key (e.g. 'request'), try flattening\n",
    "            fallback_args = None\n",
    "            if isinstance(tool_args, dict) and len(tool_args) == 1:\n",
    "                only_key = next(iter(tool_args))\n",
    "                if isinstance(tool_args[only_key], dict):\n",
    "                    fallback_args = tool_args[only_key]\n",
    "            # If not, try passing tool_args as is (in case it's already flat)\n",
    "            if fallback_args is None and isinstance(tool_args, dict):\n",
    "                fallback_args = tool_args\n",
    "            if fallback_args is not None and not tried_fallback:\n",
    "                try:\n",
    "                    print(f\"Retrying tool invocation with fallback args: {fallback_args}\")\n",
    "                    if hasattr(selected_tool, \"ainvoke\"):\n",
    "                        tool_output = await selected_tool.ainvoke(fallback_args)\n",
    "                    else:\n",
    "                        tool_output = selected_tool.invoke(fallback_args)\n",
    "                    messages.append(\n",
    "                        ToolMessage(content=tool_output, tool_call_id=tool_id)\n",
    "                    )\n",
    "                    continue\n",
    "                except Exception as e2:\n",
    "                    error_msg = f\"Error executing tool {tool_name} (fallback): {e2}\"\n",
    "                    print(error_msg)\n",
    "                    messages.append(\n",
    "                        ToolMessage(content=error_msg, tool_call_id=tool_id)\n",
    "                    )\n",
    "            else:\n",
    "                error_msg = f\"Error executing tool {tool_name}: {e}\"\n",
    "                print(error_msg)\n",
    "                messages.append(\n",
    "                    ToolMessage(content=error_msg, tool_call_id=tool_id)\n",
    "                )\n",
    "\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing tool call: web_vector_search (id: call_FWHEcSZih0LGVsBIvpGLRC5w)\n",
      "Executing tool: web_vector_search\n",
      "Original tool args: {'request': {'query': 'What is the purpose and benefits of using Helm in software development or Kubernetes management?', 'k': 1}}\n",
      "Tool schema found: web_vector_search (<class 'pydantic._internal._model_construction.ModelMetaclass'>)\n",
      "Calling tool with args: {'request': {'query': 'What is the purpose and benefits of using Helm in software development or Kubernetes management?', 'k': 1}}\n"
     ]
    }
   ],
   "source": [
    "messages = await process_tool_calls(response.tool_calls, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ToolMessage(content=\"results=['Found 1 results for: What is the purpose and benefits of using Helm in software development or Kubernetes management? (schema validated). The knowledge base indicates What is the purpose and benefits of using Helm in software development or Kubernetes management? is a package manager for Kubernetes that simplifies application deployment and management.']\", tool_call_id='call_FWHEcSZih0LGVsBIvpGLRC5w')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################### END OF FILE #########################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
