{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\workspace\\EagSession1\\eag_agentic_rag\\url_rag\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Markdown\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Clear SSL_CERT_FILE environment variable if set\n",
    "if \"SSL_CERT_FILE\" in os.environ:\n",
    "    del os.environ[\"SSL_CERT_FILE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.llm_provider import default_llm\n",
    "from utility.embedding_provider import OpenAIEmbeddingProvider\n",
    "from utility.utils import read_yaml_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = default_llm.chat_model\n",
    "embedder = OpenAIEmbeddingProvider().embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of France is Paris.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_90122d973c', 'id': 'chatcmpl-BU9rxe7qNlzRKsFlSXdww3AuRo4Ja', 'finish_reason': 'stop', 'logprobs': None} id='run-2c58d0b5-c640-44ae-8ed8-149a6f02b3ea-0' usage_metadata={'input_tokens': 14, 'output_tokens': 8, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"what is the capital of France?\"))\n",
    "print(len(embedder.embed_query(\"what is the capital of France?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'db_index_name': 'weburl_index', 'history_index_name': 'history_index', 'chunk_size': 500, 'chunk_overlap': 50, 'model_name': 'gpt-4o', 'reset_index': True}\n"
     ]
    }
   ],
   "source": [
    "# read config\n",
    "config = read_yaml_file(\"utility/config.yaml\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\workspace\\EagSession1\\eag_agentic_rag\\url_rag\\history_index\n"
     ]
    }
   ],
   "source": [
    "history_index_name = config[\"history_index_name\"]\n",
    "history_index_name = os.path.join(os.getcwd(), history_index_name)\n",
    "print(history_index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from client.memory import FaissConversationStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-06 16:01:18.673\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mutility.utils\u001b[0m:\u001b[36mcheck_and_reset_index\u001b[0m:\u001b[36m25\u001b[0m - \u001b[33m\u001b[1mreset_index is set to True. Deleting existing index at 'c:\\workspace\\EagSession1\\eag_agentic_rag\\url_rag\\history_index'\u001b[0m\n",
      "\u001b[32m2025-05-06 16:01:18.676\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mutility.utils\u001b[0m:\u001b[36mcheck_and_reset_index\u001b[0m:\u001b[36m30\u001b[0m - \u001b[32m\u001b[1mSuccessfully deleted index folder 'c:\\workspace\\EagSession1\\eag_agentic_rag\\url_rag\\history_index'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# conv_id = uuid.uuid4()\n",
    "# store = FaissConversationStore(\n",
    "#     embedder, index_folder=history_index_name, reset_index=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\"sender\": \"human\", \"content\": \"Hello!\"},\n",
    "#     {\"sender\": \"ai\", \"content\": \"Hi, how can I help you?\"},\n",
    "#     {\"sender\": \"human\", \"content\": \"Tell me a joke.\"},\n",
    "#     {\"sender\": \"ai\", \"content\": \"Why did the chicken cross the road?\"},\n",
    "# ]\n",
    "# store.store_conversation(str(conv_id), messages)\n",
    "# print(store.get_conversation(str(conv_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # new conversation with same id\n",
    "# messages = [\n",
    "#     {\"sender\": \"human\", \"content\": \"Tell me a joke about a space alien\"},\n",
    "#     {\"sender\": \"ai\", \"content\": \"Why did the space alien cross the road?\"},\n",
    "#     {\"sender\": \"human\", \"content\": \"What is the capital of France?\"},\n",
    "#     {\"sender\": \"ai\", \"content\": \"The capital of France is Paris.\"},\n",
    "# ]\n",
    "# store.store_conversation(str(conv_id), messages)\n",
    "# print(store.get_conversation(str(conv_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next steps \n",
    "- Perception code which will be used prepare the question based on the user query and chat history\n",
    "- Decision making code which will be used to decide the action based on the question and chat history\n",
    "- Action code which will be used to execute the action\n",
    "- Memory code which will be used to store the chat history\n",
    "- Client code which will be used to coordinate the conversation between the perception, decision making, action and memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perception code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebContentSearch(BaseModel):\n",
    "    enhanced_user_query: str = Field(\n",
    "        description=\"The enhanced user query to be searched on the web for similar content stored in the memory\"\n",
    "    )\n",
    "    no_of_results: int = Field(\n",
    "        1,\n",
    "        description=\"The number of results to be returned from the web search corresponding to the user query\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an intelligent assistant designed to refine user queries using current input and prior context. Your goal is to construct a precise, context-aware enhanced query that reflects the userâ€™s true intent.\n",
    "\n",
    "Your responsibilities are:\n",
    "1. Analyze the current user message to understand the core request.\n",
    "2. Examine the chat history (a list of messages with sender and content) to identify:\n",
    "   - Any contextually relevant information.\n",
    "   - Follow-ups or references to previous discussions.\n",
    "3. Determine whether the current query is:\n",
    "   a. A continuation of a previous topic.\n",
    "   b. A new, unrelated query.\n",
    "\n",
    "Based on this analysis, follow the appropriate path:\n",
    "\n",
    "**If the current message relates to previous topics:**\n",
    "- Incorporate relevant prior context into the enhanced query.\n",
    "- Clarify ambiguous references or pronouns using information from the chat history.\n",
    "- Resolve under-specified requests by grounding them in previous conversations.\n",
    "\n",
    "**If the message is a new topic:**\n",
    "- Focus solely on the current message.\n",
    "- Do not infer or inject unrelated context from chat history.\n",
    "\n",
    "**Process for forming the enhanced user query:**\n",
    "1. Carefully review the current message.\n",
    "2. Analyze the chat history (most recent last) to extract relevant past content.\n",
    "3. Identify if the user is building on a previous conversation.\n",
    "4. Integrate any necessary clarifications, references, or details from the chat history.\n",
    "5. Output a concise, context-enriched, unambiguous enhanced query.\n",
    "\n",
    "**Inputs:**\n",
    "- `chat_history`: A list of dictionaries, each with keys `sender` and `content`.\n",
    "    {chat_history}\n",
    "- `user_query`: The latest message from the user.\n",
    "    {user_query}\n",
    "\n",
    "**Output Format:**\n",
    "{format_instructions}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perception_chain(llm) -> RunnableSequence:\n",
    "    \"\"\"\n",
    "    Creates and returns the perception chain for extracting travel search parameters.\n",
    "\n",
    "    Args:\n",
    "        default_llm: The default LLM to use for the chain.\n",
    "\n",
    "    Returns:\n",
    "        A LangChain runnable sequence that takes a user query and chat history,\n",
    "        and returns a TravelSearch object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up a parser\n",
    "    parser = PydanticOutputParser(pydantic_object=WebContentSearch)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"human\", \"The user query is: {user_query}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "    # Create and return the chain\n",
    "    return prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-06 16:02:54.829\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mutility.utils\u001b[0m:\u001b[36mcheck_and_reset_index\u001b[0m:\u001b[36m25\u001b[0m - \u001b[33m\u001b[1mreset_index is set to True. Deleting existing index at 'c:\\workspace\\EagSession1\\eag_agentic_rag\\url_rag\\history_index'\u001b[0m\n",
      "\u001b[32m2025-05-06 16:02:54.831\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mutility.utils\u001b[0m:\u001b[36mcheck_and_reset_index\u001b[0m:\u001b[36m30\u001b[0m - \u001b[32m\u001b[1mSuccessfully deleted index folder 'c:\\workspace\\EagSession1\\eag_agentic_rag\\url_rag\\history_index'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "conv_id = uuid.uuid4()\n",
    "store = FaissConversationStore(\n",
    "    embedder, index_folder=history_index_name, reset_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "chat_history = store.get_conversation_as_lc_messages(str(conv_id))\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is role of helm in kubernetes?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "perception_chain = get_perception_chain(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enhanced_user_query='What is the role of Helm in Kubernetes?' no_of_results=1\n"
     ]
    }
   ],
   "source": [
    "result = perception_chain.invoke(\n",
    "    {\"user_query\": user_query, \"chat_history\": chat_history}\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the history object\n",
    "messages = [\n",
    "    {\"sender\": \"human\", \"content\": user_query},\n",
    "    {\"sender\": \"ai\", \"content\": result.enhanced_user_query},\n",
    "]\n",
    "store.store_conversation(str(conv_id), messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'human', 'content': 'What is role of helm in kubernetes?'}, {'role': 'ai', 'content': 'What is the role of Helm in Kubernetes?'}]\n",
      "enhanced_user_query='Give me two articles on the role of Helm in Kubernetes.' no_of_results=2\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Give me two articles on for the same topic?\"\n",
    "chat_history = store.get_conversation_as_lc_messages(str(conv_id))\n",
    "print(chat_history)\n",
    "\n",
    "result = perception_chain.invoke(\n",
    "    {\"user_query\": user_query, \"chat_history\": chat_history}\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the history object\n",
    "messages = [\n",
    "    {\"sender\": \"human\", \"content\": user_query},\n",
    "    {\"sender\": \"ai\", \"content\": result.enhanced_user_query},\n",
    "]\n",
    "store.store_conversation(str(conv_id), messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'human', 'content': 'What is role of helm in kubernetes?'}, {'role': 'ai', 'content': 'What is the role of Helm in Kubernetes?'}, {'role': 'human', 'content': 'Give me two articles on for the same topic?'}, {'role': 'ai', 'content': 'Give me two articles on the role of Helm in Kubernetes.'}]\n"
     ]
    }
   ],
   "source": [
    "chat_history = store.get_conversation_as_lc_messages(str(conv_id))\n",
    "print(chat_history) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function where we will pass the history object and a list of conversation and the chain\n",
    "# and it will keep processing the conversation and return the enhanced user query\n",
    "\n",
    "\n",
    "def process_conversation_and_enhance_query(\n",
    "    history_store, conversation_list, chain, conv_id\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a list of user queries (conversation_list) using the provided chain and history store.\n",
    "    For each user query, it retrieves the chat history, invokes the chain, stores the conversation,\n",
    "    and returns a list of enhanced user queries.\n",
    "\n",
    "    Args:\n",
    "        history_store: The object responsible for storing and retrieving conversation history.\n",
    "        conversation_list: List of user queries (strings) to process.\n",
    "        chain: The chain object with an 'invoke' method.\n",
    "        conv_id: The conversation id to store the conversation.\n",
    "\n",
    "    Returns:\n",
    "        List of enhanced user queries (one for each user query in conversation_list).\n",
    "    \"\"\"\n",
    "    enhanced_queries = []\n",
    "    for user_query in conversation_list:\n",
    "        # Retrieve chat history for the conversation\n",
    "        chat_history = history_store.get_conversation_as_lc_messages(str(conv_id))\n",
    "        # Invoke the chain to get the result\n",
    "        result = chain.invoke({\"user_query\": user_query, \"chat_history\": chat_history})\n",
    "        # Store the conversation\n",
    "        messages = [\n",
    "            {\"sender\": \"human\", \"content\": user_query},\n",
    "            {\"sender\": \"ai\", \"content\": result.enhanced_user_query},\n",
    "        ]\n",
    "        history_store.store_conversation(str(conv_id), messages)\n",
    "        # Collect the enhanced user query\n",
    "        enhanced_queries.append(result.enhanced_user_query)\n",
    "    return enhanced_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-06 16:07:17.994\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mutility.utils\u001b[0m:\u001b[36mcheck_and_reset_index\u001b[0m:\u001b[36m25\u001b[0m - \u001b[33m\u001b[1mreset_index is set to True. Deleting existing index at 'c:\\workspace\\EagSession1\\eag_agentic_rag\\url_rag\\history_index'\u001b[0m\n",
      "\u001b[32m2025-05-06 16:07:17.995\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mutility.utils\u001b[0m:\u001b[36mcheck_and_reset_index\u001b[0m:\u001b[36m30\u001b[0m - \u001b[32m\u001b[1mSuccessfully deleted index folder 'c:\\workspace\\EagSession1\\eag_agentic_rag\\url_rag\\history_index'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the capital city of France?', 'Tell me more about the history of Paris, the capital of France.', 'List two famous landmarks in Paris, the capital of France.']\n"
     ]
    }
   ],
   "source": [
    "conv_id = uuid.uuid4()\n",
    "memory_store = FaissConversationStore(\n",
    "    embedder, index_folder=history_index_name, reset_index=True\n",
    ")\n",
    "conversation_list = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Tell me more about its history.\",\n",
    "    \"List two famous landmarks in that city.\"\n",
    "]\n",
    "enhanced_queries = process_conversation_and_enhance_query(memory_store, conversation_list, perception_chain, conv_id)\n",
    "print(enhanced_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################### END OF FILE #########################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
